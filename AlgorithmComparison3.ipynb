{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RochaErik/AlgorithmComparison/blob/main/AlgorithmComparison3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eemeaAaCsyS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7sPV5hJCeJ2"
      },
      "source": [
        "# **Evaluating algorithms with hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knwYV1QmCuEU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnwsW6x9w1QO",
        "outputId": "fad3ad0d-f838-4cfe-e95c-f8295d5fd365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (1.2.1)\n",
            "Requirement already satisfied: graphviz in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (3.7.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (2.1.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (1.11.2)\n",
            "Requirement already satisfied: plotly in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (5.16.1)\n",
            "Requirement already satisfied: six in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (10.0.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (6.0.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from plotly->catboost) (8.2.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.16.2)\n",
            "Requirement already satisfied: lightgbm in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (4.0.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from lightgbm) (1.25.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from lightgbm) (1.11.2)\n",
            "Requirement already satisfied: xgboost in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (1.7.6)\n",
            "Requirement already satisfied: numpy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from xgboost) (1.11.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost\n",
        "!pip install lightgbm\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "sp9bGvxdqiOw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "import scipy.stats as stats\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB5jc4iDCWZI",
        "outputId": "a417925a-0c0b-4a02-e9a2-ca428226ab51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hyperopt in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (1.25.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (1.11.2)\n",
            "Requirement already satisfied: six in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (3.1)\n",
            "Requirement already satisfied: future in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (0.18.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (4.66.1)\n",
            "Requirement already satisfied: cloudpickle in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (2.2.1)\n",
            "Requirement already satisfied: py4j in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (0.10.9.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from tqdm->hyperopt) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install hyperopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "pnmKn_fsDTha"
      },
      "outputs": [],
      "source": [
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByCnDDmkDayW"
      },
      "source": [
        "# **Wine Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "23mGy-W6DZLy"
      },
      "outputs": [],
      "source": [
        "wine_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Wine\\wine.data', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "C0N1S4LWDnbw"
      },
      "outputs": [],
      "source": [
        "X = wine_df.iloc[:, 1:]\n",
        "y = wine_df.iloc[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "omlj8qxkDoM1"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "bEtKdQvTEsAR"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcZuN-z4CdXh",
        "outputId": "ee31c32a-6b6b-467e-f741-153da73f7c60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/50 [00:00<00:11,  4.42trial/s, best loss: -0.9722222222222222]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:24<00:00,  2.04trial/s, best loss: -1.0]              \n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 200.0, 'learning_rate': 0.06659352635164861, 'max_depth': 4.0, 'max_features': 'sqrt', 'min_samples_leaf': 3.0, 'min_samples_split': 2.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [01:11<00:00,  1.42s/trial, best loss: -1.0]              \n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': 'sqrt', 'n_estimators': 100, 'learning_rate': 0.04102652661864284, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.0, 'min_impurity_decrease': 1.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [02:30<00:00,  3.01s/trial, best loss: -1.0]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 550, 'learning_rate': 0.0479901225935416, 'min_child_samples': 1, 'max_depth': 6, 'reg_lambda': 3.3766279624518107, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 31.44trial/s, best loss: -0.9722222222222222]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'gbdt', 'num_leaves': 55, 'learning_rate': 0.04496177447997528, 'min_child_samples': 10, 'reg_alpha': 0.3916912792044354, 'reg_lambda': 1.4941077467431771, 'colsample_by_tree': 0.379259630420579, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:16<00:00,  2.98trial/s, best loss: -1.0]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'dart', 'learning_rate': 0.09292666170093178, 'gamma': 4, 'max_depth': 4, 'min_child_weight': 1, 'colsample_bytree': 0.8943278668489419, 'colsample_bylevel': 0.2640104690942444, 'colsample_bynode': 0.8937107554719765, 'reg_alpha': 0.056770729092546546, 'reg_lambda': 4.219736540591216, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 200.0,\n",
              " 'learning_rate': 0.06659352635164861,\n",
              " 'max_depth': 4.0,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_leaf': 3.0,\n",
              " 'min_samples_split': 2.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['AdaBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'criterion': 'friedman_mse',\n",
              " 'max_features': 'sqrt',\n",
              " 'n_estimators': 100,\n",
              " 'learning_rate': 0.04102652661864284,\n",
              " 'max_depth': 3,\n",
              " 'min_samples_split': 7,\n",
              " 'min_samples_leaf': 7,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'min_impurity_decrease': 1.0,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 173,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['GradBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 550,\n",
              " 'learning_rate': 0.0479901225935416,\n",
              " 'min_child_samples': 1,\n",
              " 'max_depth': 6,\n",
              " 'reg_lambda': 3.3766279624518107,\n",
              " 'silent': True,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['CatBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'class_weight': 'balanced',\n",
              " 'boosting_type': 'gbdt',\n",
              " 'num_leaves': 55,\n",
              " 'learning_rate': 0.04496177447997528,\n",
              " 'min_child_samples': 10,\n",
              " 'reg_alpha': 0.3916912792044354,\n",
              " 'reg_lambda': 1.4941077467431771,\n",
              " 'colsample_by_tree': 0.379259630420579,\n",
              " 'verbosity': -1,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 175,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['LightGBM']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'booster': 'dart',\n",
              " 'learning_rate': 0.09292666170093178,\n",
              " 'gamma': 4,\n",
              " 'max_depth': 4,\n",
              " 'min_child_weight': 1,\n",
              " 'colsample_bytree': 0.8943278668489419,\n",
              " 'colsample_bylevel': 0.2640104690942444,\n",
              " 'colsample_bynode': 0.8937107554719765,\n",
              " 'reg_alpha': 0.056770729092546546,\n",
              " 'reg_lambda': 4.219736540591216,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 176,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiGBWUhXmjty"
      },
      "outputs": [],
      "source": [
        "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['AdaBoost', 'GradBoost', 'CatBoost', 'LightGBM', 'XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7JQf94WmaZT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Wine Dataset ---------\n",
            "[0.94444444 0.94444444 0.94444444 1.         0.94444444 1.\n",
            " 1.         0.83333333 1.         1.         1.         1.\n",
            " 0.94444444 1.         0.94444444 1.         1.         0.88888889\n",
            " 1.         0.94117647 1.         1.         0.88888889 0.94444444\n",
            " 1.         1.         1.         1.         0.88235294 1.\n",
            " 0.94444444 1.         1.         0.94444444 1.         0.94444444\n",
            " 1.         1.         0.94117647 0.88235294 0.88888889 1.\n",
            " 1.         0.94444444 1.         1.         1.         1.\n",
            " 0.82352941 0.94117647 1.         0.94444444 1.         0.94444444\n",
            " 1.         1.         0.94444444 0.94444444 0.94117647 0.94117647\n",
            " 1.         1.         0.94444444 1.         1.         0.88888889\n",
            " 0.94444444 0.94444444 0.88235294 1.         0.94444444 1.\n",
            " 1.         0.94444444 1.         1.         0.94444444 0.94444444\n",
            " 0.94117647 0.94117647 1.         1.         1.         0.94444444\n",
            " 1.         0.94444444 0.94444444 1.         0.94117647 1.\n",
            " 0.94444444 0.94444444 1.         0.94444444 1.         0.88888889\n",
            " 0.94444444 0.94444444 0.94117647 1.        ]\n",
            "Accuracy: 96.55% (4.09%)\n",
            "Execution Time: 22.88 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Wine Dataset ---------\n",
            "[1.         0.94444444 1.         1.         1.         1.\n",
            " 1.         0.88888889 1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         0.94444444 0.94444444\n",
            " 1.         0.94117647 1.         1.         0.88888889 0.94444444\n",
            " 1.         1.         1.         1.         0.94117647 0.88235294\n",
            " 0.94444444 1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         0.82352941 1.         1.\n",
            " 1.         0.94444444 1.         0.94444444 1.         1.\n",
            " 0.94117647 1.         1.         0.94444444 0.94444444 1.\n",
            " 1.         1.         1.         0.94444444 1.         0.94117647\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         1.         1.\n",
            " 1.         0.94117647 1.         1.         1.         0.94444444\n",
            " 1.         0.88888889 1.         1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         0.94444444 1.\n",
            " 0.94444444 0.94444444 0.94117647 1.        ]\n",
            "Accuracy: 98.08% (3.44%)\n",
            "Execution Time: 16.07 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Wine Dataset ---------\n",
            "[1.         0.94444444 1.         1.         0.94444444 1.\n",
            " 1.         0.88888889 1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         0.94444444 0.94444444\n",
            " 1.         0.94117647 1.         1.         0.94444444 0.94444444\n",
            " 1.         1.         1.         1.         0.94117647 0.94117647\n",
            " 0.94444444 1.         1.         0.94444444 1.         1.\n",
            " 1.         1.         0.94117647 0.94117647 0.94444444 1.\n",
            " 1.         0.94444444 1.         1.         1.         1.\n",
            " 0.94117647 1.         1.         0.94444444 1.         0.94444444\n",
            " 1.         1.         1.         0.94444444 1.         0.94117647\n",
            " 1.         1.         0.88888889 1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         1.         1.\n",
            " 1.         0.94117647 1.         1.         1.         0.94444444\n",
            " 1.         0.88888889 1.         1.         0.94117647 1.\n",
            " 1.         0.94444444 1.         0.94444444 0.94444444 1.\n",
            " 0.94444444 1.         0.94117647 1.        ]\n",
            "Accuracy: 97.97% (3.03%)\n",
            "Execution Time: 97.15 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Wine Dataset ---------\n",
            "[1.         0.94444444 0.94444444 1.         0.94444444 1.\n",
            " 1.         0.77777778 1.         1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         0.94444444 0.94444444\n",
            " 1.         1.         1.         1.         0.94444444 0.94444444\n",
            " 1.         1.         1.         1.         0.88235294 0.94117647\n",
            " 0.94444444 1.         1.         1.         1.         1.\n",
            " 1.         0.88888889 0.94117647 0.94117647 0.94444444 1.\n",
            " 1.         0.94444444 1.         0.94444444 1.         1.\n",
            " 0.88235294 0.94117647 1.         0.94444444 1.         0.94444444\n",
            " 1.         1.         1.         0.94444444 1.         0.94117647\n",
            " 0.94444444 1.         0.94444444 1.         1.         1.\n",
            " 1.         0.94444444 0.88235294 1.         0.94444444 1.\n",
            " 1.         0.94444444 1.         1.         1.         0.94444444\n",
            " 0.94117647 0.94117647 1.         1.         1.         0.88888889\n",
            " 1.         0.88888889 1.         0.94444444 1.         1.\n",
            " 1.         0.94444444 1.         0.94444444 0.88888889 1.\n",
            " 1.         0.94444444 0.94117647 1.        ]\n",
            "Accuracy: 97.12% (4.03%)\n",
            "Execution Time: 3.34 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Wine Dataset ---------\n",
            "[1.         0.94444444 1.         1.         0.94444444 1.\n",
            " 1.         0.88888889 1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         0.94444444 0.88888889\n",
            " 1.         0.94117647 1.         1.         0.94444444 0.94444444\n",
            " 1.         1.         1.         1.         0.94117647 0.88235294\n",
            " 0.94444444 1.         1.         1.         1.         1.\n",
            " 1.         1.         0.94117647 0.88235294 0.94444444 1.\n",
            " 1.         0.94444444 1.         0.94444444 1.         1.\n",
            " 0.94117647 1.         1.         0.94444444 0.94444444 1.\n",
            " 1.         1.         1.         1.         1.         0.94117647\n",
            " 1.         1.         0.88888889 1.         1.         1.\n",
            " 0.94444444 0.94444444 1.         1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         1.         0.94444444\n",
            " 1.         0.94117647 1.         1.         1.         0.94444444\n",
            " 1.         0.88888889 1.         0.94444444 1.         1.\n",
            " 1.         0.94444444 1.         1.         0.94444444 1.\n",
            " 1.         0.94444444 0.94117647 1.        ]\n",
            "Accuracy: 97.80% (3.38%)\n",
            "Execution Time: 40.20 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "wine_scores = []\n",
        "wine_scores_mean = []\n",
        "wine_scores_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "        \n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    wine_scores.append(results)\n",
        "    wine_scores_mean.append(results.mean()*100)\n",
        "    wine_scores_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "\n",
        "    print(f'--------- {algorithm_name} on Wine Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Wine scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(wine_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results = pd.DataFrame()\n",
        "Algo_results['Names'] = names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Wine'] = wine_scores_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine\n",
              "0   AdaBoost  96.552288\n",
              "1  GradBoost  98.075163\n",
              "2   CatBoost  97.967320\n",
              "3   LightGBM  97.120915\n",
              "4    XGBoost  97.797386"
            ]
          },
          "execution_count": 183,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results = pd.DataFrame()\n",
        "Algo_time_results['Names'] = names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Wine'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>22.876993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>16.068150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.146326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>3.337753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>40.201234</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine\n",
              "0   AdaBoost  22.876993\n",
              "1  GradBoost  16.068150\n",
              "2   CatBoost  97.146326\n",
              "3   LightGBM   3.337753\n",
              "4    XGBoost  40.201234"
            ]
          },
          "execution_count": 186,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_time_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Breast Cancer Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "breast_cancer_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\BreastCancer\\Breast.dat', sep=',', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = breast_cancer_df.iloc[:, :-1]\n",
        "y = breast_cancer_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:40<00:00,  1.24trial/s, best loss: -0.9927007299270073]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 450.0, 'learning_rate': 0.01199453123793802, 'max_depth': 4.0, 'max_features': 'log2', 'min_samples_leaf': 4.0, 'min_samples_split': 6.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:16<00:00,  2.99trial/s, best loss: -0.9927007299270073]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': 'sqrt', 'n_estimators': 550, 'learning_rate': 0.0611622198189229, 'max_depth': 2, 'min_samples_split': 7, 'min_samples_leaf': 5, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 1.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:34<00:00,  1.44trial/s, best loss: -0.9927007299270073]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 300, 'learning_rate': 0.013094250183297027, 'min_child_samples': 1, 'max_depth': 5, 'reg_lambda': 4.710165866797953, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 44.43trial/s, best loss: -0.9854014598540146]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'goss', 'num_leaves': 70, 'learning_rate': 0.06466735422122151, 'min_child_samples': 60, 'reg_alpha': 1.7712918439651535, 'reg_lambda': 0.09630512995808138, 'colsample_by_tree': 0.9773212695265424, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:08<00:00,  5.86trial/s, best loss: -0.9927007299270073]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'dart', 'learning_rate': 0.01958151011028328, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 3, 'colsample_bytree': 0.17755828466772988, 'colsample_bylevel': 0.2750454258060418, 'colsample_bynode': 0.5404751722067938, 'reg_alpha': 1.5842037921731331, 'reg_lambda': 1.1583230510671016, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 450.0,\n",
              " 'learning_rate': 0.01199453123793802,\n",
              " 'max_depth': 4.0,\n",
              " 'max_features': 'log2',\n",
              " 'min_samples_leaf': 4.0,\n",
              " 'min_samples_split': 6.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 192,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['AdaBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'criterion': 'friedman_mse',\n",
              " 'max_features': 'sqrt',\n",
              " 'n_estimators': 550,\n",
              " 'learning_rate': 0.0611622198189229,\n",
              " 'max_depth': 2,\n",
              " 'min_samples_split': 7,\n",
              " 'min_samples_leaf': 5,\n",
              " 'min_weight_fraction_leaf': 0.1,\n",
              " 'min_impurity_decrease': 1.0,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 193,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['GradBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 300,\n",
              " 'learning_rate': 0.013094250183297027,\n",
              " 'min_child_samples': 1,\n",
              " 'max_depth': 5,\n",
              " 'reg_lambda': 4.710165866797953,\n",
              " 'silent': True,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 194,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['CatBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'class_weight': 'balanced',\n",
              " 'boosting_type': 'goss',\n",
              " 'num_leaves': 70,\n",
              " 'learning_rate': 0.06466735422122151,\n",
              " 'min_child_samples': 60,\n",
              " 'reg_alpha': 1.7712918439651535,\n",
              " 'reg_lambda': 0.09630512995808138,\n",
              " 'colsample_by_tree': 0.9773212695265424,\n",
              " 'verbosity': -1,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 195,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['LightGBM']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'booster': 'dart',\n",
              " 'learning_rate': 0.01958151011028328,\n",
              " 'gamma': 5,\n",
              " 'max_depth': 3,\n",
              " 'min_child_weight': 3,\n",
              " 'colsample_bytree': 0.17755828466772988,\n",
              " 'colsample_bylevel': 0.2750454258060418,\n",
              " 'colsample_bynode': 0.5404751722067938,\n",
              " 'reg_alpha': 1.5842037921731331,\n",
              " 'reg_lambda': 1.1583230510671016,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 196,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [],
      "source": [
        "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['AdaBoost', 'GradBoost', 'CatBoost', 'LightGBM', 'XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Breast Cancer Dataset ---------\n",
            "[0.95652174 0.97101449 0.92753623 0.95588235 0.97058824 1.\n",
            " 0.97058824 0.97058824 0.98529412 0.98529412 0.97101449 0.95652174\n",
            " 0.95652174 0.98529412 0.95588235 0.95588235 1.         0.97058824\n",
            " 0.98529412 0.97058824 1.         0.95652174 0.97101449 0.98529412\n",
            " 0.95588235 0.98529412 0.97058824 0.98529412 0.97058824 0.97058824\n",
            " 0.98550725 0.94202899 0.98550725 0.97058824 0.98529412 1.\n",
            " 0.97058824 0.98529412 0.94117647 0.95588235 0.95652174 0.98550725\n",
            " 0.98550725 1.         0.98529412 0.97058824 0.97058824 0.98529412\n",
            " 0.95588235 0.94117647 0.98550725 0.92753623 1.         1.\n",
            " 0.94117647 0.98529412 0.97058824 0.98529412 0.97058824 0.97058824\n",
            " 0.95652174 0.98550725 0.98550725 0.91176471 0.98529412 0.98529412\n",
            " 0.94117647 0.97058824 0.92647059 1.         0.95652174 1.\n",
            " 0.97101449 0.95588235 0.98529412 0.94117647 0.98529412 0.98529412\n",
            " 0.95588235 0.97058824 0.97101449 0.97101449 0.97101449 0.97058824\n",
            " 0.98529412 0.95588235 0.97058824 0.97058824 0.98529412 0.97058824\n",
            " 1.         0.95652174 0.98550725 0.97058824 0.95588235 1.\n",
            " 0.97058824 0.98529412 0.97058824 0.94117647]\n",
            "Accuracy: 97.16% (1.86%)\n",
            "Execution Time: 61.19 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Breast Cancer Dataset ---------\n",
            "[0.95652174 0.98550725 0.98550725 0.95588235 0.97058824 0.98529412\n",
            " 0.95588235 0.97058824 0.95588235 0.98529412 0.95652174 0.97101449\n",
            " 0.97101449 0.97058824 0.95588235 0.92647059 1.         0.92647059\n",
            " 0.97058824 0.97058824 1.         0.94202899 0.95652174 0.97058824\n",
            " 0.97058824 0.98529412 0.97058824 0.95588235 0.97058824 0.98529412\n",
            " 0.95652174 0.95652174 0.97101449 0.97058824 0.97058824 1.\n",
            " 0.98529412 0.98529412 0.89705882 0.97058824 0.95652174 0.98550725\n",
            " 0.97101449 0.98529412 0.97058824 0.94117647 0.95588235 0.98529412\n",
            " 0.94117647 0.95588235 1.         0.91304348 1.         0.98529412\n",
            " 0.89705882 0.97058824 0.95588235 0.97058824 0.98529412 0.98529412\n",
            " 0.95652174 0.98550725 0.97101449 0.95588235 0.97058824 1.\n",
            " 0.94117647 0.97058824 0.97058824 0.98529412 0.98550725 0.98550725\n",
            " 0.97101449 0.95588235 0.98529412 0.94117647 0.97058824 0.94117647\n",
            " 0.95588235 0.95588235 0.94202899 0.95652174 0.94202899 0.95588235\n",
            " 0.95588235 0.95588235 0.97058824 1.         0.98529412 0.97058824\n",
            " 0.98550725 0.94202899 0.97101449 0.98529412 0.95588235 1.\n",
            " 0.97058824 0.92647059 0.97058824 0.94117647]\n",
            "Accuracy: 96.65% (2.09%)\n",
            "Execution Time: 21.08 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Breast Cancer Dataset ---------\n",
            "[0.94202899 0.97101449 0.98550725 0.95588235 0.97058824 1.\n",
            " 0.97058824 0.97058824 0.97058824 0.98529412 0.98550725 0.97101449\n",
            " 0.97101449 0.97058824 0.97058824 0.95588235 1.         0.95588235\n",
            " 1.         0.95588235 1.         0.94202899 0.95652174 0.97058824\n",
            " 0.95588235 1.         0.95588235 0.98529412 0.97058824 0.98529412\n",
            " 0.97101449 0.95652174 0.98550725 0.98529412 0.98529412 1.\n",
            " 0.98529412 0.97058824 0.94117647 0.97058824 0.97101449 0.98550725\n",
            " 1.         1.         0.98529412 0.95588235 0.98529412 0.97058824\n",
            " 0.94117647 0.95588235 1.         0.95652174 1.         0.98529412\n",
            " 0.94117647 0.98529412 0.95588235 0.97058824 0.97058824 0.98529412\n",
            " 0.95652174 0.98550725 0.98550725 0.92647059 0.98529412 1.\n",
            " 0.94117647 0.97058824 0.98529412 0.98529412 1.         1.\n",
            " 0.97101449 0.95588235 0.98529412 0.97058824 0.97058824 0.95588235\n",
            " 0.95588235 0.95588235 0.98550725 0.98550725 0.94202899 0.97058824\n",
            " 0.98529412 0.97058824 0.97058824 1.         0.98529412 0.98529412\n",
            " 0.98550725 0.95652174 0.97101449 1.         0.97058824 1.\n",
            " 0.97058824 0.97058824 0.97058824 0.92647059]\n",
            "Accuracy: 97.38% (1.80%)\n",
            "Execution Time: 45.57 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Breast Cancer Dataset ---------\n",
            "[0.97101449 0.97101449 0.95652174 0.95588235 0.97058824 1.\n",
            " 0.95588235 0.97058824 0.98529412 0.98529412 0.98550725 0.97101449\n",
            " 0.98550725 0.98529412 0.97058824 0.92647059 1.         0.95588235\n",
            " 1.         0.95588235 1.         0.94202899 0.95652174 0.97058824\n",
            " 0.95588235 1.         0.97058824 0.98529412 0.97058824 0.97058824\n",
            " 0.97101449 0.95652174 0.97101449 0.97058824 0.98529412 0.97058824\n",
            " 0.98529412 1.         0.94117647 0.97058824 0.95652174 1.\n",
            " 0.98550725 0.98529412 0.98529412 0.95588235 0.98529412 0.97058824\n",
            " 0.95588235 0.95588235 1.         0.91304348 1.         0.98529412\n",
            " 0.89705882 0.97058824 0.97058824 0.98529412 1.         0.98529412\n",
            " 0.95652174 1.         0.98550725 0.92647059 0.98529412 1.\n",
            " 0.94117647 0.97058824 0.98529412 0.95588235 1.         1.\n",
            " 0.97101449 0.95588235 0.98529412 0.94117647 0.97058824 0.98529412\n",
            " 0.98529412 0.97058824 0.94202899 0.98550725 0.95652174 0.98529412\n",
            " 0.98529412 0.97058824 0.98529412 1.         0.98529412 0.97058824\n",
            " 1.         0.97101449 0.98550725 0.98529412 0.97058824 0.98529412\n",
            " 0.94117647 1.         0.97058824 0.94117647]\n",
            "Accuracy: 97.33% (2.05%)\n",
            "Execution Time: 1.49 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Breast Cancer Dataset ---------\n",
            "[0.95652174 0.98550725 0.98550725 0.95588235 0.95588235 0.98529412\n",
            " 0.95588235 0.97058824 0.95588235 0.97058824 0.98550725 0.97101449\n",
            " 0.95652174 0.97058824 0.97058824 0.95588235 1.         0.94117647\n",
            " 0.95588235 0.97058824 1.         0.92753623 0.95652174 0.95588235\n",
            " 0.95588235 1.         0.97058824 0.97058824 0.97058824 0.97058824\n",
            " 0.95652174 0.97101449 0.97101449 0.97058824 0.97058824 1.\n",
            " 0.98529412 1.         0.91176471 0.95588235 0.97101449 0.98550725\n",
            " 0.97101449 1.         0.97058824 0.92647059 0.98529412 0.97058824\n",
            " 0.94117647 0.94117647 1.         0.92753623 0.97101449 0.97058824\n",
            " 0.92647059 0.98529412 0.95588235 0.97058824 1.         0.97058824\n",
            " 0.95652174 0.98550725 0.97101449 0.94117647 0.97058824 1.\n",
            " 0.92647059 0.97058824 0.97058824 0.98529412 1.         0.98550725\n",
            " 0.95652174 0.92647059 0.98529412 0.95588235 0.95588235 0.97058824\n",
            " 0.98529412 0.95588235 0.95652174 0.97101449 0.94202899 0.95588235\n",
            " 0.97058824 0.97058824 0.98529412 0.98529412 0.98529412 0.97058824\n",
            " 1.         0.94202899 0.98550725 0.98529412 0.95588235 1.\n",
            " 0.97058824 0.95588235 0.95588235 0.92647059]\n",
            "Accuracy: 96.79% (2.02%)\n",
            "Execution Time: 17.14 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "breast_cancer_scores = []\n",
        "breast_cancer_mean = []\n",
        "breast_cancer_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    breast_cancer_scores.append(results)\n",
        "    breast_cancer_mean.append(results.mean()*100)\n",
        "    breast_cancer_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "\n",
        "    print(f'--------- {algorithm_name} on Breast Cancer Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Breast Cancer scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(breast_cancer_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Breast_Cancer'] = breast_cancer_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "      <td>97.159847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>96.646633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.378303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.334612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "      <td>96.792626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer\n",
              "0   AdaBoost  96.552288      97.159847\n",
              "1  GradBoost  98.075163      96.646633\n",
              "2   CatBoost  97.967320      97.378303\n",
              "3   LightGBM  97.120915      97.334612\n",
              "4    XGBoost  97.797386      96.792626"
            ]
          },
          "execution_count": 202,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Breast_Cancer'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Sonar Dataset** #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {},
      "outputs": [],
      "source": [
        "sonar_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Sonar\\Sonar.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = sonar_df.iloc[:, :-1]\n",
        "y = sonar_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [02:35<00:00,  3.10s/trial, best loss: -0.9523809523809523]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 1250.0, 'learning_rate': 0.011066661922600281, 'max_depth': 2.0, 'max_features': None, 'min_samples_leaf': 5.0, 'min_samples_split': 6.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:51<00:00,  1.03s/trial, best loss: -0.8571428571428571]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': 'sqrt', 'n_estimators': 100, 'learning_rate': 0.04102652661864284, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.0, 'min_impurity_decrease': 1.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [04:58<00:00,  5.98s/trial, best loss: -0.8809523809523809]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 1300, 'learning_rate': 0.014023863721779927, 'min_child_samples': 9, 'max_depth': 7, 'reg_lambda': 0.2645130637158699, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 42.91trial/s, best loss: -0.7857142857142857]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'gbdt', 'num_leaves': 80, 'learning_rate': 0.088633625625231, 'min_child_samples': 60, 'reg_alpha': 0.20114179877735983, 'reg_lambda': 0.021920717955273894, 'colsample_by_tree': 0.9439502913709372, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:08<00:00,  5.68trial/s, best loss: -0.8571428571428571]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'dart', 'learning_rate': 0.044025607478991216, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 1, 'colsample_bytree': 0.3050569452640979, 'colsample_bylevel': 0.914172295823844, 'colsample_bynode': 0.2986672398458319, 'reg_alpha': 0.7657116989618611, 'reg_lambda': 0.8092636688928795, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [],
      "source": [
        "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 1250.0,\n",
              " 'learning_rate': 0.011066661922600281,\n",
              " 'max_depth': 2.0,\n",
              " 'max_features': None,\n",
              " 'min_samples_leaf': 5.0,\n",
              " 'min_samples_split': 6.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 210,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['AdaBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'criterion': 'friedman_mse',\n",
              " 'max_features': 'sqrt',\n",
              " 'n_estimators': 100,\n",
              " 'learning_rate': 0.04102652661864284,\n",
              " 'max_depth': 3,\n",
              " 'min_samples_split': 7,\n",
              " 'min_samples_leaf': 7,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'min_impurity_decrease': 1.0,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 211,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['GradBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 1300,\n",
              " 'learning_rate': 0.014023863721779927,\n",
              " 'min_child_samples': 9,\n",
              " 'max_depth': 7,\n",
              " 'reg_lambda': 0.2645130637158699,\n",
              " 'silent': True,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 212,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['CatBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'class_weight': 'balanced',\n",
              " 'boosting_type': 'gbdt',\n",
              " 'num_leaves': 80,\n",
              " 'learning_rate': 0.088633625625231,\n",
              " 'min_child_samples': 60,\n",
              " 'reg_alpha': 0.20114179877735983,\n",
              " 'reg_lambda': 0.021920717955273894,\n",
              " 'colsample_by_tree': 0.9439502913709372,\n",
              " 'verbosity': -1,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 213,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['LightGBM']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'booster': 'dart',\n",
              " 'learning_rate': 0.044025607478991216,\n",
              " 'gamma': 2,\n",
              " 'max_depth': 5,\n",
              " 'min_child_weight': 1,\n",
              " 'colsample_bytree': 0.3050569452640979,\n",
              " 'colsample_bylevel': 0.914172295823844,\n",
              " 'colsample_bynode': 0.2986672398458319,\n",
              " 'reg_alpha': 0.7657116989618611,\n",
              " 'reg_lambda': 0.8092636688928795,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['AdaBoost', 'GradBoost', 'CatBoost', 'LightGBM', 'XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Sonar Dataset ---------\n",
            "[0.95238095 0.80952381 0.85714286 0.95238095 0.80952381 1.\n",
            " 0.9047619  0.85714286 0.85       1.         0.9047619  0.9047619\n",
            " 0.95238095 0.9047619  0.80952381 0.9047619  0.9047619  0.9047619\n",
            " 0.85       0.75       0.76190476 0.80952381 1.         0.9047619\n",
            " 0.80952381 0.9047619  0.85714286 0.76190476 0.85       0.9\n",
            " 0.95238095 0.85714286 0.9047619  0.71428571 0.85714286 0.80952381\n",
            " 0.80952381 0.9047619  0.85       0.8        0.9047619  0.76190476\n",
            " 0.9047619  0.71428571 0.76190476 0.85714286 0.80952381 0.71428571\n",
            " 0.9        0.95       0.9047619  0.85714286 0.85714286 0.85714286\n",
            " 0.80952381 0.9047619  0.95238095 0.9047619  0.9        0.8\n",
            " 0.66666667 0.85714286 0.9047619  0.9047619  0.76190476 0.9047619\n",
            " 0.80952381 0.9047619  0.65       1.         0.85714286 0.9047619\n",
            " 0.80952381 0.9047619  0.95238095 0.76190476 0.9047619  0.85714286\n",
            " 0.85       0.9        0.76190476 0.76190476 0.76190476 1.\n",
            " 0.95238095 0.95238095 0.9047619  0.9047619  0.9        0.9\n",
            " 0.9047619  0.9047619  0.80952381 0.85714286 0.9047619  0.9047619\n",
            " 0.76190476 0.9047619  0.9        0.8       ]\n",
            "Accuracy: 86.35% (7.35%)\n",
            "Execution Time: 684.94 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Sonar Dataset ---------\n",
            "[0.80952381 0.66666667 0.71428571 0.95238095 0.57142857 0.9047619\n",
            " 0.71428571 0.80952381 0.75       0.95       0.71428571 0.71428571\n",
            " 0.80952381 0.95238095 0.85714286 0.76190476 0.85714286 0.76190476\n",
            " 0.9        0.6        0.76190476 0.61904762 0.9047619  0.85714286\n",
            " 0.71428571 0.71428571 0.85714286 0.80952381 0.7        0.8\n",
            " 0.85714286 0.9047619  0.66666667 0.66666667 0.66666667 0.80952381\n",
            " 0.71428571 0.80952381 0.75       0.8        0.9047619  0.76190476\n",
            " 0.85714286 0.66666667 0.61904762 0.76190476 0.76190476 0.61904762\n",
            " 0.9        0.9        0.76190476 0.71428571 0.76190476 0.61904762\n",
            " 0.71428571 0.76190476 0.9047619  0.85714286 0.95       0.75\n",
            " 0.71428571 0.66666667 0.9047619  0.80952381 0.85714286 0.71428571\n",
            " 0.85714286 0.80952381 0.65       0.75       0.9047619  0.85714286\n",
            " 0.76190476 0.76190476 0.85714286 0.71428571 0.76190476 0.76190476\n",
            " 0.8        0.85       0.80952381 0.57142857 0.71428571 0.76190476\n",
            " 0.76190476 0.85714286 0.80952381 0.80952381 0.85       0.85\n",
            " 0.80952381 0.71428571 0.76190476 0.80952381 0.76190476 0.80952381\n",
            " 0.85714286 0.80952381 0.7        0.85      ]\n",
            "Accuracy: 78.15% (8.83%)\n",
            "Execution Time: 8.58 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Sonar Dataset ---------\n",
            "[0.95238095 0.76190476 0.85714286 1.         0.71428571 0.95238095\n",
            " 0.9047619  0.85714286 0.85       0.95       0.76190476 0.9047619\n",
            " 0.95238095 0.95238095 0.85714286 0.9047619  0.85714286 0.76190476\n",
            " 0.95       0.75       0.9047619  0.71428571 1.         0.95238095\n",
            " 0.76190476 0.80952381 0.85714286 0.9047619  0.95       0.9\n",
            " 0.95238095 0.95238095 0.85714286 0.76190476 0.85714286 0.80952381\n",
            " 0.9047619  0.9047619  0.85       0.85       0.9047619  0.76190476\n",
            " 0.95238095 0.76190476 0.9047619  0.80952381 0.85714286 0.9047619\n",
            " 0.9        0.9        0.95238095 0.85714286 0.80952381 0.85714286\n",
            " 0.71428571 0.9047619  0.9047619  0.95238095 0.9        0.85\n",
            " 0.85714286 0.80952381 0.9047619  0.85714286 0.80952381 0.95238095\n",
            " 0.95238095 0.9047619  0.65       0.9        0.85714286 0.95238095\n",
            " 0.85714286 0.95238095 0.9047619  0.71428571 0.85714286 0.80952381\n",
            " 0.85       0.9        0.85714286 0.76190476 0.71428571 0.9047619\n",
            " 0.85714286 0.9047619  0.95238095 0.95238095 0.95       0.9\n",
            " 0.9047619  0.80952381 0.71428571 0.85714286 1.         0.9047619\n",
            " 0.85714286 0.9047619  0.85       1.        ]\n",
            "Accuracy: 87.08% (7.54%)\n",
            "Execution Time: 777.30 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Sonar Dataset ---------\n",
            "[0.85714286 0.66666667 0.66666667 0.85714286 0.71428571 0.85714286\n",
            " 0.95238095 0.80952381 0.9        0.95       0.66666667 0.76190476\n",
            " 0.85714286 0.80952381 0.80952381 0.9047619  0.76190476 0.85714286\n",
            " 0.9        0.75       0.85714286 0.80952381 0.95238095 0.80952381\n",
            " 0.80952381 0.80952381 0.85714286 0.9047619  0.75       0.7\n",
            " 1.         0.9047619  0.80952381 0.71428571 0.71428571 0.85714286\n",
            " 0.80952381 0.76190476 0.8        0.9        0.9047619  0.80952381\n",
            " 0.9047619  0.66666667 0.76190476 0.80952381 0.9047619  0.71428571\n",
            " 0.85       0.85       0.95238095 0.71428571 0.80952381 0.80952381\n",
            " 0.76190476 0.80952381 0.85714286 0.95238095 0.9        0.9\n",
            " 0.85714286 0.85714286 0.85714286 0.9047619  0.85714286 0.80952381\n",
            " 0.80952381 0.80952381 0.65       0.75       0.9047619  0.76190476\n",
            " 0.80952381 0.80952381 0.9047619  0.85714286 0.76190476 0.80952381\n",
            " 0.75       0.75       0.85714286 0.61904762 0.71428571 0.85714286\n",
            " 0.80952381 0.85714286 0.85714286 0.95238095 0.95       0.85\n",
            " 0.80952381 0.76190476 0.66666667 0.85714286 0.85714286 0.9047619\n",
            " 0.80952381 0.85714286 0.8        0.95      ]\n",
            "Accuracy: 82.36% (7.92%)\n",
            "Execution Time: 1.50 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Sonar Dataset ---------\n",
            "[0.9047619  0.80952381 0.76190476 0.95238095 0.71428571 0.95238095\n",
            " 0.80952381 0.85714286 0.8        0.95       0.71428571 0.71428571\n",
            " 0.95238095 0.95238095 0.95238095 0.85714286 0.80952381 0.76190476\n",
            " 0.95       0.7        0.85714286 0.61904762 0.95238095 0.9047619\n",
            " 0.80952381 0.80952381 0.85714286 0.9047619  0.85       0.9\n",
            " 0.95238095 0.9047619  0.85714286 0.80952381 0.80952381 0.85714286\n",
            " 0.71428571 0.76190476 0.8        0.75       0.9047619  0.80952381\n",
            " 0.85714286 0.76190476 0.66666667 0.85714286 0.76190476 0.76190476\n",
            " 0.85       0.9        0.95238095 0.71428571 0.80952381 0.71428571\n",
            " 0.71428571 0.9047619  0.85714286 0.9047619  0.9        0.8\n",
            " 0.76190476 0.76190476 0.9047619  0.9047619  0.9047619  0.85714286\n",
            " 0.95238095 0.85714286 0.65       0.85       0.9047619  0.95238095\n",
            " 0.80952381 0.85714286 0.85714286 0.76190476 0.80952381 0.85714286\n",
            " 0.8        0.85       0.9047619  0.66666667 0.76190476 0.80952381\n",
            " 0.80952381 0.9047619  0.85714286 0.95238095 0.95       0.85\n",
            " 0.85714286 0.80952381 0.66666667 0.80952381 1.         0.85714286\n",
            " 0.9047619  0.85714286 0.8        0.95      ]\n",
            "Accuracy: 83.80% (8.27%)\n",
            "Execution Time: 21.75 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "sonar_scores = []\n",
        "sonar_mean = []\n",
        "sonar_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "        \n",
        "    start_time = time.time()\n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    sonar_scores.append(results)\n",
        "    sonar_mean.append(results.mean()*100)\n",
        "    sonar_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "\n",
        "    print(f'--------- {algorithm_name} on Sonar Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Sonar scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(sonar_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Sonar'] = sonar_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "      <td>97.159847</td>\n",
              "      <td>86.347619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>78.145238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>87.076190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>82.361905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "      <td>96.792626</td>\n",
              "      <td>83.802381</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer      Sonar\n",
              "0   AdaBoost  96.552288      97.159847  86.347619\n",
              "1  GradBoost  98.075163      96.646633  78.145238\n",
              "2   CatBoost  97.967320      97.378303  87.076190\n",
              "3   LightGBM  97.120915      97.334612  82.361905\n",
              "4    XGBoost  97.797386      96.792626  83.802381"
            ]
          },
          "execution_count": 219,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Sonar'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Ionosphere Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [],
      "source": [
        "ionosphere_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Ionosphere\\ionosphere.data', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(351, 35)"
            ]
          },
          "execution_count": 222,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ionosphere_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = ionosphere_df.iloc[:, :-1]\n",
        "y = ionosphere_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [],
      "source": [
        "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['AdaBoost', 'GradBoost', 'CatBoost', 'LightGBM', 'XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:23<00:00,  1.66s/trial, best loss: -0.971830985915493] \n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 50.0, 'learning_rate': 0.046035205781861564, 'max_depth': 6.0, 'max_features': 'log2', 'min_samples_leaf': 5.0, 'min_samples_split': 2.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.06s/trial, best loss: -0.9295774647887324]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': None, 'n_estimators': 400, 'learning_rate': 0.021178191623985942, 'max_depth': 4, 'min_samples_split': 3, 'min_samples_leaf': 8, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 2.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [06:43<00:00,  8.07s/trial, best loss: -0.9577464788732394]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 750, 'learning_rate': 0.02983152512960275, 'min_child_samples': 3, 'max_depth': 5, 'reg_lambda': 3.8771604915102147, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 43.06trial/s, best loss: -0.9436619718309859]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'gbdt', 'num_leaves': 75, 'learning_rate': 0.09085691661731564, 'min_child_samples': 20, 'reg_alpha': 0.8776705363565946, 'reg_lambda': 2.021006183231964, 'colsample_by_tree': 0.5358470999804816, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:07<00:00,  6.53trial/s, best loss: -0.9436619718309859]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'gbtree', 'learning_rate': 0.011777426690454684, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'colsample_bytree': 0.6642423404208758, 'colsample_bylevel': 0.8389604376670141, 'colsample_bynode': 0.46801910869053165, 'reg_alpha': 1.3842922617481603, 'reg_lambda': 0.25127542856871243, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Ionosphere Dataset ---------\n",
            "[1.         0.91428571 0.91428571 0.94285714 0.97142857 0.91428571\n",
            " 0.94285714 0.97142857 0.97142857 0.88571429 0.97222222 0.94285714\n",
            " 0.97142857 0.97142857 1.         0.94285714 0.91428571 0.88571429\n",
            " 0.91428571 0.88571429 0.86111111 0.85714286 0.97142857 0.94285714\n",
            " 0.97142857 0.94285714 1.         0.94285714 0.88571429 1.\n",
            " 0.91666667 0.97142857 0.94285714 1.         0.91428571 0.91428571\n",
            " 0.85714286 0.91428571 0.94285714 0.88571429 0.97222222 1.\n",
            " 0.91428571 0.91428571 0.97142857 0.82857143 1.         0.97142857\n",
            " 0.91428571 0.85714286 0.94444444 0.97142857 0.91428571 1.\n",
            " 0.94285714 0.97142857 0.97142857 0.91428571 0.91428571 0.88571429\n",
            " 0.97222222 0.91428571 0.91428571 0.94285714 0.97142857 0.91428571\n",
            " 1.         0.88571429 1.         0.94285714 0.97222222 0.88571429\n",
            " 0.91428571 0.94285714 0.88571429 0.88571429 1.         0.91428571\n",
            " 0.97142857 1.         0.91666667 0.94285714 0.94285714 0.97142857\n",
            " 0.8        1.         0.91428571 1.         0.91428571 0.91428571\n",
            " 0.91666667 0.88571429 1.         0.88571429 0.94285714 0.94285714\n",
            " 0.94285714 1.         0.94285714 0.97142857]\n",
            "Accuracy: 93.82% (4.34%)\n",
            "Execution Time: 12.95 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Ionosphere Dataset ---------\n",
            "[0.94444444 0.97142857 0.94285714 0.85714286 0.91428571 0.85714286\n",
            " 0.88571429 0.91428571 0.82857143 0.91428571 0.83333333 0.88571429\n",
            " 0.91428571 0.94285714 0.91428571 0.94285714 0.94285714 0.94285714\n",
            " 0.85714286 0.91428571 0.80555556 0.8        1.         0.97142857\n",
            " 1.         0.85714286 0.91428571 0.88571429 0.94285714 0.97142857\n",
            " 1.         1.         1.         0.97142857 0.85714286 0.94285714\n",
            " 0.82857143 0.94285714 0.82857143 0.77142857 0.94444444 0.97142857\n",
            " 0.91428571 0.94285714 0.97142857 0.82857143 0.85714286 0.88571429\n",
            " 0.91428571 0.88571429 0.97222222 0.94285714 0.97142857 0.91428571\n",
            " 0.85714286 0.91428571 0.94285714 0.82857143 0.91428571 0.82857143\n",
            " 0.88888889 0.97142857 0.88571429 0.82857143 0.94285714 0.85714286\n",
            " 0.85714286 0.94285714 0.94285714 0.91428571 0.88888889 0.85714286\n",
            " 0.91428571 0.77142857 0.88571429 0.94285714 1.         0.94285714\n",
            " 0.88571429 0.88571429 0.91666667 0.88571429 0.88571429 0.97142857\n",
            " 0.85714286 1.         0.88571429 0.94285714 0.91428571 0.94285714\n",
            " 0.88888889 0.91428571 1.         0.91428571 0.85714286 0.91428571\n",
            " 0.82857143 0.91428571 0.85714286 0.94285714]\n",
            "Accuracy: 90.85% (5.39%)\n",
            "Execution Time: 101.46 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Ionosphere Dataset ---------\n",
            "[0.91666667 0.91428571 0.97142857 0.94285714 0.97142857 0.88571429\n",
            " 0.91428571 1.         0.97142857 0.91428571 0.94444444 0.97142857\n",
            " 1.         0.97142857 0.85714286 0.91428571 0.97142857 0.91428571\n",
            " 0.88571429 0.91428571 0.94444444 0.85714286 0.97142857 0.94285714\n",
            " 0.97142857 0.91428571 0.94285714 0.91428571 0.94285714 0.97142857\n",
            " 1.         1.         1.         1.         0.88571429 0.91428571\n",
            " 0.82857143 0.97142857 0.88571429 0.85714286 0.94444444 1.\n",
            " 1.         0.94285714 0.94285714 0.82857143 0.97142857 1.\n",
            " 0.91428571 0.88571429 0.94444444 0.97142857 0.94285714 0.94285714\n",
            " 0.94285714 0.97142857 0.97142857 0.88571429 0.88571429 0.85714286\n",
            " 0.94444444 0.97142857 0.91428571 0.88571429 0.94285714 0.85714286\n",
            " 0.91428571 0.94285714 1.         1.         0.97222222 0.88571429\n",
            " 0.91428571 0.88571429 0.94285714 0.91428571 0.97142857 0.91428571\n",
            " 0.97142857 0.97142857 0.91666667 0.94285714 0.97142857 0.97142857\n",
            " 0.88571429 1.         0.94285714 0.97142857 0.94285714 0.88571429\n",
            " 0.94444444 0.94285714 1.         0.91428571 0.97142857 0.91428571\n",
            " 0.91428571 0.94285714 0.94285714 0.97142857]\n",
            "Accuracy: 93.82% (4.22%)\n",
            "Execution Time: 175.47 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Ionosphere Dataset ---------\n",
            "[0.94444444 0.91428571 0.94285714 0.91428571 0.94285714 0.88571429\n",
            " 0.94285714 0.97142857 0.88571429 0.88571429 0.91666667 0.94285714\n",
            " 0.94285714 0.97142857 0.85714286 0.91428571 0.97142857 0.94285714\n",
            " 0.88571429 0.97142857 0.86111111 0.8        0.97142857 0.97142857\n",
            " 0.97142857 0.94285714 0.94285714 0.91428571 0.94285714 0.97142857\n",
            " 1.         1.         1.         0.97142857 0.91428571 0.91428571\n",
            " 0.82857143 0.94285714 0.94285714 0.82857143 0.94444444 0.97142857\n",
            " 0.97142857 0.94285714 0.97142857 0.82857143 0.91428571 0.97142857\n",
            " 0.94285714 0.85714286 0.94444444 0.94285714 0.94285714 0.94285714\n",
            " 0.88571429 0.97142857 0.97142857 0.85714286 0.88571429 0.88571429\n",
            " 0.88888889 0.94285714 0.91428571 0.88571429 0.94285714 0.85714286\n",
            " 0.85714286 0.91428571 1.         1.         0.97222222 0.94285714\n",
            " 0.88571429 0.82857143 0.94285714 0.88571429 0.97142857 0.94285714\n",
            " 0.94285714 0.97142857 0.88888889 0.88571429 0.97142857 0.97142857\n",
            " 0.85714286 1.         0.88571429 0.97142857 0.91428571 0.94285714\n",
            " 0.91666667 0.94285714 1.         0.91428571 0.91428571 0.91428571\n",
            " 0.91428571 0.97142857 0.88571429 0.97142857]\n",
            "Accuracy: 92.85% (4.53%)\n",
            "Execution Time: 2.71 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Ionosphere Dataset ---------\n",
            "[0.91666667 0.91428571 0.97142857 0.94285714 0.94285714 0.88571429\n",
            " 0.88571429 1.         0.85714286 0.88571429 0.94444444 0.94285714\n",
            " 1.         0.97142857 0.88571429 0.91428571 0.97142857 0.94285714\n",
            " 0.88571429 0.88571429 0.91666667 0.85714286 1.         0.94285714\n",
            " 0.97142857 0.91428571 0.94285714 0.91428571 0.94285714 0.97142857\n",
            " 1.         1.         0.97142857 1.         0.88571429 0.94285714\n",
            " 0.82857143 0.97142857 0.85714286 0.82857143 0.91666667 0.97142857\n",
            " 1.         0.94285714 0.94285714 0.82857143 0.91428571 0.94285714\n",
            " 0.91428571 0.88571429 0.94444444 0.94285714 0.94285714 0.94285714\n",
            " 0.91428571 0.97142857 0.97142857 0.88571429 0.88571429 0.88571429\n",
            " 0.94444444 0.97142857 0.85714286 0.88571429 0.94285714 0.85714286\n",
            " 0.91428571 0.94285714 1.         0.97142857 0.91666667 0.88571429\n",
            " 0.94285714 0.82857143 0.94285714 0.94285714 0.97142857 0.94285714\n",
            " 0.94285714 1.         0.94444444 0.91428571 0.94285714 0.97142857\n",
            " 0.85714286 1.         0.91428571 0.94285714 0.91428571 0.88571429\n",
            " 0.94444444 0.91428571 1.         0.91428571 0.94285714 0.94285714\n",
            " 0.85714286 0.94285714 0.88571429 0.97142857]\n",
            "Accuracy: 92.96% (4.43%)\n",
            "Execution Time: 4.68 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "ionosphere_scores = []\n",
        "ionosphere_mean = []\n",
        "ionosphere_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "        \n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    ionosphere_scores.append(results)\n",
        "    ionosphere_mean.append(results.mean()*100)\n",
        "    ionosphere_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Ionosphere Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Ionosphere scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(ionosphere_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Ionosphere'] = ionosphere_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "      <td>97.159847</td>\n",
              "      <td>86.347619</td>\n",
              "      <td>93.815873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>78.145238</td>\n",
              "      <td>90.854762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>87.076190</td>\n",
              "      <td>93.815079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>82.361905</td>\n",
              "      <td>92.849206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "      <td>96.792626</td>\n",
              "      <td>83.802381</td>\n",
              "      <td>92.960317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer      Sonar  Ionosphere\n",
              "0   AdaBoost  96.552288      97.159847  86.347619   93.815873\n",
              "1  GradBoost  98.075163      96.646633  78.145238   90.854762\n",
              "2   CatBoost  97.967320      97.378303  87.076190   93.815079\n",
              "3   LightGBM  97.120915      97.334612  82.361905   92.849206\n",
              "4    XGBoost  97.797386      96.792626  83.802381   92.960317"
            ]
          },
          "execution_count": 232,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Ionosphere'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Bupa**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {},
      "outputs": [],
      "source": [
        "bupa_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Bupa\\Bupa.data', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(345, 7)"
            ]
          },
          "execution_count": 248,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bupa_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = bupa_df.iloc[:, :-1]\n",
        "y = bupa_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:06<00:00,  1.33s/trial, best loss: -0.8695652173913043]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 1150.0, 'learning_rate': 0.036566586849114326, 'max_depth': 6.0, 'max_features': 'sqrt', 'min_samples_leaf': 3.0, 'min_samples_split': 2.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:19<00:00,  2.59trial/s, best loss: -0.8115942028985508]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': None, 'n_estimators': 650, 'learning_rate': 0.06856648459048352, 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 0.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [01:01<00:00,  1.23s/trial, best loss: -0.7971014492753623]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 1000, 'learning_rate': 0.07885766008379519, 'min_child_samples': 8, 'max_depth': 2, 'reg_lambda': 2.215819236413667, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 47.34trial/s, best loss: -0.7391304347826086]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'goss', 'num_leaves': 50, 'learning_rate': 0.030743334125495195, 'min_child_samples': 20, 'reg_alpha': 1.2374175460929842, 'reg_lambda': 2.7904588669270254, 'colsample_by_tree': 0.6043264075687251, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:06<00:00,  8.17trial/s, best loss: -0.7971014492753623]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'dart', 'learning_rate': 0.04329402990235971, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 2, 'colsample_bytree': 0.8839036553414338, 'colsample_bylevel': 0.13572776354954574, 'colsample_bynode': 0.32883430164648214, 'reg_alpha': 0.43424116154739917, 'reg_lambda': 1.9753629991445285, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {},
      "outputs": [],
      "source": [
        "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['AdaBoost', 'GradBoost', 'CatBoost', 'LightGBM', 'XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Bupa Dataset ---------\n",
            "[0.77142857 0.74285714 0.71428571 0.71428571 0.77142857 0.88235294\n",
            " 0.67647059 0.64705882 0.67647059 0.61764706 0.65714286 0.57142857\n",
            " 0.68571429 0.8        0.68571429 0.70588235 0.70588235 0.76470588\n",
            " 0.73529412 0.67647059 0.65714286 0.71428571 0.8        0.74285714\n",
            " 0.77142857 0.70588235 0.79411765 0.73529412 0.70588235 0.67647059\n",
            " 0.77142857 0.77142857 0.74285714 0.82857143 0.74285714 0.79411765\n",
            " 0.64705882 0.70588235 0.70588235 0.67647059 0.74285714 0.8\n",
            " 0.65714286 0.68571429 0.77142857 0.73529412 0.73529412 0.64705882\n",
            " 0.67647059 0.70588235 0.65714286 0.77142857 0.74285714 0.62857143\n",
            " 0.77142857 0.70588235 0.70588235 0.70588235 0.58823529 0.58823529\n",
            " 0.74285714 0.77142857 0.6        0.57142857 0.68571429 0.73529412\n",
            " 0.61764706 0.82352941 0.85294118 0.67647059 0.8        0.65714286\n",
            " 0.82857143 0.74285714 0.65714286 0.79411765 0.76470588 0.70588235\n",
            " 0.67647059 0.73529412 0.8        0.68571429 0.71428571 0.82857143\n",
            " 0.68571429 0.70588235 0.76470588 0.64705882 0.73529412 0.64705882\n",
            " 0.82857143 0.74285714 0.74285714 0.6        0.65714286 0.82352941\n",
            " 0.64705882 0.76470588 0.61764706 0.67647059]\n",
            "Accuracy: 71.67% (6.58%)\n",
            "Execution Time: 170.62 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Bupa Dataset ---------\n",
            "[0.88571429 0.71428571 0.71428571 0.71428571 0.74285714 0.82352941\n",
            " 0.67647059 0.70588235 0.67647059 0.67647059 0.6        0.68571429\n",
            " 0.65714286 0.77142857 0.68571429 0.67647059 0.76470588 0.61764706\n",
            " 0.64705882 0.79411765 0.62857143 0.68571429 0.71428571 0.71428571\n",
            " 0.74285714 0.79411765 0.82352941 0.61764706 0.67647059 0.73529412\n",
            " 0.71428571 0.8        0.71428571 0.8        0.77142857 0.64705882\n",
            " 0.64705882 0.70588235 0.70588235 0.58823529 0.68571429 0.71428571\n",
            " 0.74285714 0.68571429 0.74285714 0.76470588 0.64705882 0.61764706\n",
            " 0.64705882 0.70588235 0.68571429 0.8        0.65714286 0.65714286\n",
            " 0.77142857 0.67647059 0.67647059 0.70588235 0.73529412 0.64705882\n",
            " 0.62857143 0.74285714 0.62857143 0.62857143 0.6        0.70588235\n",
            " 0.44117647 0.91176471 0.70588235 0.70588235 0.82857143 0.65714286\n",
            " 0.68571429 0.71428571 0.68571429 0.73529412 0.73529412 0.61764706\n",
            " 0.61764706 0.70588235 0.71428571 0.71428571 0.65714286 0.8\n",
            " 0.77142857 0.64705882 0.55882353 0.61764706 0.64705882 0.61764706\n",
            " 0.68571429 0.74285714 0.8        0.48571429 0.6        0.79411765\n",
            " 0.73529412 0.64705882 0.67647059 0.76470588]\n",
            "Accuracy: 69.78% (7.34%)\n",
            "Execution Time: 50.94 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Bupa Dataset ---------\n",
            "[0.85714286 0.77142857 0.71428571 0.68571429 0.65714286 0.82352941\n",
            " 0.58823529 0.55882353 0.61764706 0.70588235 0.74285714 0.6\n",
            " 0.62857143 0.71428571 0.65714286 0.64705882 0.73529412 0.61764706\n",
            " 0.73529412 0.73529412 0.65714286 0.62857143 0.65714286 0.74285714\n",
            " 0.68571429 0.67647059 0.79411765 0.64705882 0.73529412 0.70588235\n",
            " 0.77142857 0.68571429 0.68571429 0.77142857 0.82857143 0.88235294\n",
            " 0.61764706 0.64705882 0.70588235 0.64705882 0.77142857 0.68571429\n",
            " 0.74285714 0.65714286 0.77142857 0.67647059 0.70588235 0.61764706\n",
            " 0.70588235 0.70588235 0.71428571 0.68571429 0.68571429 0.65714286\n",
            " 0.8        0.73529412 0.70588235 0.76470588 0.67647059 0.64705882\n",
            " 0.74285714 0.68571429 0.62857143 0.6        0.77142857 0.73529412\n",
            " 0.52941176 0.82352941 0.85294118 0.61764706 0.77142857 0.57142857\n",
            " 0.71428571 0.82857143 0.71428571 0.79411765 0.73529412 0.58823529\n",
            " 0.64705882 0.64705882 0.74285714 0.74285714 0.6        0.77142857\n",
            " 0.65714286 0.61764706 0.76470588 0.64705882 0.64705882 0.70588235\n",
            " 0.65714286 0.65714286 0.74285714 0.6        0.71428571 0.85294118\n",
            " 0.64705882 0.64705882 0.55882353 0.79411765]\n",
            "Accuracy: 69.85% (7.31%)\n",
            "Execution Time: 97.11 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Bupa Dataset ---------\n",
            "[0.8        0.74285714 0.65714286 0.65714286 0.6        0.73529412\n",
            " 0.55882353 0.73529412 0.58823529 0.76470588 0.6        0.68571429\n",
            " 0.6        0.8        0.77142857 0.73529412 0.79411765 0.64705882\n",
            " 0.73529412 0.73529412 0.6        0.54285714 0.68571429 0.68571429\n",
            " 0.82857143 0.82352941 0.85294118 0.55882353 0.76470588 0.67647059\n",
            " 0.71428571 0.68571429 0.71428571 0.82857143 0.71428571 0.76470588\n",
            " 0.52941176 0.73529412 0.79411765 0.64705882 0.62857143 0.77142857\n",
            " 0.71428571 0.68571429 0.77142857 0.73529412 0.70588235 0.55882353\n",
            " 0.58823529 0.70588235 0.65714286 0.77142857 0.65714286 0.54285714\n",
            " 0.82857143 0.79411765 0.82352941 0.76470588 0.70588235 0.64705882\n",
            " 0.6        0.8        0.71428571 0.74285714 0.68571429 0.58823529\n",
            " 0.61764706 0.85294118 0.67647059 0.61764706 0.68571429 0.68571429\n",
            " 0.68571429 0.71428571 0.74285714 0.79411765 0.70588235 0.58823529\n",
            " 0.58823529 0.79411765 0.74285714 0.74285714 0.8        0.8\n",
            " 0.62857143 0.70588235 0.58823529 0.76470588 0.58823529 0.64705882\n",
            " 0.62857143 0.68571429 0.74285714 0.51428571 0.71428571 0.73529412\n",
            " 0.70588235 0.67647059 0.76470588 0.58823529]\n",
            "Accuracy: 69.79% (8.14%)\n",
            "Execution Time: 1.67 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Bupa Dataset ---------\n",
            "[0.91428571 0.8        0.71428571 0.71428571 0.62857143 0.79411765\n",
            " 0.64705882 0.76470588 0.70588235 0.73529412 0.71428571 0.74285714\n",
            " 0.65714286 0.82857143 0.77142857 0.70588235 0.85294118 0.67647059\n",
            " 0.67647059 0.79411765 0.77142857 0.68571429 0.82857143 0.71428571\n",
            " 0.85714286 0.76470588 0.79411765 0.61764706 0.67647059 0.73529412\n",
            " 0.77142857 0.77142857 0.71428571 0.85714286 0.77142857 0.76470588\n",
            " 0.61764706 0.76470588 0.73529412 0.73529412 0.8        0.74285714\n",
            " 0.71428571 0.74285714 0.71428571 0.85294118 0.67647059 0.67647059\n",
            " 0.73529412 0.76470588 0.77142857 0.8        0.74285714 0.65714286\n",
            " 0.8        0.76470588 0.79411765 0.70588235 0.76470588 0.70588235\n",
            " 0.74285714 0.85714286 0.71428571 0.71428571 0.71428571 0.70588235\n",
            " 0.61764706 0.85294118 0.88235294 0.67647059 0.82857143 0.65714286\n",
            " 0.82857143 0.82857143 0.74285714 0.76470588 0.73529412 0.61764706\n",
            " 0.61764706 0.70588235 0.77142857 0.74285714 0.8        0.82857143\n",
            " 0.74285714 0.70588235 0.79411765 0.70588235 0.61764706 0.76470588\n",
            " 0.68571429 0.77142857 0.85714286 0.6        0.68571429 0.88235294\n",
            " 0.76470588 0.76470588 0.76470588 0.70588235]\n",
            "Accuracy: 74.48% (6.77%)\n",
            "Execution Time: 16.04 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "bupa_scores = []\n",
        "bupa_mean = []\n",
        "bupa_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    bupa_scores.append(results)\n",
        "    bupa_mean.append(results.mean()*100)\n",
        "    bupa_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Bupa Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Bupa scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(bupa_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Bupa'] = bupa_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>TicTacToe</th>\n",
              "      <th>Bupa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "      <td>97.159847</td>\n",
              "      <td>86.347619</td>\n",
              "      <td>93.815873</td>\n",
              "      <td>81.054167</td>\n",
              "      <td>71.669748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>78.145238</td>\n",
              "      <td>90.854762</td>\n",
              "      <td>82.224232</td>\n",
              "      <td>69.783193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>87.076190</td>\n",
              "      <td>93.815079</td>\n",
              "      <td>72.318311</td>\n",
              "      <td>69.846218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>82.361905</td>\n",
              "      <td>92.849206</td>\n",
              "      <td>61.814474</td>\n",
              "      <td>69.794118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "      <td>96.792626</td>\n",
              "      <td>83.802381</td>\n",
              "      <td>92.960317</td>\n",
              "      <td>65.721053</td>\n",
              "      <td>74.475630</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer      Sonar  Ionosphere  TicTacToe  \\\n",
              "0   AdaBoost  96.552288      97.159847  86.347619   93.815873  81.054167   \n",
              "1  GradBoost  98.075163      96.646633  78.145238   90.854762  82.224232   \n",
              "2   CatBoost  97.967320      97.378303  87.076190   93.815079  72.318311   \n",
              "3   LightGBM  97.120915      97.334612  82.361905   92.849206  61.814474   \n",
              "4    XGBoost  97.797386      96.792626  83.802381   92.960317  65.721053   \n",
              "\n",
              "        Bupa  \n",
              "0  71.669748  \n",
              "1  69.783193  \n",
              "2  69.846218  \n",
              "3  69.794118  \n",
              "4  74.475630  "
            ]
          },
          "execution_count": 258,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Bupa'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Pima**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {},
      "outputs": [],
      "source": [
        "pima_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Pima\\Diabetes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = pima_df.iloc[:, :-1]\n",
        "y = pima_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:36<00:00,  1.94s/trial, best loss: -0.7857142857142857]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 1500.0, 'learning_rate': 0.010436960322525368, 'max_depth': 1.0, 'max_features': None, 'min_samples_leaf': 5.0, 'min_samples_split': 4.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:25<00:00,  1.98trial/s, best loss: -0.7922077922077922]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': 'sqrt', 'n_estimators': 200, 'learning_rate': 0.05871692740564188, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 9, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 2.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [01:12<00:00,  1.45s/trial, best loss: -0.7792207792207793]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 50, 'learning_rate': 0.010922414344918462, 'min_child_samples': 10, 'max_depth': 4, 'reg_lambda': 4.685483905860218, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 34.52trial/s, best loss: -0.7922077922077922]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'dart', 'num_leaves': 95, 'learning_rate': 0.04955748086609083, 'min_child_samples': 140, 'reg_alpha': 1.1745781431363478, 'reg_lambda': 1.5581466068782919, 'colsample_by_tree': 0.9952093023356591, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:06<00:00,  8.04trial/s, best loss: -0.7987012987012987]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'gbtree', 'learning_rate': 0.011777426690454684, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'colsample_bytree': 0.6642423404208758, 'colsample_bylevel': 0.8389604376670141, 'colsample_bynode': 0.46801910869053165, 'reg_alpha': 1.3842922617481603, 'reg_lambda': 0.25127542856871243, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [],
      "source": [
        "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['AdaBoost', 'GradBoost', 'CatBoost', 'LightGBM', 'XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Pima Dataset ---------\n",
            "[0.80519481 0.76623377 0.75324675 0.85714286 0.81818182 0.74025974\n",
            " 0.74025974 0.71428571 0.75       0.69736842 0.77922078 0.7012987\n",
            " 0.75324675 0.74025974 0.74025974 0.84415584 0.80519481 0.72727273\n",
            " 0.73684211 0.82894737 0.81818182 0.74025974 0.77922078 0.68831169\n",
            " 0.75324675 0.76623377 0.74025974 0.81818182 0.71052632 0.69736842\n",
            " 0.80519481 0.75324675 0.76623377 0.77922078 0.79220779 0.74025974\n",
            " 0.77922078 0.80519481 0.76315789 0.67105263 0.76623377 0.77922078\n",
            " 0.84415584 0.81818182 0.80519481 0.75324675 0.68831169 0.77922078\n",
            " 0.73684211 0.69736842 0.76623377 0.77922078 0.76623377 0.72727273\n",
            " 0.76623377 0.80519481 0.71428571 0.74025974 0.76315789 0.72368421\n",
            " 0.76623377 0.74025974 0.75324675 0.76623377 0.71428571 0.83116883\n",
            " 0.74025974 0.74025974 0.76315789 0.75       0.80519481 0.75324675\n",
            " 0.72727273 0.84415584 0.74025974 0.74025974 0.77922078 0.72727273\n",
            " 0.75       0.73684211 0.72727273 0.72727273 0.79220779 0.79220779\n",
            " 0.77922078 0.81818182 0.74025974 0.67532468 0.76315789 0.81578947\n",
            " 0.79220779 0.76623377 0.80519481 0.77922078 0.75324675 0.77922078\n",
            " 0.76623377 0.71428571 0.69736842 0.76315789]\n",
            "Accuracy: 76.10% (3.95%)\n",
            "Execution Time: 242.87 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Pima Dataset ---------\n",
            "[0.74025974 0.77922078 0.77922078 0.81818182 0.79220779 0.76623377\n",
            " 0.72727273 0.74025974 0.71052632 0.75       0.76623377 0.7012987\n",
            " 0.80519481 0.75324675 0.72727273 0.85714286 0.83116883 0.76623377\n",
            " 0.71052632 0.82894737 0.75324675 0.74025974 0.80519481 0.7012987\n",
            " 0.79220779 0.77922078 0.75324675 0.83116883 0.73684211 0.75\n",
            " 0.79220779 0.75324675 0.76623377 0.80519481 0.81818182 0.72727273\n",
            " 0.81818182 0.79220779 0.73684211 0.64473684 0.74025974 0.77922078\n",
            " 0.83116883 0.79220779 0.80519481 0.76623377 0.7012987  0.83116883\n",
            " 0.77631579 0.69736842 0.79220779 0.75324675 0.76623377 0.74025974\n",
            " 0.77922078 0.81818182 0.76623377 0.67532468 0.72368421 0.75\n",
            " 0.74025974 0.71428571 0.72727273 0.81818182 0.7012987  0.84415584\n",
            " 0.74025974 0.72727273 0.77631579 0.76315789 0.79220779 0.75324675\n",
            " 0.72727273 0.83116883 0.76623377 0.77922078 0.76623377 0.75324675\n",
            " 0.69736842 0.80263158 0.75324675 0.7012987  0.81818182 0.79220779\n",
            " 0.81818182 0.77922078 0.75324675 0.66233766 0.71052632 0.80263158\n",
            " 0.77922078 0.79220779 0.79220779 0.76623377 0.74025974 0.79220779\n",
            " 0.76623377 0.75324675 0.73684211 0.76315789]\n",
            "Accuracy: 76.43% (4.16%)\n",
            "Execution Time: 10.41 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Pima Dataset ---------\n",
            "[0.77922078 0.74025974 0.76623377 0.84415584 0.80519481 0.72727273\n",
            " 0.72727273 0.74025974 0.69736842 0.77631579 0.72727273 0.68831169\n",
            " 0.79220779 0.71428571 0.67532468 0.87012987 0.83116883 0.74025974\n",
            " 0.67105263 0.78947368 0.72727273 0.74025974 0.80519481 0.68831169\n",
            " 0.77922078 0.75324675 0.80519481 0.81818182 0.72368421 0.73684211\n",
            " 0.80519481 0.76623377 0.75324675 0.76623377 0.79220779 0.7012987\n",
            " 0.77922078 0.77922078 0.75       0.64473684 0.71428571 0.81818182\n",
            " 0.79220779 0.77922078 0.76623377 0.76623377 0.66233766 0.77922078\n",
            " 0.76315789 0.72368421 0.83116883 0.76623377 0.72727273 0.71428571\n",
            " 0.75324675 0.81818182 0.75324675 0.72727273 0.73684211 0.76315789\n",
            " 0.74025974 0.76623377 0.72727273 0.77922078 0.74025974 0.85714286\n",
            " 0.75324675 0.67532468 0.76315789 0.73684211 0.76623377 0.76623377\n",
            " 0.74025974 0.80519481 0.72727273 0.75324675 0.76623377 0.79220779\n",
            " 0.71052632 0.73684211 0.76623377 0.71428571 0.79220779 0.84415584\n",
            " 0.77922078 0.75324675 0.75324675 0.67532468 0.64473684 0.81578947\n",
            " 0.72727273 0.74025974 0.75324675 0.71428571 0.76623377 0.80519481\n",
            " 0.81818182 0.74025974 0.71052632 0.73684211]\n",
            "Accuracy: 75.53% (4.47%)\n",
            "Execution Time: 14.65 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Pima Dataset ---------\n",
            "[0.79220779 0.79220779 0.74025974 0.81818182 0.81818182 0.74025974\n",
            " 0.80519481 0.72727273 0.65789474 0.68421053 0.68831169 0.71428571\n",
            " 0.81818182 0.74025974 0.77922078 0.85714286 0.72727273 0.77922078\n",
            " 0.71052632 0.85526316 0.79220779 0.75324675 0.77922078 0.62337662\n",
            " 0.80519481 0.74025974 0.71428571 0.79220779 0.75       0.72368421\n",
            " 0.79220779 0.74025974 0.80519481 0.79220779 0.76623377 0.74025974\n",
            " 0.76623377 0.77922078 0.76315789 0.69736842 0.72727273 0.84415584\n",
            " 0.79220779 0.77922078 0.76623377 0.83116883 0.63636364 0.77922078\n",
            " 0.75       0.75       0.80519481 0.77922078 0.76623377 0.74025974\n",
            " 0.79220779 0.77922078 0.66233766 0.68831169 0.80263158 0.78947368\n",
            " 0.79220779 0.83116883 0.71428571 0.81818182 0.7012987  0.79220779\n",
            " 0.75324675 0.74025974 0.75       0.75       0.83116883 0.74025974\n",
            " 0.68831169 0.79220779 0.68831169 0.76623377 0.76623377 0.77922078\n",
            " 0.72368421 0.75       0.71428571 0.76623377 0.76623377 0.77922078\n",
            " 0.74025974 0.77922078 0.75324675 0.7012987  0.71052632 0.78947368\n",
            " 0.75324675 0.79220779 0.79220779 0.71428571 0.79220779 0.72727273\n",
            " 0.76623377 0.80519481 0.75       0.73684211]\n",
            "Accuracy: 75.92% (4.53%)\n",
            "Execution Time: 2.37 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Pima Dataset ---------\n",
            "[0.76623377 0.74025974 0.74025974 0.84415584 0.81818182 0.76623377\n",
            " 0.71428571 0.72727273 0.71052632 0.71052632 0.71428571 0.68831169\n",
            " 0.77922078 0.76623377 0.67532468 0.84415584 0.79220779 0.71428571\n",
            " 0.72368421 0.77631579 0.76623377 0.75324675 0.77922078 0.72727273\n",
            " 0.77922078 0.7012987  0.72727273 0.84415584 0.72368421 0.72368421\n",
            " 0.72727273 0.77922078 0.77922078 0.79220779 0.77922078 0.71428571\n",
            " 0.77922078 0.75324675 0.72368421 0.69736842 0.76623377 0.72727273\n",
            " 0.80519481 0.77922078 0.74025974 0.76623377 0.72727273 0.74025974\n",
            " 0.76315789 0.71052632 0.76623377 0.72727273 0.76623377 0.75324675\n",
            " 0.74025974 0.77922078 0.75324675 0.75324675 0.72368421 0.76315789\n",
            " 0.77922078 0.75324675 0.77922078 0.75324675 0.72727273 0.81818182\n",
            " 0.74025974 0.74025974 0.77631579 0.75       0.80519481 0.72727273\n",
            " 0.76623377 0.79220779 0.74025974 0.72727273 0.80519481 0.74025974\n",
            " 0.72368421 0.72368421 0.7012987  0.68831169 0.80519481 0.77922078\n",
            " 0.81818182 0.77922078 0.74025974 0.67532468 0.69736842 0.82894737\n",
            " 0.76623377 0.75324675 0.77922078 0.75324675 0.74025974 0.76623377\n",
            " 0.76623377 0.74025974 0.71052632 0.76315789]\n",
            "Accuracy: 75.33% (3.63%)\n",
            "Execution Time: 3.71 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "pima_scores = []\n",
        "pima_mean = []\n",
        "pima_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()     \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    pima_scores.append(results)\n",
        "    pima_mean.append(results.mean()*100)\n",
        "    pima_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Pima Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Pima scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(pima_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Pima'] = pima_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>TicTacToe</th>\n",
              "      <th>Bupa</th>\n",
              "      <th>Pima</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "      <td>97.159847</td>\n",
              "      <td>86.347619</td>\n",
              "      <td>93.815873</td>\n",
              "      <td>81.054167</td>\n",
              "      <td>71.669748</td>\n",
              "      <td>76.101504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>78.145238</td>\n",
              "      <td>90.854762</td>\n",
              "      <td>82.224232</td>\n",
              "      <td>69.783193</td>\n",
              "      <td>76.426863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>87.076190</td>\n",
              "      <td>93.815079</td>\n",
              "      <td>72.318311</td>\n",
              "      <td>69.846218</td>\n",
              "      <td>75.527683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>82.361905</td>\n",
              "      <td>92.849206</td>\n",
              "      <td>61.814474</td>\n",
              "      <td>69.794118</td>\n",
              "      <td>75.920711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "      <td>96.792626</td>\n",
              "      <td>83.802381</td>\n",
              "      <td>92.960317</td>\n",
              "      <td>65.721053</td>\n",
              "      <td>74.475630</td>\n",
              "      <td>75.334074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer      Sonar  Ionosphere  TicTacToe  \\\n",
              "0   AdaBoost  96.552288      97.159847  86.347619   93.815873  81.054167   \n",
              "1  GradBoost  98.075163      96.646633  78.145238   90.854762  82.224232   \n",
              "2   CatBoost  97.967320      97.378303  87.076190   93.815079  72.318311   \n",
              "3   LightGBM  97.120915      97.334612  82.361905   92.849206  61.814474   \n",
              "4    XGBoost  97.797386      96.792626  83.802381   92.960317  65.721053   \n",
              "\n",
              "        Bupa       Pima  \n",
              "0  71.669748  76.101504  \n",
              "1  69.783193  76.426863  \n",
              "2  69.846218  75.527683  \n",
              "3  69.794118  75.920711  \n",
              "4  74.475630  75.334074  "
            ]
          },
          "execution_count": 269,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Pima'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Heart**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {},
      "outputs": [],
      "source": [
        "heart_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Heart\\Heart.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = heart_df.iloc[:, :-1]\n",
        "y = heart_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:54<00:00,  1.09s/trial, best loss: -0.8888888888888888]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 600.0, 'learning_rate': 0.03167747886969513, 'max_depth': 1.0, 'max_features': None, 'min_samples_leaf': 3.0, 'min_samples_split': 4.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:20<00:00,  2.43trial/s, best loss: -0.8703703703703703]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': None, 'n_estimators': 850, 'learning_rate': 0.053611707225416305, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 0.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:45<00:00,  1.09trial/s, best loss: -0.9074074074074074]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 100, 'learning_rate': 0.06012626813664707, 'min_child_samples': 6, 'max_depth': 5, 'reg_lambda': 3.702478129069811, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:00<00:00, 54.93trial/s, best loss: -0.9074074074074074]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'goss', 'num_leaves': 50, 'learning_rate': 0.08551965156387403, 'min_child_samples': 80, 'reg_alpha': 1.1979901584013886, 'reg_lambda': 1.2997746583338796, 'colsample_by_tree': 0.9107668642217731, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:04<00:00, 11.03trial/s, best loss: -0.9074074074074074]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'gbtree', 'learning_rate': 0.011008587946644721, 'gamma': 0, 'max_depth': 4, 'min_child_weight': 1, 'colsample_bytree': 0.8371384244356821, 'colsample_bylevel': 0.8446187355631347, 'colsample_bynode': 0.4599194143427724, 'reg_alpha': 1.5147748652336739, 'reg_lambda': 1.4296997246408663, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {},
      "outputs": [],
      "source": [
        "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['AdaBoost', 'GradBoost', 'CatBoost', 'LightGBM', 'XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Heart Dataset ---------\n",
            "[0.81481481 0.81481481 0.81481481 0.96296296 0.77777778 0.77777778\n",
            " 0.88888889 0.77777778 0.77777778 0.96296296 0.85185185 0.77777778\n",
            " 0.88888889 0.81481481 0.85185185 0.85185185 0.92592593 0.62962963\n",
            " 0.81481481 0.85185185 0.77777778 0.96296296 0.81481481 0.77777778\n",
            " 0.92592593 0.81481481 0.85185185 0.74074074 0.81481481 0.88888889\n",
            " 0.81481481 0.88888889 0.85185185 0.81481481 0.81481481 0.88888889\n",
            " 0.81481481 0.77777778 0.85185185 0.77777778 0.81481481 0.96296296\n",
            " 0.74074074 0.85185185 0.7037037  0.88888889 0.77777778 0.88888889\n",
            " 0.77777778 0.88888889 0.66666667 0.81481481 0.85185185 0.81481481\n",
            " 0.77777778 0.88888889 0.81481481 0.81481481 0.81481481 0.88888889\n",
            " 0.85185185 0.88888889 0.85185185 0.88888889 0.85185185 0.77777778\n",
            " 0.85185185 0.74074074 0.77777778 0.92592593 0.88888889 0.85185185\n",
            " 0.77777778 0.85185185 0.77777778 0.81481481 0.77777778 0.85185185\n",
            " 0.81481481 0.85185185 0.92592593 0.85185185 0.85185185 0.85185185\n",
            " 0.74074074 0.85185185 0.85185185 0.7037037  0.88888889 0.85185185\n",
            " 0.85185185 0.88888889 0.88888889 0.81481481 0.88888889 0.7037037\n",
            " 0.81481481 0.85185185 0.77777778 0.85185185]\n",
            "Accuracy: 83.11% (6.16%)\n",
            "Execution Time: 72.53 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Heart Dataset ---------\n",
            "[0.74074074 0.85185185 0.81481481 0.88888889 0.77777778 0.7037037\n",
            " 0.77777778 0.7037037  0.85185185 0.92592593 0.85185185 0.81481481\n",
            " 0.88888889 0.88888889 0.81481481 0.85185185 0.81481481 0.74074074\n",
            " 0.85185185 0.81481481 0.74074074 0.92592593 0.77777778 0.7037037\n",
            " 0.85185185 0.77777778 0.81481481 0.74074074 0.74074074 0.88888889\n",
            " 0.85185185 0.88888889 0.74074074 0.81481481 0.74074074 0.88888889\n",
            " 0.74074074 0.77777778 0.77777778 0.74074074 0.7037037  0.88888889\n",
            " 0.74074074 0.77777778 0.85185185 0.77777778 0.74074074 0.81481481\n",
            " 0.81481481 0.85185185 0.59259259 0.74074074 0.77777778 0.85185185\n",
            " 0.74074074 0.88888889 0.85185185 0.85185185 0.77777778 0.77777778\n",
            " 0.88888889 0.81481481 0.81481481 0.88888889 0.77777778 0.74074074\n",
            " 0.77777778 0.77777778 0.81481481 0.88888889 0.92592593 0.85185185\n",
            " 0.74074074 0.77777778 0.88888889 0.7037037  0.77777778 0.77777778\n",
            " 0.77777778 0.88888889 0.81481481 0.85185185 0.74074074 0.81481481\n",
            " 0.77777778 0.85185185 0.85185185 0.7037037  0.88888889 0.81481481\n",
            " 0.81481481 0.74074074 0.88888889 0.74074074 0.85185185 0.74074074\n",
            " 0.85185185 0.74074074 0.74074074 0.81481481]\n",
            "Accuracy: 80.44% (6.31%)\n",
            "Execution Time: 65.33 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Heart Dataset ---------\n",
            "[0.81481481 0.85185185 0.85185185 0.96296296 0.77777778 0.81481481\n",
            " 0.85185185 0.74074074 0.81481481 0.96296296 0.85185185 0.81481481\n",
            " 0.85185185 0.77777778 0.85185185 0.88888889 0.92592593 0.66666667\n",
            " 0.85185185 0.81481481 0.7037037  0.96296296 0.81481481 0.81481481\n",
            " 0.96296296 0.81481481 0.85185185 0.81481481 0.77777778 0.96296296\n",
            " 0.88888889 0.92592593 0.85185185 0.81481481 0.77777778 0.88888889\n",
            " 0.77777778 0.81481481 0.81481481 0.81481481 0.81481481 0.92592593\n",
            " 0.81481481 0.85185185 0.74074074 0.81481481 0.81481481 0.88888889\n",
            " 0.88888889 0.88888889 0.7037037  0.81481481 0.88888889 0.88888889\n",
            " 0.85185185 0.88888889 0.85185185 0.77777778 0.85185185 0.88888889\n",
            " 0.85185185 0.88888889 0.81481481 0.88888889 0.88888889 0.77777778\n",
            " 0.85185185 0.7037037  0.88888889 0.92592593 0.88888889 0.77777778\n",
            " 0.81481481 0.88888889 0.92592593 0.81481481 0.77777778 0.92592593\n",
            " 0.81481481 0.88888889 0.96296296 0.85185185 0.92592593 0.85185185\n",
            " 0.77777778 0.85185185 0.81481481 0.77777778 0.88888889 0.85185185\n",
            " 0.81481481 0.92592593 0.85185185 0.81481481 0.85185185 0.81481481\n",
            " 0.81481481 0.85185185 0.81481481 0.92592593]\n",
            "Accuracy: 84.48% (6.06%)\n",
            "Execution Time: 16.62 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Heart Dataset ---------\n",
            "[0.92592593 0.88888889 0.85185185 0.88888889 0.74074074 0.77777778\n",
            " 0.81481481 0.62962963 0.66666667 0.92592593 0.81481481 0.88888889\n",
            " 1.         0.81481481 0.81481481 0.77777778 0.96296296 0.74074074\n",
            " 0.85185185 0.81481481 0.7037037  0.96296296 0.81481481 0.77777778\n",
            " 0.96296296 0.81481481 0.81481481 0.81481481 0.74074074 0.92592593\n",
            " 0.92592593 0.81481481 0.85185185 0.85185185 0.74074074 0.96296296\n",
            " 0.85185185 0.85185185 0.77777778 0.81481481 0.74074074 0.88888889\n",
            " 0.85185185 0.66666667 0.74074074 0.74074074 0.7037037  0.85185185\n",
            " 0.85185185 0.85185185 0.62962963 0.88888889 0.88888889 0.88888889\n",
            " 0.81481481 0.88888889 0.96296296 0.81481481 0.85185185 0.77777778\n",
            " 0.88888889 0.77777778 0.85185185 0.88888889 0.81481481 0.85185185\n",
            " 0.85185185 0.66666667 0.88888889 0.85185185 0.92592593 0.92592593\n",
            " 0.77777778 0.85185185 0.88888889 0.85185185 0.77777778 0.81481481\n",
            " 0.81481481 0.81481481 0.88888889 0.81481481 0.7037037  0.85185185\n",
            " 0.85185185 0.85185185 0.74074074 0.7037037  0.81481481 0.74074074\n",
            " 0.81481481 0.88888889 0.85185185 0.77777778 0.88888889 0.81481481\n",
            " 0.81481481 0.77777778 0.88888889 0.92592593]\n",
            "Accuracy: 82.85% (7.58%)\n",
            "Execution Time: 0.79 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Heart Dataset ---------\n",
            "[0.81481481 0.81481481 0.85185185 0.96296296 0.81481481 0.77777778\n",
            " 0.81481481 0.74074074 0.81481481 0.96296296 0.85185185 0.77777778\n",
            " 0.88888889 0.77777778 0.88888889 0.85185185 0.96296296 0.7037037\n",
            " 0.81481481 0.88888889 0.77777778 0.96296296 0.77777778 0.85185185\n",
            " 0.96296296 0.74074074 0.88888889 0.77777778 0.81481481 0.96296296\n",
            " 0.88888889 0.92592593 0.88888889 0.81481481 0.74074074 0.88888889\n",
            " 0.77777778 0.85185185 0.85185185 0.81481481 0.77777778 0.92592593\n",
            " 0.85185185 0.96296296 0.74074074 0.81481481 0.81481481 0.92592593\n",
            " 0.85185185 0.85185185 0.7037037  0.81481481 0.85185185 0.92592593\n",
            " 0.85185185 0.85185185 0.85185185 0.81481481 0.88888889 0.92592593\n",
            " 0.81481481 0.85185185 0.88888889 0.88888889 0.88888889 0.81481481\n",
            " 0.81481481 0.7037037  0.88888889 0.92592593 0.88888889 0.81481481\n",
            " 0.77777778 0.85185185 0.88888889 0.81481481 0.77777778 0.92592593\n",
            " 0.81481481 0.92592593 0.92592593 0.85185185 0.77777778 0.85185185\n",
            " 0.74074074 0.85185185 0.81481481 0.77777778 0.85185185 0.85185185\n",
            " 0.85185185 0.92592593 0.88888889 0.81481481 0.88888889 0.77777778\n",
            " 0.81481481 0.81481481 0.85185185 0.88888889]\n",
            "Accuracy: 84.52% (6.25%)\n",
            "Execution Time: 3.51 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "heart_scores = []\n",
        "heart_mean = []\n",
        "heart_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    heart_scores.append(results)\n",
        "    heart_mean.append(results.mean()*100)\n",
        "    heart_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Heart Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Heart scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(heart_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Heart'] = heart_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>TicTacToe</th>\n",
              "      <th>Bupa</th>\n",
              "      <th>Pima</th>\n",
              "      <th>Heart</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "      <td>97.159847</td>\n",
              "      <td>86.347619</td>\n",
              "      <td>93.815873</td>\n",
              "      <td>81.054167</td>\n",
              "      <td>71.669748</td>\n",
              "      <td>76.101504</td>\n",
              "      <td>83.111111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>78.145238</td>\n",
              "      <td>90.854762</td>\n",
              "      <td>82.224232</td>\n",
              "      <td>69.783193</td>\n",
              "      <td>76.426863</td>\n",
              "      <td>80.444444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>87.076190</td>\n",
              "      <td>93.815079</td>\n",
              "      <td>72.318311</td>\n",
              "      <td>69.846218</td>\n",
              "      <td>75.527683</td>\n",
              "      <td>84.481481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>82.361905</td>\n",
              "      <td>92.849206</td>\n",
              "      <td>61.814474</td>\n",
              "      <td>69.794118</td>\n",
              "      <td>75.920711</td>\n",
              "      <td>82.851852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "      <td>96.792626</td>\n",
              "      <td>83.802381</td>\n",
              "      <td>92.960317</td>\n",
              "      <td>65.721053</td>\n",
              "      <td>74.475630</td>\n",
              "      <td>75.334074</td>\n",
              "      <td>84.518519</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer      Sonar  Ionosphere  TicTacToe  \\\n",
              "0   AdaBoost  96.552288      97.159847  86.347619   93.815873  81.054167   \n",
              "1  GradBoost  98.075163      96.646633  78.145238   90.854762  82.224232   \n",
              "2   CatBoost  97.967320      97.378303  87.076190   93.815079  72.318311   \n",
              "3   LightGBM  97.120915      97.334612  82.361905   92.849206  61.814474   \n",
              "4    XGBoost  97.797386      96.792626  83.802381   92.960317  65.721053   \n",
              "\n",
              "        Bupa       Pima      Heart  \n",
              "0  71.669748  76.101504  83.111111  \n",
              "1  69.783193  76.426863  80.444444  \n",
              "2  69.846218  75.527683  84.481481  \n",
              "3  69.794118  75.920711  82.851852  \n",
              "4  74.475630  75.334074  84.518519  "
            ]
          },
          "execution_count": 281,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Heart'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Liver**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {},
      "outputs": [],
      "source": [
        "liver_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Liver\\Bupa.data', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = liver_df.iloc[:, :-1]\n",
        "y = liver_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:54<00:00,  1.10s/trial, best loss: -0.8888888888888888]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 600.0, 'learning_rate': 0.03167747886969513, 'max_depth': 1.0, 'max_features': None, 'min_samples_leaf': 3.0, 'min_samples_split': 4.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:20<00:00,  2.44trial/s, best loss: -0.8703703703703703]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': None, 'n_estimators': 850, 'learning_rate': 0.053611707225416305, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 0.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:45<00:00,  1.10trial/s, best loss: -0.9074074074074074]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 100, 'learning_rate': 0.06012626813664707, 'min_child_samples': 6, 'max_depth': 5, 'reg_lambda': 3.702478129069811, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 49.49trial/s, best loss: -0.9074074074074074]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'goss', 'num_leaves': 50, 'learning_rate': 0.08551965156387403, 'min_child_samples': 80, 'reg_alpha': 1.1979901584013886, 'reg_lambda': 1.2997746583338796, 'colsample_by_tree': 0.9107668642217731, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:04<00:00, 10.52trial/s, best loss: -0.9074074074074074]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'gbtree', 'learning_rate': 0.011008587946644721, 'gamma': 0, 'max_depth': 4, 'min_child_weight': 1, 'colsample_bytree': 0.8371384244356821, 'colsample_bylevel': 0.8446187355631347, 'colsample_bynode': 0.4599194143427724, 'reg_alpha': 1.5147748652336739, 'reg_lambda': 1.4296997246408663, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {},
      "outputs": [],
      "source": [
        "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['AdaBoost', 'GradBoost', 'CatBoost', 'LightGBM', 'XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Liver Dataset ---------\n",
            "[0.85714286 0.8        0.65714286 0.77142857 0.62857143 0.85294118\n",
            " 0.82352941 0.64705882 0.61764706 0.79411765 0.8        0.65714286\n",
            " 0.62857143 0.82857143 0.8        0.73529412 0.79411765 0.73529412\n",
            " 0.76470588 0.76470588 0.62857143 0.62857143 0.77142857 0.82857143\n",
            " 0.85714286 0.85294118 0.82352941 0.67647059 0.70588235 0.76470588\n",
            " 0.8        0.8        0.74285714 0.85714286 0.77142857 0.76470588\n",
            " 0.64705882 0.70588235 0.73529412 0.64705882 0.74285714 0.77142857\n",
            " 0.68571429 0.57142857 0.82857143 0.79411765 0.76470588 0.64705882\n",
            " 0.73529412 0.76470588 0.71428571 0.77142857 0.74285714 0.65714286\n",
            " 0.77142857 0.76470588 0.76470588 0.82352941 0.70588235 0.70588235\n",
            " 0.68571429 0.88571429 0.65714286 0.68571429 0.74285714 0.67647059\n",
            " 0.70588235 0.82352941 0.88235294 0.64705882 0.74285714 0.65714286\n",
            " 0.82857143 0.77142857 0.74285714 0.79411765 0.79411765 0.61764706\n",
            " 0.73529412 0.70588235 0.68571429 0.71428571 0.77142857 0.8\n",
            " 0.74285714 0.67647059 0.79411765 0.70588235 0.73529412 0.76470588\n",
            " 0.68571429 0.8        0.71428571 0.71428571 0.71428571 0.88235294\n",
            " 0.79411765 0.70588235 0.76470588 0.73529412]\n",
            "Accuracy: 74.38% (6.86%)\n",
            "Execution Time: 75.07 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Liver Dataset ---------\n",
            "[0.88571429 0.65714286 0.68571429 0.68571429 0.74285714 0.76470588\n",
            " 0.67647059 0.64705882 0.67647059 0.70588235 0.68571429 0.68571429\n",
            " 0.6        0.8        0.77142857 0.61764706 0.85294118 0.67647059\n",
            " 0.67647059 0.79411765 0.68571429 0.71428571 0.74285714 0.74285714\n",
            " 0.74285714 0.76470588 0.76470588 0.55882353 0.73529412 0.76470588\n",
            " 0.77142857 0.8        0.71428571 0.8        0.77142857 0.64705882\n",
            " 0.64705882 0.61764706 0.70588235 0.64705882 0.68571429 0.68571429\n",
            " 0.74285714 0.65714286 0.71428571 0.76470588 0.64705882 0.58823529\n",
            " 0.70588235 0.70588235 0.71428571 0.77142857 0.71428571 0.68571429\n",
            " 0.77142857 0.64705882 0.73529412 0.64705882 0.73529412 0.64705882\n",
            " 0.62857143 0.8        0.62857143 0.68571429 0.6        0.70588235\n",
            " 0.58823529 0.94117647 0.76470588 0.61764706 0.91428571 0.6\n",
            " 0.71428571 0.71428571 0.65714286 0.79411765 0.67647059 0.61764706\n",
            " 0.64705882 0.70588235 0.71428571 0.68571429 0.68571429 0.74285714\n",
            " 0.77142857 0.64705882 0.58823529 0.55882353 0.67647059 0.64705882\n",
            " 0.68571429 0.74285714 0.8        0.48571429 0.6        0.76470588\n",
            " 0.76470588 0.67647059 0.61764706 0.73529412]\n",
            "Accuracy: 70.19% (7.50%)\n",
            "Execution Time: 61.45 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Liver Dataset ---------\n",
            "[0.91428571 0.8        0.65714286 0.71428571 0.71428571 0.79411765\n",
            " 0.64705882 0.73529412 0.70588235 0.70588235 0.77142857 0.62857143\n",
            " 0.65714286 0.82857143 0.77142857 0.73529412 0.79411765 0.64705882\n",
            " 0.73529412 0.79411765 0.68571429 0.65714286 0.74285714 0.82857143\n",
            " 0.88571429 0.76470588 0.76470588 0.64705882 0.70588235 0.73529412\n",
            " 0.77142857 0.74285714 0.74285714 0.88571429 0.68571429 0.79411765\n",
            " 0.52941176 0.73529412 0.76470588 0.67647059 0.68571429 0.74285714\n",
            " 0.71428571 0.62857143 0.71428571 0.70588235 0.67647059 0.73529412\n",
            " 0.70588235 0.79411765 0.74285714 0.74285714 0.74285714 0.65714286\n",
            " 0.71428571 0.76470588 0.79411765 0.73529412 0.73529412 0.70588235\n",
            " 0.68571429 0.8        0.71428571 0.65714286 0.68571429 0.64705882\n",
            " 0.61764706 0.85294118 0.88235294 0.64705882 0.74285714 0.65714286\n",
            " 0.82857143 0.77142857 0.74285714 0.76470588 0.70588235 0.58823529\n",
            " 0.67647059 0.70588235 0.74285714 0.82857143 0.77142857 0.8\n",
            " 0.68571429 0.67647059 0.70588235 0.67647059 0.70588235 0.67647059\n",
            " 0.71428571 0.77142857 0.74285714 0.65714286 0.71428571 0.85294118\n",
            " 0.70588235 0.70588235 0.73529412 0.73529412]\n",
            "Accuracy: 72.92% (6.60%)\n",
            "Execution Time: 16.57 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Liver Dataset ---------\n",
            "[0.68571429 0.77142857 0.68571429 0.48571429 0.62857143 0.5\n",
            " 0.58823529 0.52941176 0.61764706 0.67647059 0.68571429 0.51428571\n",
            " 0.6        0.82857143 0.6        0.85294118 0.70588235 0.5\n",
            " 0.52941176 0.67647059 0.71428571 0.6        0.65714286 0.54285714\n",
            " 0.74285714 0.73529412 0.70588235 0.58823529 0.67647059 0.79411765\n",
            " 0.65714286 0.62857143 0.6        0.8        0.68571429 0.70588235\n",
            " 0.55882353 0.82352941 0.64705882 0.52941176 0.54285714 0.62857143\n",
            " 0.57142857 0.71428571 0.62857143 0.76470588 0.52941176 0.67647059\n",
            " 0.64705882 0.73529412 0.65714286 0.77142857 0.71428571 0.62857143\n",
            " 0.68571429 0.67647059 0.79411765 0.70588235 0.67647059 0.55882353\n",
            " 0.71428571 0.77142857 0.71428571 0.68571429 0.62857143 0.70588235\n",
            " 0.58823529 0.76470588 0.73529412 0.58823529 0.62857143 0.68571429\n",
            " 0.74285714 0.8        0.65714286 0.79411765 0.61764706 0.58823529\n",
            " 0.61764706 0.67647059 0.68571429 0.65714286 0.77142857 0.74285714\n",
            " 0.62857143 0.61764706 0.76470588 0.52941176 0.52941176 0.58823529\n",
            " 0.62857143 0.71428571 0.65714286 0.71428571 0.54285714 0.79411765\n",
            " 0.73529412 0.61764706 0.61764706 0.67647059]\n",
            "Accuracy: 66.28% (8.41%)\n",
            "Execution Time: 0.83 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Liver Dataset ---------\n",
            "[0.8        0.8        0.68571429 0.71428571 0.68571429 0.76470588\n",
            " 0.67647059 0.70588235 0.64705882 0.70588235 0.68571429 0.62857143\n",
            " 0.68571429 0.82857143 0.74285714 0.67647059 0.67647059 0.64705882\n",
            " 0.67647059 0.79411765 0.68571429 0.62857143 0.68571429 0.65714286\n",
            " 0.74285714 0.73529412 0.73529412 0.58823529 0.67647059 0.64705882\n",
            " 0.71428571 0.8        0.65714286 0.85714286 0.71428571 0.67647059\n",
            " 0.70588235 0.70588235 0.73529412 0.61764706 0.71428571 0.68571429\n",
            " 0.65714286 0.65714286 0.74285714 0.73529412 0.64705882 0.58823529\n",
            " 0.70588235 0.76470588 0.71428571 0.77142857 0.68571429 0.62857143\n",
            " 0.74285714 0.70588235 0.79411765 0.73529412 0.64705882 0.64705882\n",
            " 0.71428571 0.8        0.65714286 0.68571429 0.65714286 0.67647059\n",
            " 0.67647059 0.82352941 0.82352941 0.58823529 0.8        0.65714286\n",
            " 0.77142857 0.77142857 0.68571429 0.67647059 0.67647059 0.67647059\n",
            " 0.64705882 0.55882353 0.68571429 0.68571429 0.77142857 0.77142857\n",
            " 0.65714286 0.67647059 0.76470588 0.70588235 0.73529412 0.61764706\n",
            " 0.71428571 0.68571429 0.74285714 0.62857143 0.57142857 0.79411765\n",
            " 0.70588235 0.76470588 0.73529412 0.70588235]\n",
            "Accuracy: 70.31% (6.01%)\n",
            "Execution Time: 2.91 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "liver_scores = []\n",
        "liver_mean = []\n",
        "liver_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    liver_scores.append(results)\n",
        "    liver_mean.append(results.mean()*100)\n",
        "    liver_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Liver Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Liver scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(liver_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Liver'] = liver_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>TicTacToe</th>\n",
              "      <th>Bupa</th>\n",
              "      <th>Pima</th>\n",
              "      <th>Heart</th>\n",
              "      <th>Liver</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "      <td>97.159847</td>\n",
              "      <td>86.347619</td>\n",
              "      <td>93.815873</td>\n",
              "      <td>81.054167</td>\n",
              "      <td>71.669748</td>\n",
              "      <td>76.101504</td>\n",
              "      <td>83.111111</td>\n",
              "      <td>74.378992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>78.145238</td>\n",
              "      <td>90.854762</td>\n",
              "      <td>82.224232</td>\n",
              "      <td>69.783193</td>\n",
              "      <td>76.426863</td>\n",
              "      <td>80.444444</td>\n",
              "      <td>70.185714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>87.076190</td>\n",
              "      <td>93.815079</td>\n",
              "      <td>72.318311</td>\n",
              "      <td>69.846218</td>\n",
              "      <td>75.527683</td>\n",
              "      <td>84.481481</td>\n",
              "      <td>72.915126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>82.361905</td>\n",
              "      <td>92.849206</td>\n",
              "      <td>61.814474</td>\n",
              "      <td>69.794118</td>\n",
              "      <td>75.920711</td>\n",
              "      <td>82.851852</td>\n",
              "      <td>66.281513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "      <td>96.792626</td>\n",
              "      <td>83.802381</td>\n",
              "      <td>92.960317</td>\n",
              "      <td>65.721053</td>\n",
              "      <td>74.475630</td>\n",
              "      <td>75.334074</td>\n",
              "      <td>84.518519</td>\n",
              "      <td>70.308403</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer      Sonar  Ionosphere  TicTacToe  \\\n",
              "0   AdaBoost  96.552288      97.159847  86.347619   93.815873  81.054167   \n",
              "1  GradBoost  98.075163      96.646633  78.145238   90.854762  82.224232   \n",
              "2   CatBoost  97.967320      97.378303  87.076190   93.815079  72.318311   \n",
              "3   LightGBM  97.120915      97.334612  82.361905   92.849206  61.814474   \n",
              "4    XGBoost  97.797386      96.792626  83.802381   92.960317  65.721053   \n",
              "\n",
              "        Bupa       Pima      Heart      Liver  \n",
              "0  71.669748  76.101504  83.111111  74.378992  \n",
              "1  69.783193  76.426863  80.444444  70.185714  \n",
              "2  69.846218  75.527683  84.481481  72.915126  \n",
              "3  69.794118  75.920711  82.851852  66.281513  \n",
              "4  74.475630  75.334074  84.518519  70.308403  "
            ]
          },
          "execution_count": 292,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_results_tr = Algo_results.set_index('Names').T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_results_tr.to_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Algo_Results\\TunedAlgoResults.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Names</th>\n",
              "      <th>AdaBoost</th>\n",
              "      <th>GradBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>XGBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Wine</th>\n",
              "      <td>96.552288</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.797386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <td>97.159847</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>96.792626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sonar</th>\n",
              "      <td>86.347619</td>\n",
              "      <td>78.145238</td>\n",
              "      <td>87.076190</td>\n",
              "      <td>82.361905</td>\n",
              "      <td>83.802381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ionosphere</th>\n",
              "      <td>93.815873</td>\n",
              "      <td>90.854762</td>\n",
              "      <td>93.815079</td>\n",
              "      <td>92.849206</td>\n",
              "      <td>92.960317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TicTacToe</th>\n",
              "      <td>81.054167</td>\n",
              "      <td>82.224232</td>\n",
              "      <td>72.318311</td>\n",
              "      <td>61.814474</td>\n",
              "      <td>65.721053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bupa</th>\n",
              "      <td>71.669748</td>\n",
              "      <td>69.783193</td>\n",
              "      <td>69.846218</td>\n",
              "      <td>69.794118</td>\n",
              "      <td>74.475630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pima</th>\n",
              "      <td>76.101504</td>\n",
              "      <td>76.426863</td>\n",
              "      <td>75.527683</td>\n",
              "      <td>75.920711</td>\n",
              "      <td>75.334074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Heart</th>\n",
              "      <td>83.111111</td>\n",
              "      <td>80.444444</td>\n",
              "      <td>84.481481</td>\n",
              "      <td>82.851852</td>\n",
              "      <td>84.518519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Liver</th>\n",
              "      <td>74.378992</td>\n",
              "      <td>70.185714</td>\n",
              "      <td>72.915126</td>\n",
              "      <td>66.281513</td>\n",
              "      <td>70.308403</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Names           AdaBoost  GradBoost   CatBoost   LightGBM    XGBoost\n",
              "Wine           96.552288  98.075163  97.967320  97.120915  97.797386\n",
              "Breast_Cancer  97.159847  96.646633  97.378303  97.334612  96.792626\n",
              "Sonar          86.347619  78.145238  87.076190  82.361905  83.802381\n",
              "Ionosphere     93.815873  90.854762  93.815079  92.849206  92.960317\n",
              "TicTacToe      81.054167  82.224232  72.318311  61.814474  65.721053\n",
              "Bupa           71.669748  69.783193  69.846218  69.794118  74.475630\n",
              "Pima           76.101504  76.426863  75.527683  75.920711  75.334074\n",
              "Heart          83.111111  80.444444  84.481481  82.851852  84.518519\n",
              "Liver          74.378992  70.185714  72.915126  66.281513  70.308403"
            ]
          },
          "execution_count": 295,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Tuned_Algo_results_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Liver'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>TicTacToe</th>\n",
              "      <th>Bupa</th>\n",
              "      <th>Pima</th>\n",
              "      <th>Heart</th>\n",
              "      <th>Liver</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>22.876993</td>\n",
              "      <td>61.194410</td>\n",
              "      <td>684.942121</td>\n",
              "      <td>12.945922</td>\n",
              "      <td>180.591471</td>\n",
              "      <td>170.621086</td>\n",
              "      <td>242.872719</td>\n",
              "      <td>72.530710</td>\n",
              "      <td>75.069522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>16.068150</td>\n",
              "      <td>21.079964</td>\n",
              "      <td>8.578935</td>\n",
              "      <td>101.464809</td>\n",
              "      <td>218.208239</td>\n",
              "      <td>50.943508</td>\n",
              "      <td>10.406996</td>\n",
              "      <td>65.325738</td>\n",
              "      <td>61.445898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.146326</td>\n",
              "      <td>45.571171</td>\n",
              "      <td>777.298369</td>\n",
              "      <td>175.465500</td>\n",
              "      <td>66.157809</td>\n",
              "      <td>97.110876</td>\n",
              "      <td>14.647845</td>\n",
              "      <td>16.624023</td>\n",
              "      <td>16.569025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>3.337753</td>\n",
              "      <td>1.491335</td>\n",
              "      <td>1.497339</td>\n",
              "      <td>2.713613</td>\n",
              "      <td>5.699997</td>\n",
              "      <td>1.671589</td>\n",
              "      <td>2.365534</td>\n",
              "      <td>0.788177</td>\n",
              "      <td>0.830725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>40.201234</td>\n",
              "      <td>17.142870</td>\n",
              "      <td>21.754911</td>\n",
              "      <td>4.676054</td>\n",
              "      <td>5.930719</td>\n",
              "      <td>16.036321</td>\n",
              "      <td>3.706836</td>\n",
              "      <td>3.511206</td>\n",
              "      <td>2.912373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer       Sonar  Ionosphere   TicTacToe  \\\n",
              "0   AdaBoost  22.876993      61.194410  684.942121   12.945922  180.591471   \n",
              "1  GradBoost  16.068150      21.079964    8.578935  101.464809  218.208239   \n",
              "2   CatBoost  97.146326      45.571171  777.298369  175.465500   66.157809   \n",
              "3   LightGBM   3.337753       1.491335    1.497339    2.713613    5.699997   \n",
              "4    XGBoost  40.201234      17.142870   21.754911    4.676054    5.930719   \n",
              "\n",
              "         Bupa        Pima      Heart      Liver  \n",
              "0  170.621086  242.872719  72.530710  75.069522  \n",
              "1   50.943508   10.406996  65.325738  61.445898  \n",
              "2   97.110876   14.647845  16.624023  16.569025  \n",
              "3    1.671589    2.365534   0.788177   0.830725  \n",
              "4   16.036321    3.706836   3.511206   2.912373  "
            ]
          },
          "execution_count": 297,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_time_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_time_results_tr = Algo_time_results.set_index('Names').T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_time_results_tr.to_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Algo_Results\\TunedAlgoTimeResults.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Friedman's Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-posthocs in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (0.7.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (1.25.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (1.11.2)\n",
            "Requirement already satisfied: statsmodels in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (0.14.0)\n",
            "Requirement already satisfied: pandas>=0.20.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (2.1.0)\n",
            "Requirement already satisfied: seaborn in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (0.12.2)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (3.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.20.0->scikit-posthocs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.20.0->scikit-posthocs) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.20.0->scikit-posthocs) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (10.0.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (6.0.1)\n",
            "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from statsmodels->scikit-posthocs) (0.5.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->scikit-posthocs) (3.16.2)\n",
            "Requirement already satisfied: six in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from patsy>=0.5.2->statsmodels->scikit-posthocs) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-posthocs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scikit_posthocs as sp\n",
        "from scipy.stats import friedmanchisquare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_results_tr = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Algo_Results\\TunedAlgoResults.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AdaBoost</th>\n",
              "      <th>GradBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>XGBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>96.552288</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.797386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>97.159847</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>96.792626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>86.347619</td>\n",
              "      <td>78.145238</td>\n",
              "      <td>87.076190</td>\n",
              "      <td>82.361905</td>\n",
              "      <td>83.802381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>93.815873</td>\n",
              "      <td>90.854762</td>\n",
              "      <td>93.815079</td>\n",
              "      <td>92.849206</td>\n",
              "      <td>92.960317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>81.054167</td>\n",
              "      <td>82.224232</td>\n",
              "      <td>72.318311</td>\n",
              "      <td>61.814474</td>\n",
              "      <td>65.721053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>71.669748</td>\n",
              "      <td>69.783193</td>\n",
              "      <td>69.846218</td>\n",
              "      <td>69.794118</td>\n",
              "      <td>74.475630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>76.101504</td>\n",
              "      <td>76.426863</td>\n",
              "      <td>75.527683</td>\n",
              "      <td>75.920711</td>\n",
              "      <td>75.334074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>83.111111</td>\n",
              "      <td>80.444444</td>\n",
              "      <td>84.481481</td>\n",
              "      <td>82.851852</td>\n",
              "      <td>84.518519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>74.378992</td>\n",
              "      <td>70.185714</td>\n",
              "      <td>72.915126</td>\n",
              "      <td>66.281513</td>\n",
              "      <td>70.308403</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    AdaBoost  GradBoost   CatBoost   LightGBM    XGBoost\n",
              "0  96.552288  98.075163  97.967320  97.120915  97.797386\n",
              "1  97.159847  96.646633  97.378303  97.334612  96.792626\n",
              "2  86.347619  78.145238  87.076190  82.361905  83.802381\n",
              "3  93.815873  90.854762  93.815079  92.849206  92.960317\n",
              "4  81.054167  82.224232  72.318311  61.814474  65.721053\n",
              "5  71.669748  69.783193  69.846218  69.794118  74.475630\n",
              "6  76.101504  76.426863  75.527683  75.920711  75.334074\n",
              "7  83.111111  80.444444  84.481481  82.851852  84.518519\n",
              "8  74.378992  70.185714  72.915126  66.281513  70.308403"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Tuned_Algo_results_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "stat, p_value = friedmanchisquare(Tuned_Algo_results_tr['AdaBoost'], Tuned_Algo_results_tr['GradBoost'], Tuned_Algo_results_tr['CatBoost'], Tuned_Algo_results_tr['LightGBM'], Tuned_Algo_results_tr['XGBoost'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.10184966488235873"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are no significant differences among the models.\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print('There are significant differences among the models.')\n",
        "else:\n",
        "    print('There are no significant differences among the models.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Algorithms running time Friedman's Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_results_tr = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Algo_Results\\TunedAlgoTimeResults.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AdaBoost</th>\n",
              "      <th>GradBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>XGBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22.876993</td>\n",
              "      <td>16.068150</td>\n",
              "      <td>97.146326</td>\n",
              "      <td>3.337753</td>\n",
              "      <td>40.201234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>61.194410</td>\n",
              "      <td>21.079964</td>\n",
              "      <td>45.571171</td>\n",
              "      <td>1.491335</td>\n",
              "      <td>17.142870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>684.942121</td>\n",
              "      <td>8.578935</td>\n",
              "      <td>777.298369</td>\n",
              "      <td>1.497339</td>\n",
              "      <td>21.754911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12.945922</td>\n",
              "      <td>101.464809</td>\n",
              "      <td>175.465500</td>\n",
              "      <td>2.713613</td>\n",
              "      <td>4.676054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>180.591471</td>\n",
              "      <td>218.208239</td>\n",
              "      <td>66.157809</td>\n",
              "      <td>5.699997</td>\n",
              "      <td>5.930719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>170.621086</td>\n",
              "      <td>50.943508</td>\n",
              "      <td>97.110876</td>\n",
              "      <td>1.671589</td>\n",
              "      <td>16.036321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>242.872719</td>\n",
              "      <td>10.406996</td>\n",
              "      <td>14.647845</td>\n",
              "      <td>2.365534</td>\n",
              "      <td>3.706836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>72.530710</td>\n",
              "      <td>65.325738</td>\n",
              "      <td>16.624023</td>\n",
              "      <td>0.788177</td>\n",
              "      <td>3.511206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>75.069522</td>\n",
              "      <td>61.445898</td>\n",
              "      <td>16.569025</td>\n",
              "      <td>0.830725</td>\n",
              "      <td>2.912373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     AdaBoost   GradBoost    CatBoost  LightGBM    XGBoost\n",
              "0   22.876993   16.068150   97.146326  3.337753  40.201234\n",
              "1   61.194410   21.079964   45.571171  1.491335  17.142870\n",
              "2  684.942121    8.578935  777.298369  1.497339  21.754911\n",
              "3   12.945922  101.464809  175.465500  2.713613   4.676054\n",
              "4  180.591471  218.208239   66.157809  5.699997   5.930719\n",
              "5  170.621086   50.943508   97.110876  1.671589  16.036321\n",
              "6  242.872719   10.406996   14.647845  2.365534   3.706836\n",
              "7   72.530710   65.325738   16.624023  0.788177   3.511206\n",
              "8   75.069522   61.445898   16.569025  0.830725   2.912373"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Tuned_Algo_results_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "stat, p_value = friedmanchisquare(Tuned_Algo_results_tr['AdaBoost'], Tuned_Algo_results_tr['GradBoost'], Tuned_Algo_results_tr['CatBoost'], Tuned_Algo_results_tr['LightGBM'], Tuned_Algo_results_tr['XGBoost'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.627853700566302e-05"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are significant running time differences among the algorithm.\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print('There are significant running time differences among the algorithm.')\n",
        "else:\n",
        "    print('There are no significant running time differences among the algorithm.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Algorithms running time Nemenyi test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "nemenyi_results = sp.posthoc_nemenyi_friedman(a=Tuned_Algo_results_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AdaBoost</th>\n",
              "      <th>GradBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>XGBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AdaBoost</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.643932</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.056476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GradBoost</th>\n",
              "      <td>0.643932</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.897740</td>\n",
              "      <td>0.015044</td>\n",
              "      <td>0.643932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CatBoost</th>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.897740</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.166548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LightGBM</th>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.015044</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.381128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost</th>\n",
              "      <td>0.056476</td>\n",
              "      <td>0.643932</td>\n",
              "      <td>0.166548</td>\n",
              "      <td>0.381128</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           AdaBoost  GradBoost  CatBoost  LightGBM   XGBoost\n",
              "AdaBoost   1.000000   0.643932  0.900000  0.001000  0.056476\n",
              "GradBoost  0.643932   1.000000  0.897740  0.015044  0.643932\n",
              "CatBoost   0.900000   0.897740  1.000000  0.001000  0.166548\n",
              "LightGBM   0.001000   0.015044  0.001000  1.000000  0.381128\n",
              "XGBoost    0.056476   0.643932  0.166548  0.381128  1.000000"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nemenyi_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Algorithm 1 and 2 are not significantly different (p-value = 0.6439).\n",
            "Algorithm 1 and 3 are not significantly different (p-value = 0.9000).\n",
            "Algorithm 1 and 4 are significantly different (p-value = 0.0010).\n",
            "Algorithm 1 and 5 are not significantly different (p-value = 0.0565).\n",
            "Algorithm 2 and 3 are not significantly different (p-value = 0.8977).\n",
            "Algorithm 2 and 4 are significantly different (p-value = 0.0150).\n",
            "Algorithm 2 and 5 are not significantly different (p-value = 0.6439).\n",
            "Algorithm 3 and 4 are significantly different (p-value = 0.0010).\n",
            "Algorithm 3 and 5 are not significantly different (p-value = 0.1665).\n",
            "Algorithm 4 and 5 are not significantly different (p-value = 0.3811).\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.05\n",
        "\n",
        "# Print p-values for all pairs of models\n",
        "for i in range(nemenyi_results.shape[0]):\n",
        "    for j in range(i + 1, nemenyi_results.shape[1]):\n",
        "        model1 = i + 1\n",
        "        model2 = j + 1\n",
        "        p_value = nemenyi_results.iloc[i, j]\n",
        "\n",
        "        if p_value < alpha:\n",
        "            print(f\"Algorithm {model1} and {model2} are significantly different (p-value = {p_value:.4f}).\")\n",
        "        else:\n",
        "            print(f\"Algorithm {model1} and {model2} are not significantly different (p-value = {p_value:.4f}).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Comparison between the untuned and tuned algorithm performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "untuned_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Algo_Results\\AlgoResults.csv')\n",
        "tuned_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Algo_Results\\TunedAlgoResults.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9, 5)\n",
            "(9, 5)\n"
          ]
        }
      ],
      "source": [
        "print(untuned_df.shape)\n",
        "print(tuned_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets = ['Wine', 'Breast Cancer', 'Sonar', 'Ionosphere', 'Bupa', 'Pima', 'Heart', 'Liver']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **AdaBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "AdaBoost_before_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    AdaBoost_value = untuned_df.iloc[i, :]['AdaBoost']\n",
        "    AdaBoost_before_array.append(AdaBoost_value)\n",
        "\n",
        "AdaBoost_before = np.array(AdaBoost_before_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([89.83006536, 95.80051151, 83.18809524, 93.02777778, 73.70745614,\n",
              "       72.25462185, 75.11859193, 79.81481481, 72.25462185])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "AdaBoost_before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "AdaBoost_after_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    AdaBoost_value = tuned_df.iloc[i, :]['AdaBoost']\n",
        "    AdaBoost_after_array.append(AdaBoost_value)\n",
        "\n",
        "AdaBoost_after = np.array(AdaBoost_after_array)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([96.55228758, 97.15984655, 86.34761905, 93.81587302, 81.05416667,\n",
              "       71.6697479 , 76.10150376, 83.11111111, 74.3789916 ])"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "AdaBoost_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistic, p_value = stats.wilcoxon(AdaBoost_before, AdaBoost_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Statistic: 1.0\n",
            "P-value: 0.0078125\n",
            "Reject the null hypothesis: There is a significant difference between AdaBoost algorithm.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Statistic: {statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f'Reject the null hypothesis: There is a significant difference between AdaBoost algorithm.')\n",
        "else:\n",
        "    print(f'Fail to reject the null hypothesis: There is no significant difference between AdaBoost algorithm.')      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **GradBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "GradBoost_before_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    GradBoost_value = untuned_df.iloc[i, :]['GradBoost']\n",
        "    GradBoost_before_array.append(GradBoost_value)\n",
        "\n",
        "GradBoost_before = np.array(GradBoost_before_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([95.01633987, 96.5741688 , 84.27619048, 92.94365079, 64.35471491,\n",
              "       72.44033613, 76.2359877 , 80.07407407, 72.29243697])"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GradBoost_before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "GradBoost_after_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    GradBoost_value = tuned_df.iloc[i, :]['GradBoost']\n",
        "    GradBoost_after_array.append(GradBoost_value)\n",
        "\n",
        "GradBoost_after = np.array(GradBoost_after_array)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([98.0751634 , 96.64663257, 78.1452381 , 90.8547619 , 82.22423246,\n",
              "       69.78319328, 76.42686261, 80.44444444, 70.18571429])"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GradBoost_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistic, p_value = stats.wilcoxon(GradBoost_before, GradBoost_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Statistic: 22.0\n",
            "P-value: 1.0\n",
            "Fail to reject the null hypothesis: There is no significant difference between GradBoost algorithm.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Statistic: {statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f'Reject the null hypothesis: There is a significant difference between GradBoost algorithm.')\n",
        "else:\n",
        "    print(f'Fail to reject the null hypothesis: There is no significant difference between GradBoost algorithm.')      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **CatBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "CatBoost_before_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    CatBoost_value = untuned_df.iloc[i, :]['CatBoost']\n",
        "    CatBoost_before_array.append(CatBoost_value)\n",
        "\n",
        "CatBoost_before = np.array(CatBoost_before_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([97.97712418, 97.08716965, 87.15238095, 93.4531746 , 51.49122807,\n",
              "       74.40336134, 76.24931647, 81.88888889, 74.40336134])"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CatBoost_before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "CatBoost_after_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    CatBoost_value = tuned_df.iloc[i, :]['CatBoost']\n",
        "    CatBoost_after_array.append(CatBoost_value)\n",
        "\n",
        "CatBoost_after = np.array(CatBoost_after_array)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([97.96732026, 97.3783035 , 87.07619048, 93.81507937, 72.3183114 ,\n",
              "       69.84621849, 75.52768284, 84.48148148, 72.91512605])"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CatBoost_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistic, p_value = stats.wilcoxon(CatBoost_before, CatBoost_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Statistic: 22.0\n",
            "P-value: 1.0\n",
            "Fail to reject the null hypothesis: There is no significant difference between CatBoost algorithm.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Statistic: {statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f'Reject the null hypothesis: There is a significant difference between CatBoost algorithm.')\n",
        "else:\n",
        "    print(f'Fail to reject the null hypothesis: There is no significant difference between CatBoost algorithm.')      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **LightGBM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "LightGBM_before_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    LightGBM_value = untuned_df.iloc[i, :]['LightGBM']\n",
        "    LightGBM_before_array.append(LightGBM_value)\n",
        "\n",
        "LightGBM_before = np.array(LightGBM_before_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([97.42156863, 96.63299233, 88.12380952, 93.70634921, 55.96151316,\n",
              "       71.80168067, 73.98513329, 80.88888889, 71.80168067])"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LightGBM_before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "LightGBM_after_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    LightGBM_value = tuned_df.iloc[i, :]['LightGBM']\n",
        "    LightGBM_after_array.append(LightGBM_value)\n",
        "\n",
        "LightGBM_after = np.array(LightGBM_after_array)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([97.12091503, 97.33461211, 82.36190476, 92.84920635, 61.81447368,\n",
              "       69.79411765, 75.92071087, 82.85185185, 66.28151261])"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LightGBM_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistic, p_value = stats.wilcoxon(LightGBM_before, LightGBM_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Statistic: 20.0\n",
            "P-value: 0.8203125\n",
            "Fail to reject the null hypothesis: There is no significant difference between LightGBM algorithm.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Statistic: {statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f'Reject the null hypothesis: There is a significant difference between LightGBM algorithm.')\n",
        "else:\n",
        "    print(f'Fail to reject the null hypothesis: There is no significant difference between LightGBM algorithm.')      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "XGBoost_before_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    XGBoost_value = untuned_df.iloc[i, :]['XGBoost']\n",
        "    XGBoost_before_array.append(XGBoost_value)\n",
        "\n",
        "XGBoost_before = np.array(XGBoost_before_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([96.67647059, 96.42902813, 85.00714286, 92.43015873, 45.98881579,\n",
              "       70.57983193, 73.84244703, 80.37037037, 70.57983193])"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "XGBoost_before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "XGBoost_after_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    XGBoost_value = tuned_df.iloc[i, :]['XGBoost']\n",
        "    XGBoost_after_array.append(XGBoost_value)\n",
        "\n",
        "XGBoost_after = np.array(XGBoost_after_array)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([97.79738562, 96.79262575, 83.80238095, 92.96031746, 65.72105263,\n",
              "       74.47563025, 75.33407382, 84.51851852, 70.30840336])"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "XGBoost_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistic, p_value = stats.wilcoxon(XGBoost_before, XGBoost_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Statistic: 6.0\n",
            "P-value: 0.0546875\n",
            "Fail to reject the null hypothesis: There is no significant difference between XGBoost algorithm.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Statistic: {statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f'Reject the null hypothesis: There is a significant difference between XGBoost algorithm.')\n",
        "else:\n",
        "    print(f'Fail to reject the null hypothesis: There is no significant difference between XGBoost algorithm.')      "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMVO8koMTTTdYQJS3YoNuih",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
