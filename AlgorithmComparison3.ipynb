{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RochaErik/AlgorithmComparison/blob/main/AlgorithmComparison3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eemeaAaCsyS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7sPV5hJCeJ2"
      },
      "source": [
        "# **Evaluating algorithms with hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knwYV1QmCuEU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnwsW6x9w1QO",
        "outputId": "fad3ad0d-f838-4cfe-e95c-f8295d5fd365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (1.2.1)\n",
            "Requirement already satisfied: graphviz in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (3.7.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (2.1.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (1.11.2)\n",
            "Requirement already satisfied: plotly in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (5.16.1)\n",
            "Requirement already satisfied: six in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (10.0.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (6.0.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from plotly->catboost) (8.2.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.16.2)\n",
            "Requirement already satisfied: lightgbm in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (4.0.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from lightgbm) (1.25.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from lightgbm) (1.11.2)\n",
            "Requirement already satisfied: xgboost in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (1.7.6)\n",
            "Requirement already satisfied: numpy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from xgboost) (1.11.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost\n",
        "!pip install lightgbm\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sp9bGvxdqiOw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "import scipy.stats as stats\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB5jc4iDCWZI",
        "outputId": "a417925a-0c0b-4a02-e9a2-ca428226ab51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hyperopt in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (1.25.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (1.11.2)\n",
            "Requirement already satisfied: six in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (3.1)\n",
            "Requirement already satisfied: future in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (0.18.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (4.66.1)\n",
            "Requirement already satisfied: cloudpickle in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (2.2.1)\n",
            "Requirement already satisfied: py4j in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (0.10.9.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from tqdm->hyperopt) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install hyperopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pnmKn_fsDTha"
      },
      "outputs": [],
      "source": [
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByCnDDmkDayW"
      },
      "source": [
        "# **Wine Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "23mGy-W6DZLy"
      },
      "outputs": [],
      "source": [
        "wine_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\AprendizadoDeMaquina\\Seminario\\Datasets\\Wine\\wine.data', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C0N1S4LWDnbw"
      },
      "outputs": [],
      "source": [
        "# X = wine_df.iloc[:, 1:]\n",
        "# y = wine_df.iloc[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "omlj8qxkDoM1"
      },
      "outputs": [],
      "source": [
        "# le = LabelEncoder()\n",
        "# y = le.fit_transform(wine_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bEtKdQvTEsAR"
      },
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (106, 14)\n",
            "Validation data shape: (36, 14)\n",
            "Test data shape: (36, 14)\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.6  # 60% for training\n",
        "val_ratio = 0.2    # 20% for validation\n",
        "test_ratio = 0.2   # 20% for testing\n",
        "\n",
        "# First, split the data into training and temporary sets\n",
        "temp_data, test_data = train_test_split(wine_df, test_size=test_ratio, random_state=42)\n",
        "\n",
        "# Then, split the temporary data into training and validation sets\n",
        "train_data, val_data = train_test_split(temp_data, test_size=val_ratio / (1 - test_ratio), random_state=42)\n",
        "\n",
        "# Now, you have your training, validation, and test sets\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: ((106, 13), (106,))\n",
            "Validation data shape: ((36, 13), (36,))\n",
            "Test data shape: ((36, 13), (36,))\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "X_train = train_data.iloc[:, 1:]\n",
        "y_train = train_data.iloc[:, 0]\n",
        "print(f\"Training data shape: {X_train.shape, y_train.shape}\")\n",
        "\n",
        "# Validation\n",
        "X_val = val_data.iloc[:, 1:]\n",
        "y_val = val_data.iloc[:, 0]\n",
        "print(f\"Validation data shape: {X_val.shape, y_val.shape}\")\n",
        "\n",
        "# Test\n",
        "X_test = test_data.iloc[:, 1:]\n",
        "y_test = test_data.iloc[:, 0]\n",
        "print(f\"Test data shape: {X_test.shape, y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_val = le.fit_transform(y_val)\n",
        "y_test = le.fit_transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcZuN-z4CdXh",
        "outputId": "ee31c32a-6b6b-467e-f741-153da73f7c60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:22<00:00,  2.23trial/s, best loss: -1.0]               \n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 50.0, 'learning_rate': 0.03569852905094406, 'max_depth': 6.0, 'max_features': 'sqrt', 'min_samples_leaf': 2.0, 'min_samples_split': 2.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [01:15<00:00,  1.52s/trial, best loss: -0.9722222222222222]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': None, 'n_estimators': 750, 'learning_rate': 0.08767165012042817, 'max_depth': 2, 'min_samples_split': 3, 'min_samples_leaf': 6, 'min_weight_fraction_leaf': 0.0, 'min_impurity_decrease': 1.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [01:34<00:00,  1.90s/trial, best loss: -0.9444444444444444]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 550, 'learning_rate': 0.0479901225935416, 'min_child_samples': 1, 'max_depth': 6, 'reg_lambda': 3.3766279624518107, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 36.98trial/s, best loss: -0.9722222222222222]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'goss', 'num_leaves': 50, 'learning_rate': 0.04565607244585249, 'min_child_samples': 30, 'reg_alpha': 1.6427345495548966, 'reg_lambda': 2.054240212545051, 'colsample_by_tree': 0.2226952960698365, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:10<00:00,  4.63trial/s, best loss: -0.9722222222222222]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'dart', 'learning_rate': 0.09292666170093178, 'gamma': 4, 'max_depth': 4, 'min_child_weight': 1, 'colsample_bytree': 0.8943278668489419, 'colsample_bylevel': 0.2640104690942444, 'colsample_bynode': 0.8937107554719765, 'reg_alpha': 0.056770729092546546, 'reg_lambda': 4.219736540591216, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 50.0,\n",
              " 'learning_rate': 0.03569852905094406,\n",
              " 'max_depth': 6.0,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_leaf': 2.0,\n",
              " 'min_samples_split': 2.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['AdaBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'criterion': 'friedman_mse',\n",
              " 'max_features': None,\n",
              " 'n_estimators': 750,\n",
              " 'learning_rate': 0.08767165012042817,\n",
              " 'max_depth': 2,\n",
              " 'min_samples_split': 3,\n",
              " 'min_samples_leaf': 6,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'min_impurity_decrease': 1.0,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['GradBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 550,\n",
              " 'learning_rate': 0.0479901225935416,\n",
              " 'min_child_samples': 1,\n",
              " 'max_depth': 6,\n",
              " 'reg_lambda': 3.3766279624518107,\n",
              " 'silent': True,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['CatBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'class_weight': 'balanced',\n",
              " 'boosting_type': 'goss',\n",
              " 'num_leaves': 50,\n",
              " 'learning_rate': 0.04565607244585249,\n",
              " 'min_child_samples': 30,\n",
              " 'reg_alpha': 1.6427345495548966,\n",
              " 'reg_lambda': 2.054240212545051,\n",
              " 'colsample_by_tree': 0.2226952960698365,\n",
              " 'verbosity': -1,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['LightGBM']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'booster': 'dart',\n",
              " 'learning_rate': 0.09292666170093178,\n",
              " 'gamma': 4,\n",
              " 'max_depth': 4,\n",
              " 'min_child_weight': 1,\n",
              " 'colsample_bytree': 0.8943278668489419,\n",
              " 'colsample_bylevel': 0.2640104690942444,\n",
              " 'colsample_bynode': 0.8937107554719765,\n",
              " 'reg_alpha': 0.056770729092546546,\n",
              " 'reg_lambda': 4.219736540591216,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AiGBWUhXmjty"
      },
      "outputs": [],
      "source": [
        "rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=42)\n",
        "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['AdaBoost', 'GradBoost', 'CatBoost', 'LightGBM', 'XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "x7JQf94WmaZT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Wine Dataset ---------\n",
            "[1.         1.         1.         1.         0.75       0.75\n",
            " 1.         0.66666667 1.         1.         1.         1.\n",
            " 0.75       1.         0.75       1.         1.         0.66666667\n",
            " 0.66666667 1.         1.         1.         0.5        1.\n",
            " 0.25       1.         0.33333333 1.         1.         1.\n",
            " 1.         1.         1.         0.75       1.         1.\n",
            " 1.         0.66666667 1.         1.         1.         1.\n",
            " 0.5        0.75       0.5        1.         0.66666667 0.66666667\n",
            " 1.         1.         0.75       1.         1.         1.\n",
            " 0.75       0.75       0.66666667 1.         1.         1.\n",
            " 1.         0.75       0.5        1.         1.         1.\n",
            " 0.66666667 0.66666667 1.         1.         0.75       0.75\n",
            " 1.         0.75       1.         1.         1.         1.\n",
            " 1.         1.         1.         0.5        1.         1.\n",
            " 0.75       1.         0.66666667 1.         1.         0.66666667\n",
            " 1.         0.75       1.         0.75       0.75       1.\n",
            " 1.         1.         1.         1.        ]\n",
            "Accuracy: 88.17% (17.72%)\n",
            "Execution Time: 1.01 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Wine Dataset ---------\n",
            "[0.75       1.         1.         1.         0.75       0.5\n",
            " 0.66666667 1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         0.33333333 0.66666667\n",
            " 1.         0.66666667 0.75       0.75       0.5        1.\n",
            " 0.75       1.         1.         0.66666667 1.         1.\n",
            " 1.         1.         1.         0.75       1.         0.75\n",
            " 1.         0.66666667 1.         0.66666667 0.75       1.\n",
            " 1.         0.75       0.75       1.         0.66666667 1.\n",
            " 1.         1.         1.         1.         0.75       0.75\n",
            " 0.75       1.         0.66666667 1.         1.         0.66666667\n",
            " 1.         0.75       0.25       1.         1.         1.\n",
            " 1.         0.66666667 1.         0.66666667 1.         1.\n",
            " 1.         0.75       1.         0.75       1.         0.66666667\n",
            " 1.         1.         1.         0.5        1.         1.\n",
            " 1.         0.75       0.66666667 0.66666667 1.         0.66666667\n",
            " 1.         1.         1.         0.75       1.         0.75\n",
            " 1.         1.         0.66666667 1.        ]\n",
            "Accuracy: 87.00% (17.41%)\n",
            "Execution Time: 99.00 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Wine Dataset ---------\n",
            "[0.75       0.75       1.         1.         1.         0.5\n",
            " 1.         1.         1.         1.         1.         0.75\n",
            " 1.         1.         0.75       1.         0.66666667 1.\n",
            " 1.         1.         1.         1.         0.5        0.75\n",
            " 0.75       1.         1.         0.66666667 1.         1.\n",
            " 1.         1.         1.         0.75       0.75       1.\n",
            " 1.         0.66666667 1.         1.         1.         1.\n",
            " 0.75       0.75       0.5        1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         0.75\n",
            " 1.         0.75       0.66666667 1.         1.         1.\n",
            " 1.         0.75       0.25       1.         1.         1.\n",
            " 1.         0.66666667 1.         1.         1.         0.75\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         0.75       1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         0.75       1.\n",
            " 1.         1.         1.         0.66666667]\n",
            "Accuracy: 91.75% (15.16%)\n",
            "Execution Time: 53.57 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Wine Dataset ---------\n",
            "[0.         0.75       0.25       0.         0.25       0.\n",
            " 0.66666667 0.66666667 0.33333333 0.33333333 0.75       0.\n",
            " 0.25       0.75       0.75       0.75       0.33333333 0.33333333\n",
            " 0.66666667 0.33333333 0.25       0.75       0.         0.\n",
            " 0.25       0.         0.66666667 0.66666667 0.66666667 0.33333333\n",
            " 0.         0.75       0.25       0.25       0.75       0.\n",
            " 0.66666667 0.66666667 0.66666667 0.66666667 0.25       0.25\n",
            " 0.75       0.         0.75       0.         0.66666667 0.66666667\n",
            " 0.66666667 0.66666667 0.75       0.25       0.75       0.\n",
            " 0.75       0.25       0.66666667 0.66666667 0.66666667 0.66666667\n",
            " 0.         0.         0.         0.         0.25       0.75\n",
            " 0.66666667 0.66666667 1.         0.33333333 0.25       0.75\n",
            " 0.75       0.25       0.75       0.25       0.66666667 0.66666667\n",
            " 0.66666667 0.66666667 0.75       0.75       0.75       0.\n",
            " 0.75       0.         0.66666667 0.33333333 0.66666667 0.66666667\n",
            " 0.25       0.         0.25       0.25       0.75       0.25\n",
            " 0.66666667 0.66666667 0.33333333 0.33333333]\n",
            "Accuracy: 44.92% (28.96%)\n",
            "Execution Time: 0.76 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Wine Dataset ---------\n",
            "[0.75       0.75       1.         1.         0.75       0.5\n",
            " 0.33333333 1.         1.         1.         1.         0.75\n",
            " 1.         1.         0.25       1.         1.         0.66666667\n",
            " 1.         0.66666667 0.5        0.75       0.5        0.75\n",
            " 0.5        1.         1.         0.66666667 1.         1.\n",
            " 1.         1.         1.         0.75       0.75       0.5\n",
            " 1.         0.66666667 1.         0.33333333 0.75       1.\n",
            " 0.75       0.5        0.25       0.75       0.66666667 1.\n",
            " 1.         1.         1.         1.         0.75       0.75\n",
            " 0.75       0.5        0.66666667 1.         1.         0.33333333\n",
            " 1.         0.75       0.25       1.         0.75       1.\n",
            " 1.         0.33333333 1.         0.66666667 0.5        0.75\n",
            " 1.         0.5        0.75       0.5        1.         0.33333333\n",
            " 1.         1.         1.         0.5        1.         1.\n",
            " 1.         0.75       0.33333333 0.66666667 1.         0.33333333\n",
            " 1.         1.         0.5        0.75       0.75       0.75\n",
            " 1.         1.         0.66666667 0.66666667]\n",
            "Accuracy: 78.25% (23.57%)\n",
            "Execution Time: 17.99 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "wine_scores = []\n",
        "wine_scores_mean = []\n",
        "wine_scores_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "        \n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X_test, y_test, cv=rkf)\n",
        "    end_time = time.time()\n",
        "    wine_scores.append(results)\n",
        "    wine_scores_mean.append(results.mean()*100)\n",
        "    wine_scores_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "\n",
        "    print(f'--------- {algorithm_name} on Wine Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Wine scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(wine_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results = pd.DataFrame()\n",
        "Algo_results['Names'] = names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Wine'] = wine_scores_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>88.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>87.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>91.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>44.916667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>78.250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine\n",
              "0   AdaBoost  88.166667\n",
              "1  GradBoost  87.000000\n",
              "2   CatBoost  91.750000\n",
              "3   LightGBM  44.916667\n",
              "4    XGBoost  78.250000"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results = pd.DataFrame()\n",
        "Algo_time_results['Names'] = names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Wine'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>1.013227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.996717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>53.566599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>0.755169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>17.993899</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine\n",
              "0   AdaBoost   1.013227\n",
              "1  GradBoost  98.996717\n",
              "2   CatBoost  53.566599\n",
              "3   LightGBM   0.755169\n",
              "4    XGBoost  17.993899"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_time_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Breast Cancer Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "breast_cancer_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\AprendizadoDeMaquina\\Seminario\\Datasets\\BreastCancer\\Breast.dat', sep=',', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (409, 10)\n",
            "Validation data shape: (137, 10)\n",
            "Test data shape: (137, 10)\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.6  # 60% for training\n",
        "val_ratio = 0.2    # 20% for validation\n",
        "test_ratio = 0.2   # 20% for testing\n",
        "\n",
        "# First, split the data into training and temporary sets\n",
        "temp_data, test_data = train_test_split(breast_cancer_df, test_size=test_ratio, random_state=42)\n",
        "\n",
        "# Then, split the temporary data into training and validation sets\n",
        "train_data, val_data = train_test_split(temp_data, test_size=val_ratio / (1 - test_ratio), random_state=42)\n",
        "\n",
        "# Now, you have your training, validation, and test sets\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: ((409, 9), (409,))\n",
            "Validation data shape: ((137, 9), (137,))\n",
            "Test data shape: ((137, 9), (137,))\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "X_train = train_data.iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "print(f\"Training data shape: {X_train.shape, y_train.shape}\")\n",
        "\n",
        "# Validation\n",
        "X_val = val_data.iloc[:, :-1]\n",
        "y_val = val_data.iloc[:, -1]\n",
        "print(f\"Validation data shape: {X_val.shape, y_val.shape}\")\n",
        "\n",
        "# Test\n",
        "X_test = test_data.iloc[:, :-1]\n",
        "y_test = test_data.iloc[:, -1]\n",
        "print(f\"Test data shape: {X_test.shape, y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_val = le.fit_transform(y_val)\n",
        "y_test = le.fit_transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X = breast_cancer_df.iloc[:, :-1]\n",
        "# y = breast_cancer_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:39<00:00,  1.26trial/s, best loss: -0.9854014598540146]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 300.0, 'learning_rate': 0.03187156794580643, 'max_depth': 2.0, 'max_features': 'sqrt', 'min_samples_leaf': 3.0, 'min_samples_split': 2.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:15<00:00,  3.22trial/s, best loss: -0.9781021897810219]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': 'sqrt', 'n_estimators': 350, 'learning_rate': 0.02169498620711493, 'max_depth': 2, 'min_samples_split': 3, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.30000000000000004, 'min_impurity_decrease': 1.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [01:01<00:00,  1.22s/trial, best loss: -0.9781021897810219]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 1350, 'learning_rate': 0.022260180099049558, 'min_child_samples': 9, 'max_depth': 10, 'reg_lambda': 2.3890620775373677, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 39.72trial/s, best loss: -0.9854014598540146]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'dart', 'num_leaves': 65, 'learning_rate': 0.05159022361237221, 'min_child_samples': 80, 'reg_alpha': 0.8515154151071994, 'reg_lambda': 1.6452421725389161, 'colsample_by_tree': 0.11325175413854531, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:06<00:00,  7.36trial/s, best loss: -0.9781021897810219]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'dart', 'learning_rate': 0.028333773798751113, 'gamma': 7, 'max_depth': 3, 'min_child_weight': 3, 'colsample_bytree': 0.35891611046647764, 'colsample_bylevel': 0.4525159458882072, 'colsample_bynode': 0.5248347077120967, 'reg_alpha': 0.5515570000191785, 'reg_lambda': 0.6777375007659625, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 300.0,\n",
              " 'learning_rate': 0.03187156794580643,\n",
              " 'max_depth': 2.0,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_leaf': 3.0,\n",
              " 'min_samples_split': 2.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['AdaBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'criterion': 'friedman_mse',\n",
              " 'max_features': 'sqrt',\n",
              " 'n_estimators': 350,\n",
              " 'learning_rate': 0.02169498620711493,\n",
              " 'max_depth': 2,\n",
              " 'min_samples_split': 3,\n",
              " 'min_samples_leaf': 7,\n",
              " 'min_weight_fraction_leaf': 0.30000000000000004,\n",
              " 'min_impurity_decrease': 1.0,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['GradBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 1350,\n",
              " 'learning_rate': 0.022260180099049558,\n",
              " 'min_child_samples': 9,\n",
              " 'max_depth': 10,\n",
              " 'reg_lambda': 2.3890620775373677,\n",
              " 'silent': True,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['CatBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'class_weight': 'balanced',\n",
              " 'boosting_type': 'dart',\n",
              " 'num_leaves': 65,\n",
              " 'learning_rate': 0.05159022361237221,\n",
              " 'min_child_samples': 80,\n",
              " 'reg_alpha': 0.8515154151071994,\n",
              " 'reg_lambda': 1.6452421725389161,\n",
              " 'colsample_by_tree': 0.11325175413854531,\n",
              " 'verbosity': -1,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['LightGBM']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'booster': 'dart',\n",
              " 'learning_rate': 0.028333773798751113,\n",
              " 'gamma': 7,\n",
              " 'max_depth': 3,\n",
              " 'min_child_weight': 3,\n",
              " 'colsample_bytree': 0.35891611046647764,\n",
              " 'colsample_bylevel': 0.4525159458882072,\n",
              " 'colsample_bynode': 0.5248347077120967,\n",
              " 'reg_alpha': 0.5515570000191785,\n",
              " 'reg_lambda': 0.6777375007659625,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Breast Cancer Dataset ---------\n",
            "[1.         1.         0.92857143 0.92857143 1.         1.\n",
            " 1.         1.         0.92307692 1.         0.92857143 0.92857143\n",
            " 1.         0.92857143 1.         0.92857143 1.         0.92307692\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 0.92857143 0.92857143 0.92857143 0.92307692 1.         0.92307692\n",
            " 0.92857143 1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         0.92307692 1.         0.85714286\n",
            " 1.         1.         0.92857143 0.92857143 1.         1.\n",
            " 1.         0.92307692 0.92857143 1.         1.         1.\n",
            " 1.         0.85714286 1.         1.         1.         1.\n",
            " 1.         1.         1.         0.92857143 0.92857143 0.92857143\n",
            " 1.         1.         1.         0.92307692 1.         0.92857143\n",
            " 1.         0.92857143 1.         1.         0.92857143 1.\n",
            " 1.         0.92307692 0.92857143 0.92857143 0.92857143 1.\n",
            " 1.         1.         1.         0.92307692 0.92307692 1.\n",
            " 1.         1.         1.         0.92857143 1.         0.85714286\n",
            " 1.         1.         0.92307692 1.        ]\n",
            "Accuracy: 97.08% (3.98%)\n",
            "Execution Time: 34.87 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Breast Cancer Dataset ---------\n",
            "[0.92857143 0.92857143 0.85714286 0.92857143 1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         0.85714286 1.         1.         1.         0.92307692\n",
            " 0.92307692 0.92307692 1.         0.92857143 0.85714286 0.92857143\n",
            " 1.         0.92857143 1.         1.         0.92307692 1.\n",
            " 1.         1.         1.         0.92857143 1.         0.92857143\n",
            " 1.         0.92307692 0.92307692 0.84615385 1.         0.85714286\n",
            " 0.85714286 1.         0.92857143 0.92857143 1.         1.\n",
            " 1.         1.         1.         1.         0.92857143 1.\n",
            " 0.92857143 0.85714286 1.         1.         1.         1.\n",
            " 0.92857143 0.92857143 1.         1.         0.92857143 1.\n",
            " 1.         1.         1.         1.         0.92857143 1.\n",
            " 1.         0.85714286 1.         0.92857143 0.92857143 0.92307692\n",
            " 1.         1.         0.92857143 0.92857143 1.         1.\n",
            " 1.         1.         1.         1.         0.92307692 1.\n",
            " 0.92857143 0.92857143 1.         1.         1.         0.92857143\n",
            " 1.         0.92307692 1.         0.92307692]\n",
            "Accuracy: 96.43% (4.65%)\n",
            "Execution Time: 12.21 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Breast Cancer Dataset ---------\n",
            "[1.         1.         0.92857143 0.92857143 1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         0.85714286 1.         1.         1.         0.92307692\n",
            " 0.92307692 1.         1.         1.         1.         0.92857143\n",
            " 1.         0.92857143 1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         0.92857143\n",
            " 1.         1.         1.         0.84615385 1.         0.85714286\n",
            " 1.         1.         1.         0.92857143 1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         0.85714286 1.         1.         1.         1.\n",
            " 1.         1.         0.92857143 1.         0.92857143 1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         0.85714286 1.         1.         1.         1.\n",
            " 1.         1.         1.         0.92857143 1.         1.\n",
            " 1.         1.         1.         1.         1.         0.92307692\n",
            " 1.         1.         1.         1.         1.         0.92857143\n",
            " 0.92857143 1.         1.         1.        ]\n",
            "Accuracy: 98.26% (3.86%)\n",
            "Execution Time: 270.59 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Breast Cancer Dataset ---------\n",
            "[0.64285714 0.64285714 0.64285714 0.64285714 0.64285714 0.64285714\n",
            " 0.57142857 0.38461538 0.38461538 0.38461538 0.64285714 0.64285714\n",
            " 0.64285714 0.64285714 0.64285714 0.64285714 0.57142857 0.38461538\n",
            " 0.38461538 0.38461538 0.64285714 0.64285714 0.64285714 0.64285714\n",
            " 0.64285714 0.64285714 0.57142857 0.38461538 0.38461538 0.38461538\n",
            " 0.64285714 0.64285714 0.64285714 0.64285714 0.64285714 0.64285714\n",
            " 0.57142857 0.38461538 0.38461538 0.38461538 0.64285714 0.64285714\n",
            " 0.64285714 0.64285714 0.64285714 0.64285714 0.57142857 0.38461538\n",
            " 0.38461538 0.38461538 0.64285714 0.64285714 0.64285714 0.64285714\n",
            " 0.64285714 0.64285714 0.57142857 0.38461538 0.38461538 0.38461538\n",
            " 0.64285714 0.64285714 0.64285714 0.64285714 0.64285714 0.64285714\n",
            " 0.57142857 0.38461538 0.38461538 0.38461538 0.64285714 0.64285714\n",
            " 0.64285714 0.64285714 0.64285714 0.64285714 0.57142857 0.38461538\n",
            " 0.38461538 0.38461538 0.64285714 0.64285714 0.64285714 0.64285714\n",
            " 0.64285714 0.64285714 0.57142857 0.38461538 0.38461538 0.38461538\n",
            " 0.64285714 0.64285714 0.64285714 0.64285714 0.64285714 0.64285714\n",
            " 0.57142857 0.38461538 0.38461538 0.38461538]\n",
            "Accuracy: 55.82% (11.56%)\n",
            "Execution Time: 0.60 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Breast Cancer Dataset ---------\n",
            "[0.92857143 1.         0.92857143 0.92857143 1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         0.85714286 1.         1.         1.         1.\n",
            " 1.         0.92307692 1.         1.         0.92857143 1.\n",
            " 1.         0.92857143 1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         0.92307692 1.         0.92857143\n",
            " 0.92857143 1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         0.92857143 1.\n",
            " 0.92857143 0.92857143 1.         1.         1.         1.\n",
            " 0.92857143 1.         1.         1.         0.92857143 1.\n",
            " 1.         1.         1.         1.         0.92857143 1.\n",
            " 1.         0.92857143 1.         1.         1.         1.\n",
            " 1.         1.         0.92857143 0.92857143 1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         0.92857143\n",
            " 1.         0.92307692 1.         1.        ]\n",
            "Accuracy: 98.41% (3.16%)\n",
            "Execution Time: 10.22 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "breast_cancer_scores = []\n",
        "breast_cancer_mean = []\n",
        "breast_cancer_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X_test, y_test, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    breast_cancer_scores.append(results)\n",
        "    breast_cancer_mean.append(results.mean()*100)\n",
        "    breast_cancer_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "\n",
        "    print(f'--------- {algorithm_name} on Breast Cancer Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Breast Cancer scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(breast_cancer_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Breast_Cancer'] = breast_cancer_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>88.166667</td>\n",
              "      <td>97.082418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>96.434066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>91.750000</td>\n",
              "      <td>98.258242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>44.916667</td>\n",
              "      <td>55.824176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>78.250000</td>\n",
              "      <td>98.412088</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer\n",
              "0   AdaBoost  88.166667      97.082418\n",
              "1  GradBoost  87.000000      96.434066\n",
              "2   CatBoost  91.750000      98.258242\n",
              "3   LightGBM  44.916667      55.824176\n",
              "4    XGBoost  78.250000      98.412088"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Breast_Cancer'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Sonar Dataset** #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "sonar_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\AprendizadoDeMaquina\\Seminario\\Datasets\\Sonar\\Sonar.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (124, 61)\n",
            "Validation data shape: (42, 61)\n",
            "Test data shape: (42, 61)\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.6  # 60% for training\n",
        "val_ratio = 0.2    # 20% for validation\n",
        "test_ratio = 0.2   # 20% for testing\n",
        "\n",
        "# First, split the data into training and temporary sets\n",
        "temp_data, test_data = train_test_split(sonar_df, test_size=test_ratio, random_state=42)\n",
        "\n",
        "# Then, split the temporary data into training and validation sets\n",
        "train_data, val_data = train_test_split(temp_data, test_size=val_ratio / (1 - test_ratio), random_state=42)\n",
        "\n",
        "# Now, you have your training, validation, and test sets\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: ((124, 60), (124,))\n",
            "Validation data shape: ((42, 60), (42,))\n",
            "Test data shape: ((42, 60), (42,))\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "X_train = train_data.iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "print(f\"Training data shape: {X_train.shape, y_train.shape}\")\n",
        "\n",
        "# Validation\n",
        "X_val = val_data.iloc[:, :-1]\n",
        "y_val = val_data.iloc[:, -1]\n",
        "print(f\"Validation data shape: {X_val.shape, y_val.shape}\")\n",
        "\n",
        "# Test\n",
        "X_test = test_data.iloc[:, :-1]\n",
        "y_test = test_data.iloc[:, -1]\n",
        "print(f\"Test data shape: {X_test.shape, y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_val = le.fit_transform(y_val)\n",
        "y_test = le.fit_transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X = sonar_df.iloc[:, :-1]\n",
        "# y = sonar_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:34<00:00,  1.89s/trial, best loss: -0.8809523809523809]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 1500.0, 'learning_rate': 0.010450243715621818, 'max_depth': 3.0, 'max_features': 'log2', 'min_samples_leaf': 5.0, 'min_samples_split': 4.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [01:13<00:00,  1.46s/trial, best loss: -0.8095238095238095]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': None, 'n_estimators': 750, 'learning_rate': 0.09835742587463962, 'max_depth': 2, 'min_samples_split': 3, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 0.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [02:06<00:00,  2.53s/trial, best loss: -0.8571428571428571]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 600, 'learning_rate': 0.029110730695044405, 'min_child_samples': 2, 'max_depth': 2, 'reg_lambda': 4.8671593597739, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:00<00:00, 51.80trial/s, best loss: -0.9285714285714286]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'goss', 'num_leaves': 95, 'learning_rate': 0.07044945147979818, 'min_child_samples': 10, 'reg_alpha': 1.9914759381549856, 'reg_lambda': 1.2771332838079967, 'colsample_by_tree': 0.7974693873061367, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:05<00:00,  9.54trial/s, best loss: -0.8809523809523809]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'gbtree', 'learning_rate': 0.07407580593717424, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 5, 'colsample_bytree': 0.9989873316558338, 'colsample_bylevel': 0.5170842285607928, 'colsample_bynode': 0.7621180610212037, 'reg_alpha': 0.4015160144797284, 'reg_lambda': 3.712557768424784, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 1500.0,\n",
              " 'learning_rate': 0.010450243715621818,\n",
              " 'max_depth': 3.0,\n",
              " 'max_features': 'log2',\n",
              " 'min_samples_leaf': 5.0,\n",
              " 'min_samples_split': 4.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['AdaBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'criterion': 'friedman_mse',\n",
              " 'max_features': None,\n",
              " 'n_estimators': 750,\n",
              " 'learning_rate': 0.09835742587463962,\n",
              " 'max_depth': 2,\n",
              " 'min_samples_split': 3,\n",
              " 'min_samples_leaf': 10,\n",
              " 'min_weight_fraction_leaf': 0.1,\n",
              " 'min_impurity_decrease': 0.0,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['GradBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 600,\n",
              " 'learning_rate': 0.029110730695044405,\n",
              " 'min_child_samples': 2,\n",
              " 'max_depth': 2,\n",
              " 'reg_lambda': 4.8671593597739,\n",
              " 'silent': True,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['CatBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'class_weight': 'balanced',\n",
              " 'boosting_type': 'goss',\n",
              " 'num_leaves': 95,\n",
              " 'learning_rate': 0.07044945147979818,\n",
              " 'min_child_samples': 10,\n",
              " 'reg_alpha': 1.9914759381549856,\n",
              " 'reg_lambda': 1.2771332838079967,\n",
              " 'colsample_by_tree': 0.7974693873061367,\n",
              " 'verbosity': -1,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['LightGBM']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'booster': 'gbtree',\n",
              " 'learning_rate': 0.07407580593717424,\n",
              " 'gamma': 0,\n",
              " 'max_depth': 6,\n",
              " 'min_child_weight': 5,\n",
              " 'colsample_bytree': 0.9989873316558338,\n",
              " 'colsample_bylevel': 0.5170842285607928,\n",
              " 'colsample_bynode': 0.7621180610212037,\n",
              " 'reg_alpha': 0.4015160144797284,\n",
              " 'reg_lambda': 3.712557768424784,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Sonar Dataset ---------\n",
            "[0.8  0.6  0.75 1.   0.5  1.   0.75 0.75 0.75 0.25 0.6  0.6  0.5  1.\n",
            " 0.25 0.75 0.5  1.   0.75 0.75 0.8  0.8  0.75 0.75 0.75 0.5  1.   0.5\n",
            " 0.5  0.5  0.6  1.   1.   0.5  0.75 0.75 0.5  0.5  0.5  1.   0.8  0.8\n",
            " 0.5  1.   0.75 1.   0.75 0.75 0.75 0.25 0.6  0.8  1.   0.75 0.75 0.75\n",
            " 0.5  0.75 0.75 0.5  1.   0.6  1.   0.5  0.25 0.5  0.75 0.75 0.75 0.75\n",
            " 0.8  0.6  0.75 0.75 1.   0.75 0.25 1.   0.75 0.75 1.   0.8  0.75 0.75\n",
            " 1.   0.5  0.5  0.5  0.5  0.5  0.8  0.6  0.5  1.   0.5  0.75 0.75 0.75\n",
            " 0.75 0.5 ]\n",
            "Accuracy: 70.25% (19.63%)\n",
            "Execution Time: 172.10 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Sonar Dataset ---------\n",
            "[0.8  0.8  1.   0.75 0.25 0.5  0.5  1.   1.   0.25 0.6  0.6  0.25 0.5\n",
            " 0.25 0.5  0.25 0.5  0.75 0.5  0.6  0.6  0.25 0.75 0.5  0.5  0.5  0.5\n",
            " 0.75 0.75 0.6  0.6  1.   0.25 0.75 0.75 0.5  0.5  0.5  1.   0.4  0.6\n",
            " 0.5  1.   0.75 1.   0.75 0.5  0.75 0.5  0.6  0.8  0.75 0.5  0.75 0.75\n",
            " 0.25 0.5  0.75 0.5  0.8  0.6  1.   0.5  0.   0.   1.   0.25 0.75 0.25\n",
            " 0.6  0.6  0.25 0.75 1.   0.75 0.25 1.   0.75 0.75 0.8  0.8  0.5  1.\n",
            " 0.75 0.5  0.75 0.75 0.75 0.5  0.6  0.6  0.25 1.   0.25 0.75 0.5  0.75\n",
            " 0.25 0.5 ]\n",
            "Accuracy: 61.00% (23.92%)\n",
            "Execution Time: 49.97 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Sonar Dataset ---------\n",
            "[0.6  0.6  1.   0.75 0.25 0.75 0.5  1.   1.   0.25 0.8  0.6  0.5  1.\n",
            " 0.25 0.5  0.25 0.5  0.75 0.75 0.6  0.8  0.5  0.75 0.5  0.5  0.75 0.5\n",
            " 0.75 0.5  0.6  0.8  1.   0.25 0.75 0.5  0.5  0.25 0.75 1.   0.2  0.6\n",
            " 0.5  0.75 0.75 1.   0.75 0.5  0.75 0.5  0.6  0.6  0.5  0.25 0.75 0.75\n",
            " 0.25 0.75 0.75 0.5  0.8  0.6  1.   0.5  0.25 0.25 1.   0.25 0.75 0.5\n",
            " 0.6  0.6  0.5  0.75 1.   0.75 0.25 0.75 0.75 0.75 0.6  0.8  0.5  1.\n",
            " 0.5  0.5  0.25 1.   0.5  0.5  0.6  0.8  0.25 1.   0.25 0.75 0.75 0.75\n",
            " 0.75 0.25]\n",
            "Accuracy: 62.05% (22.90%)\n",
            "Execution Time: 67.88 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Sonar Dataset ---------\n",
            "[0.8  0.4  0.75 0.75 0.25 0.25 0.75 1.   1.   0.75 0.8  0.8  0.5  1.\n",
            " 0.25 0.5  1.   0.5  0.75 0.75 0.4  0.8  0.75 0.5  0.5  0.5  0.75 0.75\n",
            " 1.   0.75 0.6  0.8  1.   0.   0.25 0.75 0.5  0.75 0.75 1.   0.2  0.8\n",
            " 1.   1.   0.75 1.   0.75 0.5  0.75 0.5  0.6  0.8  1.   0.25 0.75 0.75\n",
            " 0.25 0.75 0.75 0.75 0.8  0.8  1.   1.   0.5  0.5  1.   0.5  0.5  0.25\n",
            " 0.6  0.6  0.5  0.75 0.75 0.75 0.75 0.75 0.75 0.5  0.6  0.6  0.5  1.\n",
            " 1.   0.5  0.5  1.   0.75 0.5  0.6  0.8  0.5  1.   0.25 0.5  0.75 0.5\n",
            " 1.   0.5 ]\n",
            "Accuracy: 67.45% (23.12%)\n",
            "Execution Time: 1.03 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Sonar Dataset ---------\n",
            "[0.6  0.6  0.75 0.75 0.75 0.75 0.5  0.5  0.5  0.5  0.6  0.6  0.75 0.75\n",
            " 0.75 0.75 0.5  0.5  0.5  0.5  0.6  0.6  0.75 0.75 0.75 0.75 0.5  0.5\n",
            " 0.5  0.5  0.6  0.6  0.75 0.75 0.75 0.75 0.5  0.5  0.5  0.5  0.6  0.6\n",
            " 0.75 0.75 0.75 0.75 0.5  0.5  0.5  0.5  0.6  0.6  0.75 0.75 0.75 0.75\n",
            " 0.5  0.5  0.5  0.5  0.6  0.6  0.75 0.75 0.75 0.75 0.5  0.5  0.5  0.5\n",
            " 0.6  0.6  0.75 0.75 0.75 0.75 0.5  0.5  0.5  0.5  0.6  0.6  0.75 0.75\n",
            " 0.75 0.75 0.5  0.5  0.5  0.5  0.6  0.6  0.75 0.75 0.75 0.75 0.5  0.5\n",
            " 0.5  0.5 ]\n",
            "Accuracy: 62.00% (11.22%)\n",
            "Execution Time: 4.02 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "sonar_scores = []\n",
        "sonar_mean = []\n",
        "sonar_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "        \n",
        "    start_time = time.time()\n",
        "    results = cross_val_score(clf, X_test, y_test, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    sonar_scores.append(results)\n",
        "    sonar_mean.append(results.mean()*100)\n",
        "    sonar_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "\n",
        "    print(f'--------- {algorithm_name} on Sonar Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Sonar scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(sonar_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Sonar'] = sonar_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>88.166667</td>\n",
              "      <td>97.082418</td>\n",
              "      <td>70.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>96.434066</td>\n",
              "      <td>61.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>91.750000</td>\n",
              "      <td>98.258242</td>\n",
              "      <td>62.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>44.916667</td>\n",
              "      <td>55.824176</td>\n",
              "      <td>67.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>78.250000</td>\n",
              "      <td>98.412088</td>\n",
              "      <td>62.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer  Sonar\n",
              "0   AdaBoost  88.166667      97.082418  70.25\n",
              "1  GradBoost  87.000000      96.434066  61.00\n",
              "2   CatBoost  91.750000      98.258242  62.05\n",
              "3   LightGBM  44.916667      55.824176  67.45\n",
              "4    XGBoost  78.250000      98.412088  62.00"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Sonar'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Ionosphere Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "ionosphere_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\AprendizadoDeMaquina\\Seminario\\Datasets\\Ionosphere\\ionosphere.data', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(351, 35)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ionosphere_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (210, 35)\n",
            "Validation data shape: (70, 35)\n",
            "Test data shape: (71, 35)\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.6  # 60% for training\n",
        "val_ratio = 0.2    # 20% for validation\n",
        "test_ratio = 0.2   # 20% for testing\n",
        "\n",
        "# First, split the data into training and temporary sets\n",
        "temp_data, test_data = train_test_split(ionosphere_df, test_size=test_ratio, random_state=42)\n",
        "\n",
        "# Then, split the temporary data into training and validation sets\n",
        "train_data, val_data = train_test_split(temp_data, test_size=val_ratio / (1 - test_ratio), random_state=42)\n",
        "\n",
        "# Now, you have your training, validation, and test sets\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: ((210, 34), (210,))\n",
            "Validation data shape: ((70, 34), (70,))\n",
            "Test data shape: ((71, 34), (71,))\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "X_train = train_data.iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "print(f\"Training data shape: {X_train.shape, y_train.shape}\")\n",
        "\n",
        "# Validation\n",
        "X_val = val_data.iloc[:, :-1]\n",
        "y_val = val_data.iloc[:, -1]\n",
        "print(f\"Validation data shape: {X_val.shape, y_val.shape}\")\n",
        "\n",
        "# Test\n",
        "X_test = test_data.iloc[:, :-1]\n",
        "y_test = test_data.iloc[:, -1]\n",
        "print(f\"Test data shape: {X_test.shape, y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_val = le.fit_transform(y_val)\n",
        "y_test = le.fit_transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X = ionosphere_df.iloc[:, :-1]\n",
        "# y = ionosphere_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:52<00:00,  2.25s/trial, best loss: -0.9714285714285714]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 1200.0, 'learning_rate': 0.012995597957317272, 'max_depth': 5.0, 'max_features': 'sqrt', 'min_samples_leaf': 5.0, 'min_samples_split': 6.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [01:04<00:00,  1.28s/trial, best loss: -0.9142857142857143]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': None, 'n_estimators': 850, 'learning_rate': 0.053611707225416305, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 0.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [02:19<00:00,  2.79s/trial, best loss: -0.9142857142857143]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 600, 'learning_rate': 0.029110730695044405, 'min_child_samples': 2, 'max_depth': 2, 'reg_lambda': 4.8671593597739, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 46.63trial/s, best loss: -0.9142857142857143]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'goss', 'num_leaves': 60, 'learning_rate': 0.014000382530118339, 'min_child_samples': 10, 'reg_alpha': 1.3343428739988858, 'reg_lambda': 0.13071156229151032, 'colsample_by_tree': 0.15721035835720476, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:08<00:00,  5.82trial/s, best loss: -0.9142857142857143]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'dart', 'learning_rate': 0.09292666170093178, 'gamma': 4, 'max_depth': 4, 'min_child_weight': 1, 'colsample_bytree': 0.8943278668489419, 'colsample_bylevel': 0.2640104690942444, 'colsample_bynode': 0.8937107554719765, 'reg_alpha': 0.056770729092546546, 'reg_lambda': 4.219736540591216, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Ionosphere Dataset ---------\n",
            "[0.875      0.85714286 1.         0.85714286 1.         0.71428571\n",
            " 0.85714286 0.71428571 1.         0.85714286 0.875      1.\n",
            " 0.71428571 0.71428571 0.85714286 0.71428571 0.85714286 0.85714286\n",
            " 1.         1.         0.75       1.         1.         0.85714286\n",
            " 0.85714286 1.         0.71428571 0.71428571 0.85714286 0.85714286\n",
            " 1.         1.         0.85714286 0.85714286 0.85714286 0.85714286\n",
            " 0.71428571 0.85714286 0.85714286 0.71428571 1.         0.85714286\n",
            " 1.         0.85714286 0.85714286 0.71428571 0.85714286 0.71428571\n",
            " 0.85714286 0.85714286 0.875      0.71428571 0.71428571 1.\n",
            " 0.85714286 0.71428571 0.85714286 0.85714286 0.85714286 0.85714286\n",
            " 0.875      0.85714286 0.85714286 0.71428571 0.85714286 1.\n",
            " 0.85714286 0.85714286 0.85714286 0.71428571 0.75       0.85714286\n",
            " 1.         0.85714286 1.         0.85714286 1.         0.71428571\n",
            " 0.71428571 0.71428571 1.         1.         1.         0.57142857\n",
            " 0.85714286 0.85714286 0.71428571 0.85714286 1.         0.85714286\n",
            " 0.875      1.         1.         0.71428571 1.         0.71428571\n",
            " 0.71428571 0.85714286 0.71428571 0.71428571]\n",
            "Accuracy: 85.30% (10.61%)\n",
            "Execution Time: 146.82 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Ionosphere Dataset ---------\n",
            "[0.75       0.85714286 1.         1.         1.         0.71428571\n",
            " 0.85714286 0.85714286 1.         0.85714286 0.875      1.\n",
            " 0.85714286 0.42857143 0.85714286 0.71428571 1.         1.\n",
            " 1.         1.         0.875      1.         1.         0.85714286\n",
            " 1.         1.         1.         0.85714286 0.85714286 0.85714286\n",
            " 0.875      1.         0.85714286 0.85714286 1.         1.\n",
            " 0.71428571 1.         0.85714286 0.71428571 0.875      0.85714286\n",
            " 1.         0.85714286 0.71428571 0.85714286 0.85714286 0.71428571\n",
            " 1.         1.         0.875      0.71428571 1.         1.\n",
            " 0.85714286 0.57142857 0.85714286 0.85714286 0.85714286 1.\n",
            " 0.75       0.85714286 0.85714286 0.85714286 0.85714286 1.\n",
            " 1.         0.85714286 0.71428571 0.85714286 0.75       1.\n",
            " 1.         0.71428571 1.         1.         0.85714286 0.71428571\n",
            " 1.         0.85714286 0.875      1.         1.         0.85714286\n",
            " 1.         0.85714286 0.85714286 0.85714286 1.         0.71428571\n",
            " 1.         1.         1.         1.         1.         0.85714286\n",
            " 0.85714286 1.         0.71428571 0.85714286]\n",
            "Accuracy: 88.93% (11.31%)\n",
            "Execution Time: 86.93 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Ionosphere Dataset ---------\n",
            "[0.75       1.         0.85714286 0.85714286 1.         0.71428571\n",
            " 0.85714286 1.         1.         1.         0.875      1.\n",
            " 1.         0.57142857 0.85714286 0.85714286 0.85714286 1.\n",
            " 1.         1.         0.875      1.         1.         0.85714286\n",
            " 1.         1.         0.71428571 1.         0.71428571 0.85714286\n",
            " 1.         1.         0.85714286 0.71428571 1.         0.85714286\n",
            " 0.85714286 1.         0.85714286 0.71428571 1.         0.85714286\n",
            " 1.         0.85714286 1.         0.71428571 0.85714286 0.85714286\n",
            " 0.85714286 1.         0.875      0.85714286 1.         1.\n",
            " 1.         1.         0.85714286 0.85714286 0.85714286 0.85714286\n",
            " 0.875      1.         0.85714286 0.71428571 0.85714286 1.\n",
            " 1.         1.         0.71428571 1.         0.625      1.\n",
            " 1.         1.         1.         0.71428571 1.         0.85714286\n",
            " 0.85714286 1.         1.         1.         1.         0.85714286\n",
            " 1.         0.85714286 0.85714286 0.85714286 1.         0.85714286\n",
            " 0.875      1.         1.         1.         1.         0.85714286\n",
            " 0.71428571 1.         0.85714286 0.85714286]\n",
            "Accuracy: 90.61% (10.45%)\n",
            "Execution Time: 65.28 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Ionosphere Dataset ---------\n",
            "[0.75       0.85714286 0.85714286 0.85714286 0.85714286 0.71428571\n",
            " 0.71428571 0.71428571 0.57142857 0.71428571 0.75       0.71428571\n",
            " 0.85714286 0.57142857 0.85714286 0.85714286 0.85714286 0.85714286\n",
            " 1.         0.85714286 0.875      0.85714286 1.         0.71428571\n",
            " 0.71428571 1.         0.71428571 0.71428571 0.57142857 0.71428571\n",
            " 1.         1.         0.85714286 0.85714286 0.85714286 0.57142857\n",
            " 0.71428571 0.85714286 0.71428571 0.71428571 0.75       0.85714286\n",
            " 0.71428571 0.85714286 0.71428571 0.42857143 0.85714286 0.57142857\n",
            " 0.85714286 0.71428571 0.75       0.57142857 0.71428571 0.85714286\n",
            " 1.         0.71428571 0.85714286 0.71428571 0.85714286 1.\n",
            " 0.625      1.         0.85714286 0.85714286 0.85714286 0.85714286\n",
            " 1.         1.         0.71428571 0.71428571 0.75       1.\n",
            " 1.         0.85714286 0.71428571 0.85714286 0.71428571 0.71428571\n",
            " 0.85714286 0.85714286 0.875      0.71428571 1.         0.57142857\n",
            " 1.         0.85714286 0.71428571 0.85714286 0.85714286 0.85714286\n",
            " 0.875      0.85714286 1.         1.         1.         0.85714286\n",
            " 0.57142857 0.85714286 0.71428571 0.71428571]\n",
            "Accuracy: 80.71% (12.72%)\n",
            "Execution Time: 1.43 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Ionosphere Dataset ---------\n",
            "[1.         0.85714286 0.71428571 0.85714286 0.85714286 0.71428571\n",
            " 0.85714286 0.71428571 1.         0.71428571 0.875      1.\n",
            " 0.85714286 0.57142857 0.85714286 0.71428571 1.         0.85714286\n",
            " 1.         0.85714286 0.625      1.         1.         0.85714286\n",
            " 0.85714286 0.85714286 0.85714286 0.71428571 0.71428571 0.85714286\n",
            " 1.         1.         0.85714286 0.85714286 0.85714286 0.85714286\n",
            " 0.71428571 0.85714286 0.85714286 0.71428571 1.         0.85714286\n",
            " 1.         0.85714286 0.71428571 0.85714286 0.85714286 0.57142857\n",
            " 0.71428571 0.71428571 0.75       0.71428571 0.57142857 1.\n",
            " 1.         1.         0.85714286 0.85714286 0.85714286 1.\n",
            " 0.875      0.85714286 0.85714286 0.57142857 0.85714286 1.\n",
            " 0.71428571 0.85714286 0.71428571 0.85714286 0.75       1.\n",
            " 1.         0.85714286 1.         0.85714286 1.         0.71428571\n",
            " 0.85714286 0.71428571 1.         1.         0.85714286 0.57142857\n",
            " 1.         0.85714286 0.85714286 0.57142857 1.         0.85714286\n",
            " 0.875      1.         1.         1.         1.         0.71428571\n",
            " 0.71428571 1.         0.71428571 0.71428571]\n",
            "Accuracy: 84.75% (12.56%)\n",
            "Execution Time: 11.44 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "ionosphere_scores = []\n",
        "ionosphere_mean = []\n",
        "ionosphere_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "        \n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X_test, y_test, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    ionosphere_scores.append(results)\n",
        "    ionosphere_mean.append(results.mean()*100)\n",
        "    ionosphere_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Ionosphere Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Ionosphere scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(ionosphere_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Ionosphere'] = ionosphere_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>88.166667</td>\n",
              "      <td>97.082418</td>\n",
              "      <td>70.25</td>\n",
              "      <td>85.303571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>96.434066</td>\n",
              "      <td>61.00</td>\n",
              "      <td>88.928571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>91.750000</td>\n",
              "      <td>98.258242</td>\n",
              "      <td>62.05</td>\n",
              "      <td>90.607143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>44.916667</td>\n",
              "      <td>55.824176</td>\n",
              "      <td>67.45</td>\n",
              "      <td>80.714286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>78.250000</td>\n",
              "      <td>98.412088</td>\n",
              "      <td>62.00</td>\n",
              "      <td>84.750000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer  Sonar  Ionosphere\n",
              "0   AdaBoost  88.166667      97.082418  70.25   85.303571\n",
              "1  GradBoost  87.000000      96.434066  61.00   88.928571\n",
              "2   CatBoost  91.750000      98.258242  62.05   90.607143\n",
              "3   LightGBM  44.916667      55.824176  67.45   80.714286\n",
              "4    XGBoost  78.250000      98.412088  62.00   84.750000"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Ionosphere'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Bupa**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "bupa_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\AprendizadoDeMaquina\\Seminario\\Datasets\\Bupa\\Bupa.data', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(345, 7)"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bupa_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (207, 7)\n",
            "Validation data shape: (69, 7)\n",
            "Test data shape: (69, 7)\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.6  # 60% for training\n",
        "val_ratio = 0.2    # 20% for validation\n",
        "test_ratio = 0.2   # 20% for testing\n",
        "\n",
        "# First, split the data into training and temporary sets\n",
        "temp_data, test_data = train_test_split(bupa_df, test_size=test_ratio, random_state=42)\n",
        "\n",
        "# Then, split the temporary data into training and validation sets\n",
        "train_data, val_data = train_test_split(temp_data, test_size=val_ratio / (1 - test_ratio), random_state=42)\n",
        "\n",
        "# Now, you have your training, validation, and test sets\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: ((207, 6), (207,))\n",
            "Validation data shape: ((69, 6), (69,))\n",
            "Test data shape: ((69, 6), (69,))\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "X_train = train_data.iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "print(f\"Training data shape: {X_train.shape, y_train.shape}\")\n",
        "\n",
        "# Validation\n",
        "X_val = val_data.iloc[:, :-1]\n",
        "y_val = val_data.iloc[:, -1]\n",
        "print(f\"Validation data shape: {X_val.shape, y_val.shape}\")\n",
        "\n",
        "# Test\n",
        "X_test = test_data.iloc[:, :-1]\n",
        "y_test = test_data.iloc[:, -1]\n",
        "print(f\"Test data shape: {X_test.shape, y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_val = le.fit_transform(y_val)\n",
        "y_test = le.fit_transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X = bupa_df.iloc[:, :-1]\n",
        "# y = bupa_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:41<00:00,  1.22trial/s, best loss: -0.7681159420289855]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 200.0, 'learning_rate': 0.028745858811684347, 'max_depth': 2.0, 'max_features': 'log2', 'min_samples_leaf': 2.0, 'min_samples_split': 8.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:13<00:00,  3.70trial/s, best loss: -0.7681159420289855]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': 'sqrt', 'n_estimators': 100, 'learning_rate': 0.04102652661864284, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.0, 'min_impurity_decrease': 1.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:49<00:00,  1.02trial/s, best loss: -0.782608695652174] \n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 500, 'learning_rate': 0.01002981253798441, 'min_child_samples': 2, 'max_depth': 10, 'reg_lambda': 2.6701577095887252, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 48.86trial/s, best loss: -0.7391304347826086]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'gbdt', 'num_leaves': 45, 'learning_rate': 0.04824163099686186, 'min_child_samples': 40, 'reg_alpha': 0.4714117799670238, 'reg_lambda': 3.6517162746818896, 'colsample_by_tree': 0.6717443359974058, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:04<00:00, 10.19trial/s, best loss: -0.7971014492753623]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'gbtree', 'learning_rate': 0.011777426690454684, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'colsample_bytree': 0.6642423404208758, 'colsample_bylevel': 0.8389604376670141, 'colsample_bynode': 0.46801910869053165, 'reg_alpha': 1.3842922617481603, 'reg_lambda': 0.25127542856871243, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Bupa Dataset ---------\n",
            "[0.57142857 0.57142857 0.28571429 0.71428571 0.71428571 0.71428571\n",
            " 0.28571429 0.57142857 0.85714286 0.66666667 0.71428571 0.57142857\n",
            " 0.71428571 0.71428571 0.71428571 0.71428571 0.42857143 0.85714286\n",
            " 0.28571429 0.66666667 0.57142857 0.42857143 0.57142857 0.85714286\n",
            " 0.57142857 0.71428571 0.57142857 0.71428571 0.57142857 0.66666667\n",
            " 0.57142857 0.85714286 0.57142857 0.71428571 0.57142857 0.71428571\n",
            " 0.71428571 0.42857143 0.71428571 0.83333333 0.57142857 0.71428571\n",
            " 0.57142857 0.85714286 0.42857143 0.71428571 0.57142857 0.57142857\n",
            " 0.57142857 0.83333333 0.57142857 0.85714286 0.57142857 0.85714286\n",
            " 0.71428571 0.85714286 0.57142857 0.85714286 0.42857143 0.5\n",
            " 0.71428571 0.42857143 0.85714286 0.57142857 0.71428571 0.57142857\n",
            " 0.71428571 0.57142857 0.71428571 0.83333333 0.71428571 0.85714286\n",
            " 0.57142857 0.85714286 0.42857143 0.85714286 0.42857143 0.71428571\n",
            " 0.71428571 0.66666667 0.57142857 0.71428571 0.85714286 0.57142857\n",
            " 0.71428571 0.85714286 0.42857143 0.71428571 0.57142857 0.83333333\n",
            " 0.71428571 0.71428571 0.71428571 0.71428571 0.42857143 0.57142857\n",
            " 0.42857143 0.57142857 0.71428571 0.83333333]\n",
            "Accuracy: 65.19% (14.43%)\n",
            "Execution Time: 22.17 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Bupa Dataset ---------\n",
            "[0.71428571 0.71428571 0.57142857 0.71428571 0.71428571 0.57142857\n",
            " 0.71428571 0.71428571 1.         0.66666667 0.85714286 0.71428571\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.71428571 1.\n",
            " 0.85714286 0.66666667 0.42857143 0.57142857 0.57142857 0.85714286\n",
            " 0.71428571 0.71428571 0.57142857 0.85714286 0.71428571 0.83333333\n",
            " 0.57142857 0.57142857 0.71428571 0.57142857 0.57142857 0.57142857\n",
            " 0.71428571 0.71428571 0.71428571 0.83333333 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.71428571 0.57142857 0.71428571 0.71428571\n",
            " 0.57142857 0.83333333 0.57142857 0.71428571 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.42857143 0.85714286 0.85714286 0.5\n",
            " 0.71428571 0.57142857 0.85714286 0.57142857 0.71428571 0.57142857\n",
            " 0.71428571 0.57142857 0.71428571 0.83333333 0.85714286 0.71428571\n",
            " 0.57142857 0.71428571 0.57142857 0.57142857 0.57142857 0.85714286\n",
            " 0.71428571 0.83333333 0.71428571 0.57142857 0.71428571 0.71428571\n",
            " 0.85714286 0.57142857 0.57142857 0.71428571 0.71428571 0.66666667\n",
            " 0.85714286 0.85714286 0.57142857 0.71428571 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.85714286 0.66666667]\n",
            "Accuracy: 67.62% (11.87%)\n",
            "Execution Time: 3.49 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Bupa Dataset ---------\n",
            "[0.57142857 0.85714286 0.57142857 0.71428571 0.28571429 0.71428571\n",
            " 0.71428571 0.57142857 0.71428571 0.5        0.85714286 0.57142857\n",
            " 0.57142857 0.71428571 0.57142857 0.57142857 0.57142857 0.85714286\n",
            " 0.57142857 0.66666667 0.57142857 0.57142857 0.71428571 0.71428571\n",
            " 0.71428571 0.71428571 0.85714286 0.71428571 0.42857143 0.5\n",
            " 0.42857143 0.85714286 0.85714286 0.57142857 0.71428571 0.57142857\n",
            " 0.57142857 0.57142857 0.71428571 0.66666667 0.71428571 0.71428571\n",
            " 0.42857143 0.85714286 0.42857143 0.71428571 0.28571429 0.71428571\n",
            " 0.57142857 1.         0.71428571 0.57142857 0.57142857 0.85714286\n",
            " 0.57142857 0.85714286 0.57142857 0.85714286 0.28571429 0.5\n",
            " 0.57142857 0.85714286 0.85714286 0.42857143 0.42857143 0.85714286\n",
            " 0.71428571 0.57142857 0.71428571 0.83333333 0.85714286 0.85714286\n",
            " 0.42857143 0.71428571 0.28571429 1.         0.42857143 0.85714286\n",
            " 0.57142857 0.66666667 0.57142857 0.85714286 0.71428571 0.42857143\n",
            " 0.85714286 0.71428571 0.28571429 0.71428571 0.71428571 0.66666667\n",
            " 0.85714286 0.85714286 1.         0.57142857 0.57142857 0.57142857\n",
            " 0.42857143 0.57142857 0.71428571 0.66666667]\n",
            "Accuracy: 65.38% (16.60%)\n",
            "Execution Time: 141.79 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Bupa Dataset ---------\n",
            "[0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.28571429 0.28571429 0.66666667 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.28571429\n",
            " 0.28571429 0.66666667 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.57142857 0.28571429 0.28571429 0.66666667\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.28571429 0.28571429 0.66666667 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.28571429\n",
            " 0.28571429 0.66666667 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.57142857 0.28571429 0.28571429 0.66666667\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.28571429 0.28571429 0.66666667 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.28571429\n",
            " 0.28571429 0.66666667 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.57142857 0.28571429 0.28571429 0.66666667\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.28571429 0.28571429 0.66666667]\n",
            "Accuracy: 52.38% (12.23%)\n",
            "Execution Time: 0.56 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Bupa Dataset ---------\n",
            "[0.71428571 0.71428571 0.42857143 0.71428571 0.57142857 0.57142857\n",
            " 0.71428571 0.71428571 1.         0.66666667 0.57142857 0.71428571\n",
            " 0.57142857 0.42857143 0.57142857 0.57142857 0.57142857 1.\n",
            " 0.71428571 0.66666667 0.42857143 0.57142857 0.57142857 0.71428571\n",
            " 0.71428571 0.57142857 0.85714286 0.71428571 0.71428571 0.66666667\n",
            " 0.71428571 0.71428571 0.71428571 0.57142857 0.57142857 0.85714286\n",
            " 0.57142857 0.71428571 0.57142857 0.83333333 0.57142857 0.57142857\n",
            " 0.57142857 0.85714286 0.57142857 0.71428571 0.71428571 0.71428571\n",
            " 0.71428571 0.66666667 0.71428571 0.71428571 0.57142857 0.57142857\n",
            " 0.57142857 0.71428571 0.28571429 0.71428571 0.85714286 0.66666667\n",
            " 0.71428571 0.85714286 0.57142857 0.57142857 0.71428571 0.71428571\n",
            " 0.57142857 0.57142857 0.57142857 0.83333333 0.85714286 0.57142857\n",
            " 0.71428571 0.57142857 0.57142857 0.57142857 0.57142857 0.71428571\n",
            " 0.85714286 0.83333333 0.57142857 0.57142857 0.85714286 0.57142857\n",
            " 0.85714286 0.57142857 0.57142857 0.71428571 0.85714286 0.83333333\n",
            " 0.71428571 0.71428571 0.71428571 0.71428571 0.42857143 0.57142857\n",
            " 0.57142857 0.71428571 0.71428571 0.66666667]\n",
            "Accuracy: 66.62% (12.21%)\n",
            "Execution Time: 2.32 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "bupa_scores = []\n",
        "bupa_mean = []\n",
        "bupa_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X_test, y_test, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    bupa_scores.append(results)\n",
        "    bupa_mean.append(results.mean()*100)\n",
        "    bupa_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Bupa Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Bupa scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(bupa_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Bupa'] = bupa_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>Bupa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>88.166667</td>\n",
              "      <td>97.082418</td>\n",
              "      <td>70.25</td>\n",
              "      <td>85.303571</td>\n",
              "      <td>65.190476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>96.434066</td>\n",
              "      <td>61.00</td>\n",
              "      <td>88.928571</td>\n",
              "      <td>67.619048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>91.750000</td>\n",
              "      <td>98.258242</td>\n",
              "      <td>62.05</td>\n",
              "      <td>90.607143</td>\n",
              "      <td>65.380952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>44.916667</td>\n",
              "      <td>55.824176</td>\n",
              "      <td>67.45</td>\n",
              "      <td>80.714286</td>\n",
              "      <td>52.380952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>78.250000</td>\n",
              "      <td>98.412088</td>\n",
              "      <td>62.00</td>\n",
              "      <td>84.750000</td>\n",
              "      <td>66.619048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer  Sonar  Ionosphere       Bupa\n",
              "0   AdaBoost  88.166667      97.082418  70.25   85.303571  65.190476\n",
              "1  GradBoost  87.000000      96.434066  61.00   88.928571  67.619048\n",
              "2   CatBoost  91.750000      98.258242  62.05   90.607143  65.380952\n",
              "3   LightGBM  44.916667      55.824176  67.45   80.714286  52.380952\n",
              "4    XGBoost  78.250000      98.412088  62.00   84.750000  66.619048"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Bupa'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Pima**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "pima_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\AprendizadoDeMaquina\\Seminario\\Datasets\\Pima\\Diabetes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (460, 9)\n",
            "Validation data shape: (154, 9)\n",
            "Test data shape: (154, 9)\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.6  # 60% for training\n",
        "val_ratio = 0.2    # 20% for validation\n",
        "test_ratio = 0.2   # 20% for testing\n",
        "\n",
        "# First, split the data into training and temporary sets\n",
        "temp_data, test_data = train_test_split(pima_df, test_size=test_ratio, random_state=42)\n",
        "\n",
        "# Then, split the temporary data into training and validation sets\n",
        "train_data, val_data = train_test_split(temp_data, test_size=val_ratio / (1 - test_ratio), random_state=42)\n",
        "\n",
        "# Now, you have your training, validation, and test sets\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: ((460, 8), (460,))\n",
            "Validation data shape: ((154, 8), (154,))\n",
            "Test data shape: ((154, 8), (154,))\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "X_train = train_data.iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "print(f\"Training data shape: {X_train.shape, y_train.shape}\")\n",
        "\n",
        "# Validation\n",
        "X_val = val_data.iloc[:, :-1]\n",
        "y_val = val_data.iloc[:, -1]\n",
        "print(f\"Validation data shape: {X_val.shape, y_val.shape}\")\n",
        "\n",
        "# Test\n",
        "X_test = test_data.iloc[:, :-1]\n",
        "y_test = test_data.iloc[:, -1]\n",
        "print(f\"Test data shape: {X_test.shape, y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_val = le.fit_transform(y_val)\n",
        "y_test = le.fit_transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X = pima_df.iloc[:, :-1]\n",
        "# y = pima_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:24<00:00,  1.69s/trial, best loss: -0.7922077922077922]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 850.0, 'learning_rate': 0.04142816402618046, 'max_depth': 5.0, 'max_features': None, 'min_samples_leaf': 2.0, 'min_samples_split': 8.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:27<00:00,  1.79trial/s, best loss: -0.7857142857142857]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': None, 'n_estimators': 750, 'learning_rate': 0.09835742587463962, 'max_depth': 2, 'min_samples_split': 3, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 0.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [01:08<00:00,  1.37s/trial, best loss: -0.7922077922077922]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 300, 'learning_rate': 0.07188209594317357, 'min_child_samples': 2, 'max_depth': 6, 'reg_lambda': 0.3038873988970403, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 41.01trial/s, best loss: -0.7857142857142857]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'gbdt', 'num_leaves': 55, 'learning_rate': 0.05784372191670596, 'min_child_samples': 90, 'reg_alpha': 0.465300872840409, 'reg_lambda': 3.854658467167749, 'colsample_by_tree': 0.3892702432379343, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:08<00:00,  5.91trial/s, best loss: -0.7857142857142857]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'dart', 'learning_rate': 0.09982749156805354, 'gamma': 4, 'max_depth': 6, 'min_child_weight': 6, 'colsample_bytree': 0.9365895999514975, 'colsample_bylevel': 0.7656934531515648, 'colsample_bynode': 0.9939257413783843, 'reg_alpha': 1.9856303891727132, 'reg_lambda': 3.7056219146347864, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Pima Dataset ---------\n",
            "[0.6875     0.4375     0.75       0.625      0.86666667 0.53333333\n",
            " 0.8        0.66666667 0.8        0.86666667 0.8125     0.5625\n",
            " 0.625      0.75       0.6        0.93333333 0.66666667 0.8\n",
            " 0.6        0.73333333 0.8125     0.75       0.75       0.6875\n",
            " 0.8        0.66666667 0.73333333 0.46666667 0.86666667 0.26666667\n",
            " 0.6875     0.8125     0.75       0.5625     0.66666667 0.8\n",
            " 0.8        0.73333333 0.86666667 0.66666667 0.75       0.8125\n",
            " 0.75       0.875      0.66666667 0.6        0.8        0.66666667\n",
            " 0.53333333 0.66666667 0.6875     0.625      0.75       0.6875\n",
            " 0.8        0.53333333 0.8        0.86666667 0.53333333 0.8\n",
            " 0.625      0.8125     0.8125     0.6875     0.6        0.73333333\n",
            " 0.66666667 0.8        0.6        0.53333333 0.5        0.75\n",
            " 0.5625     0.625      0.86666667 0.66666667 0.8        0.73333333\n",
            " 0.73333333 0.8        0.8125     0.75       0.75       0.75\n",
            " 0.8        0.73333333 0.53333333 0.73333333 0.66666667 0.73333333\n",
            " 0.8125     0.6875     0.625      0.4375     0.73333333 0.73333333\n",
            " 0.8        0.6        0.73333333 0.93333333]\n",
            "Accuracy: 70.73% (11.60%)\n",
            "Execution Time: 151.68 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Pima Dataset ---------\n",
            "[0.6875     0.6875     0.75       0.5625     0.8        0.6\n",
            " 0.66666667 0.8        0.66666667 0.8        0.6875     0.625\n",
            " 0.6875     0.75       0.73333333 0.8        0.66666667 0.93333333\n",
            " 0.66666667 0.8        0.5        0.75       0.625      0.8125\n",
            " 0.6        0.8        0.66666667 0.8        0.6        0.6\n",
            " 0.75       0.875      0.75       0.5        0.73333333 0.6\n",
            " 0.86666667 0.66666667 0.8        0.73333333 0.75       0.75\n",
            " 0.875      0.75       0.53333333 0.66666667 0.73333333 0.86666667\n",
            " 0.53333333 0.6        0.75       0.625      0.6875     0.625\n",
            " 0.66666667 0.66666667 0.73333333 0.73333333 0.8        0.8\n",
            " 0.75       0.6875     0.8125     0.6875     0.53333333 0.8\n",
            " 0.66666667 0.8        0.6        0.6        0.6875     0.75\n",
            " 0.5625     0.75       0.86666667 0.66666667 0.73333333 0.6\n",
            " 0.73333333 0.86666667 0.625      0.75       0.5        0.8125\n",
            " 0.73333333 0.73333333 0.6        0.86666667 0.8        0.66666667\n",
            " 0.75       0.75       0.75       0.5        0.8        0.8\n",
            " 0.8        0.46666667 0.8        0.8       ]\n",
            "Accuracy: 71.00% (9.96%)\n",
            "Execution Time: 45.41 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Pima Dataset ---------\n",
            "[0.8125     0.6875     0.75       0.625      0.93333333 0.66666667\n",
            " 0.66666667 0.8        0.73333333 0.8        0.75       0.625\n",
            " 0.625      0.75       0.73333333 0.86666667 0.73333333 0.86666667\n",
            " 0.66666667 0.73333333 0.6875     0.6875     0.6875     0.8125\n",
            " 0.66666667 0.86666667 0.6        0.8        0.73333333 0.66666667\n",
            " 0.75       0.8125     0.6875     0.5        0.8        0.66666667\n",
            " 0.93333333 0.8        0.86666667 0.6        0.8125     0.875\n",
            " 0.8125     0.8125     0.73333333 0.6        0.8        0.8\n",
            " 0.66666667 0.6        0.75       0.6875     0.6875     0.6875\n",
            " 0.73333333 0.66666667 0.73333333 0.93333333 0.86666667 0.73333333\n",
            " 0.6875     0.6875     0.75       0.6875     0.66666667 0.93333333\n",
            " 0.66666667 0.86666667 0.66666667 0.66666667 0.75       0.6875\n",
            " 0.6875     0.625      0.93333333 0.73333333 0.86666667 0.86666667\n",
            " 0.8        0.86666667 0.625      0.6875     0.75       0.875\n",
            " 0.8        0.73333333 0.66666667 0.8        0.73333333 0.8\n",
            " 0.75       0.6875     0.8125     0.625      0.86666667 0.73333333\n",
            " 0.73333333 0.46666667 0.8        0.8       ]\n",
            "Accuracy: 74.28% (9.32%)\n",
            "Execution Time: 48.57 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Pima Dataset ---------\n",
            "[0.375      0.375      0.375      0.375      0.66666667 0.66666667\n",
            " 0.66666667 0.66666667 0.66666667 0.4        0.375      0.375\n",
            " 0.375      0.375      0.66666667 0.66666667 0.66666667 0.66666667\n",
            " 0.66666667 0.4        0.375      0.375      0.375      0.375\n",
            " 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.4\n",
            " 0.375      0.375      0.375      0.375      0.66666667 0.66666667\n",
            " 0.66666667 0.66666667 0.66666667 0.4        0.375      0.375\n",
            " 0.375      0.375      0.66666667 0.66666667 0.66666667 0.66666667\n",
            " 0.66666667 0.4        0.375      0.375      0.375      0.375\n",
            " 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.4\n",
            " 0.375      0.375      0.375      0.375      0.66666667 0.66666667\n",
            " 0.66666667 0.66666667 0.66666667 0.4        0.375      0.375\n",
            " 0.375      0.375      0.66666667 0.66666667 0.66666667 0.66666667\n",
            " 0.66666667 0.4        0.375      0.375      0.375      0.375\n",
            " 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.4\n",
            " 0.375      0.375      0.375      0.375      0.66666667 0.66666667\n",
            " 0.66666667 0.66666667 0.66666667 0.4       ]\n",
            "Accuracy: 52.33% (14.35%)\n",
            "Execution Time: 0.57 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Pima Dataset ---------\n",
            "[0.625      0.75       0.6875     0.75       0.6        0.8\n",
            " 0.6        0.66666667 0.86666667 0.73333333 0.625      0.75\n",
            " 0.625      0.8125     0.66666667 0.66666667 0.66666667 0.8\n",
            " 0.73333333 0.6        0.75       0.6875     0.75       0.8125\n",
            " 0.66666667 0.66666667 0.6        0.8        0.86666667 0.6\n",
            " 0.75       0.8125     0.5625     0.75       0.6        0.73333333\n",
            " 0.8        0.66666667 0.86666667 0.6        0.8125     0.6875\n",
            " 0.75       0.6875     0.6        0.66666667 0.73333333 0.73333333\n",
            " 0.6        0.66666667 0.6875     0.75       0.75       0.6875\n",
            " 0.8        0.66666667 0.66666667 0.86666667 0.86666667 0.6\n",
            " 0.6875     0.6875     0.6875     0.75       0.66666667 0.8\n",
            " 0.73333333 0.86666667 0.4        0.53333333 0.6875     0.5625\n",
            " 0.5625     0.625      0.93333333 0.73333333 0.73333333 0.6\n",
            " 0.8        0.8        0.6875     0.75       0.625      0.75\n",
            " 0.8        0.73333333 0.66666667 0.66666667 0.73333333 0.8\n",
            " 0.75       0.8125     0.75       0.5        0.8        0.8\n",
            " 0.6        0.53333333 0.8        0.8       ]\n",
            "Accuracy: 70.85% (9.35%)\n",
            "Execution Time: 11.14 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "pima_scores = []\n",
        "pima_mean = []\n",
        "pima_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()     \n",
        "    results = cross_val_score(clf, X_test, y_test, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    pima_scores.append(results)\n",
        "    pima_mean.append(results.mean()*100)\n",
        "    pima_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Pima Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Pima scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(pima_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Pima'] = pima_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>Bupa</th>\n",
              "      <th>Pima</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>88.166667</td>\n",
              "      <td>97.082418</td>\n",
              "      <td>70.25</td>\n",
              "      <td>85.303571</td>\n",
              "      <td>65.190476</td>\n",
              "      <td>70.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>96.434066</td>\n",
              "      <td>61.00</td>\n",
              "      <td>88.928571</td>\n",
              "      <td>67.619048</td>\n",
              "      <td>71.004167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>91.750000</td>\n",
              "      <td>98.258242</td>\n",
              "      <td>62.05</td>\n",
              "      <td>90.607143</td>\n",
              "      <td>65.380952</td>\n",
              "      <td>74.283333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>44.916667</td>\n",
              "      <td>55.824176</td>\n",
              "      <td>67.45</td>\n",
              "      <td>80.714286</td>\n",
              "      <td>52.380952</td>\n",
              "      <td>52.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>78.250000</td>\n",
              "      <td>98.412088</td>\n",
              "      <td>62.00</td>\n",
              "      <td>84.750000</td>\n",
              "      <td>66.619048</td>\n",
              "      <td>70.854167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer  Sonar  Ionosphere       Bupa  \\\n",
              "0   AdaBoost  88.166667      97.082418  70.25   85.303571  65.190476   \n",
              "1  GradBoost  87.000000      96.434066  61.00   88.928571  67.619048   \n",
              "2   CatBoost  91.750000      98.258242  62.05   90.607143  65.380952   \n",
              "3   LightGBM  44.916667      55.824176  67.45   80.714286  52.380952   \n",
              "4    XGBoost  78.250000      98.412088  62.00   84.750000  66.619048   \n",
              "\n",
              "        Pima  \n",
              "0  70.733333  \n",
              "1  71.004167  \n",
              "2  74.283333  \n",
              "3  52.333333  \n",
              "4  70.854167  "
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Pima'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Heart**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "heart_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\AprendizadoDeMaquina\\Seminario\\Datasets\\Heart\\Heart.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (162, 14)\n",
            "Validation data shape: (54, 14)\n",
            "Test data shape: (54, 14)\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.6  # 60% for training\n",
        "val_ratio = 0.2    # 20% for validation\n",
        "test_ratio = 0.2   # 20% for testing\n",
        "\n",
        "# First, split the data into training and temporary sets\n",
        "temp_data, test_data = train_test_split(heart_df, test_size=test_ratio, random_state=42)\n",
        "\n",
        "# Then, split the temporary data into training and validation sets\n",
        "train_data, val_data = train_test_split(temp_data, test_size=val_ratio / (1 - test_ratio), random_state=42)\n",
        "\n",
        "# Now, you have your training, validation, and test sets\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: ((162, 13), (162,))\n",
            "Validation data shape: ((54, 13), (54,))\n",
            "Test data shape: ((54, 13), (54,))\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "X_train = train_data.iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "print(f\"Training data shape: {X_train.shape, y_train.shape}\")\n",
        "\n",
        "# Validation\n",
        "X_val = val_data.iloc[:, :-1]\n",
        "y_val = val_data.iloc[:, -1]\n",
        "print(f\"Validation data shape: {X_val.shape, y_val.shape}\")\n",
        "\n",
        "# Test\n",
        "X_test = test_data.iloc[:, :-1]\n",
        "y_test = test_data.iloc[:, -1]\n",
        "print(f\"Test data shape: {X_test.shape, y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_val = le.fit_transform(y_val)\n",
        "y_test = le.fit_transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X = heart_df.iloc[:, :-1]\n",
        "# y = heart_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:51<00:00,  1.02s/trial, best loss: -0.8148148148148148]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 600.0, 'learning_rate': 0.03167747886969513, 'max_depth': 1.0, 'max_features': None, 'min_samples_leaf': 3.0, 'min_samples_split': 4.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:17<00:00,  2.84trial/s, best loss: -0.8148148148148148]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': 'sqrt', 'n_estimators': 100, 'learning_rate': 0.04102652661864284, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.0, 'min_impurity_decrease': 1.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:51<00:00,  1.04s/trial, best loss: -0.8148148148148148]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 750, 'learning_rate': 0.02983152512960275, 'min_child_samples': 3, 'max_depth': 5, 'reg_lambda': 3.8771604915102147, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:00<00:00, 54.34trial/s, best loss: -0.8703703703703703]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'dart', 'num_leaves': 70, 'learning_rate': 0.07114958596849458, 'min_child_samples': 70, 'reg_alpha': 1.7296938688346057, 'reg_lambda': 3.5354753167111355, 'colsample_by_tree': 0.29964089123187165, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:03<00:00, 14.03trial/s, best loss: -0.8888888888888888]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'gbtree', 'learning_rate': 0.05925202569009605, 'gamma': 2, 'max_depth': 1, 'min_child_weight': 5, 'colsample_bytree': 0.8915564912396405, 'colsample_bylevel': 0.216645202224329, 'colsample_bynode': 0.7797956745786879, 'reg_alpha': 0.9517741146065928, 'reg_lambda': 1.424228333526235, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Heart Dataset ---------\n",
            "[0.83333333 0.66666667 1.         0.83333333 0.8        0.8\n",
            " 0.6        0.8        1.         0.6        0.66666667 0.83333333\n",
            " 0.66666667 0.66666667 0.8        0.8        0.6        1.\n",
            " 1.         0.6        0.83333333 0.66666667 0.83333333 0.5\n",
            " 0.8        0.6        0.4        1.         0.8        1.\n",
            " 0.83333333 0.66666667 0.66666667 1.         0.8        0.8\n",
            " 1.         0.8        0.8        0.6        0.66666667 1.\n",
            " 0.33333333 0.83333333 0.6        0.8        0.6        0.8\n",
            " 0.8        0.8        1.         0.83333333 0.66666667 0.66666667\n",
            " 1.         0.8        0.4        0.8        0.6        0.6\n",
            " 1.         0.66666667 0.83333333 1.         0.4        0.8\n",
            " 0.6        0.4        0.6        1.         0.5        0.83333333\n",
            " 0.83333333 0.66666667 0.8        0.6        0.8        1.\n",
            " 0.8        0.6        0.83333333 0.5        0.66666667 0.83333333\n",
            " 0.8        0.8        0.8        0.4        0.8        0.8\n",
            " 0.66666667 0.83333333 0.83333333 1.         0.8        0.8\n",
            " 1.         0.4        0.8        0.6       ]\n",
            "Accuracy: 75.27% (16.64%)\n",
            "Execution Time: 66.34 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Heart Dataset ---------\n",
            "[0.66666667 0.66666667 0.83333333 0.83333333 0.8        0.8\n",
            " 0.8        1.         0.8        0.6        0.83333333 0.83333333\n",
            " 0.66666667 0.66666667 0.8        0.8        0.6        0.8\n",
            " 1.         0.6        0.66666667 0.83333333 0.83333333 0.66666667\n",
            " 0.8        0.6        0.4        1.         0.6        1.\n",
            " 0.66666667 0.66666667 0.83333333 1.         0.8        0.8\n",
            " 1.         0.8        0.8        0.8        1.         1.\n",
            " 0.66666667 0.66666667 0.4        0.6        0.6        0.8\n",
            " 0.8        0.6        0.83333333 0.83333333 0.66666667 0.66666667\n",
            " 1.         0.8        0.8        0.6        0.4        0.6\n",
            " 1.         0.66666667 0.66666667 1.         0.8        0.8\n",
            " 0.6        0.8        1.         0.8        0.83333333 0.83333333\n",
            " 0.83333333 0.66666667 0.6        0.6        0.8        1.\n",
            " 0.6        0.8        0.83333333 0.5        0.83333333 0.5\n",
            " 0.8        0.8        0.8        0.6        0.8        0.8\n",
            " 0.66666667 0.66666667 0.83333333 0.66666667 0.6        0.8\n",
            " 1.         0.6        0.4        0.6       ]\n",
            "Accuracy: 75.10% (14.82%)\n",
            "Execution Time: 3.62 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Heart Dataset ---------\n",
            "[0.83333333 0.83333333 1.         0.83333333 0.8        0.8\n",
            " 1.         0.6        0.6        1.         0.83333333 0.83333333\n",
            " 0.66666667 0.83333333 0.8        0.8        1.         1.\n",
            " 0.8        0.4        0.83333333 0.5        1.         0.5\n",
            " 0.8        0.6        0.6        1.         0.6        0.8\n",
            " 0.83333333 1.         0.5        0.83333333 0.8        0.8\n",
            " 1.         0.6        0.8        0.6        0.83333333 1.\n",
            " 0.5        0.83333333 0.6        0.8        0.6        1.\n",
            " 0.8        0.8        1.         0.83333333 0.66666667 0.66666667\n",
            " 1.         0.6        0.4        1.         0.6        0.6\n",
            " 1.         0.66666667 0.66666667 1.         0.6        1.\n",
            " 0.6        0.6        0.6        1.         0.83333333 1.\n",
            " 1.         0.66666667 0.6        0.8        0.8        0.6\n",
            " 0.8        1.         0.83333333 0.5        0.66666667 0.66666667\n",
            " 0.8        1.         0.8        0.8        0.8        1.\n",
            " 1.         0.83333333 1.         0.83333333 0.6        0.8\n",
            " 1.         0.6        0.6        0.8       ]\n",
            "Accuracy: 78.17% (16.66%)\n",
            "Execution Time: 87.46 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Heart Dataset ---------\n",
            "[0.5        0.66666667 0.66666667 0.66666667 0.6        0.6\n",
            " 0.6        0.6        0.6        0.6        0.5        0.66666667\n",
            " 0.66666667 0.66666667 0.6        0.6        0.6        0.6\n",
            " 0.6        0.6        0.5        0.66666667 0.66666667 0.66666667\n",
            " 0.6        0.6        0.6        0.6        0.6        0.6\n",
            " 0.5        0.66666667 0.66666667 0.66666667 0.6        0.6\n",
            " 0.6        0.6        0.6        0.6        0.5        0.66666667\n",
            " 0.66666667 0.66666667 0.6        0.6        0.6        0.6\n",
            " 0.6        0.6        0.5        0.66666667 0.66666667 0.66666667\n",
            " 0.6        0.6        0.6        0.6        0.6        0.6\n",
            " 0.5        0.66666667 0.66666667 0.66666667 0.6        0.6\n",
            " 0.6        0.6        0.6        0.6        0.5        0.66666667\n",
            " 0.66666667 0.66666667 0.6        0.6        0.6        0.6\n",
            " 0.6        0.6        0.5        0.66666667 0.66666667 0.66666667\n",
            " 0.6        0.6        0.6        0.6        0.6        0.6\n",
            " 0.5        0.66666667 0.66666667 0.66666667 0.6        0.6\n",
            " 0.6        0.6        0.6        0.6       ]\n",
            "Accuracy: 61.00% (4.73%)\n",
            "Execution Time: 0.58 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Heart Dataset ---------\n",
            "[0.66666667 1.         0.5        0.83333333 0.6        0.6\n",
            " 0.6        0.6        0.8        0.6        0.33333333 0.83333333\n",
            " 0.66666667 0.66666667 0.8        0.6        0.8        0.8\n",
            " 0.8        0.6        0.5        0.33333333 0.83333333 0.83333333\n",
            " 0.8        0.6        0.4        0.6        0.8        0.8\n",
            " 0.5        0.83333333 0.66666667 0.83333333 0.4        0.6\n",
            " 0.8        0.6        0.8        0.4        0.5        0.83333333\n",
            " 1.         0.66666667 0.6        0.6        0.6        0.8\n",
            " 1.         0.6        0.5        0.83333333 0.5        0.66666667\n",
            " 1.         0.6        0.8        0.6        1.         0.6\n",
            " 0.5        0.66666667 1.         0.66666667 0.6        0.8\n",
            " 0.6        0.6        0.6        0.8        0.33333333 0.83333333\n",
            " 1.         0.83333333 0.6        0.6        0.6        0.8\n",
            " 0.6        0.6        0.5        0.66666667 0.83333333 0.83333333\n",
            " 0.8        1.         0.8        0.6        0.8        0.8\n",
            " 0.5        0.66666667 1.         0.66666667 0.6        0.8\n",
            " 0.6        0.6        0.8        0.6       ]\n",
            "Accuracy: 69.03% (16.09%)\n",
            "Execution Time: 2.12 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "heart_scores = []\n",
        "heart_mean = []\n",
        "heart_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X_test, y_test, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    heart_scores.append(results)\n",
        "    heart_mean.append(results.mean()*100)\n",
        "    heart_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Heart Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Heart scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(heart_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Heart'] = heart_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>Bupa</th>\n",
              "      <th>Pima</th>\n",
              "      <th>Heart</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>88.166667</td>\n",
              "      <td>97.082418</td>\n",
              "      <td>70.25</td>\n",
              "      <td>85.303571</td>\n",
              "      <td>65.190476</td>\n",
              "      <td>70.733333</td>\n",
              "      <td>75.266667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>96.434066</td>\n",
              "      <td>61.00</td>\n",
              "      <td>88.928571</td>\n",
              "      <td>67.619048</td>\n",
              "      <td>71.004167</td>\n",
              "      <td>75.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>91.750000</td>\n",
              "      <td>98.258242</td>\n",
              "      <td>62.05</td>\n",
              "      <td>90.607143</td>\n",
              "      <td>65.380952</td>\n",
              "      <td>74.283333</td>\n",
              "      <td>78.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>44.916667</td>\n",
              "      <td>55.824176</td>\n",
              "      <td>67.45</td>\n",
              "      <td>80.714286</td>\n",
              "      <td>52.380952</td>\n",
              "      <td>52.333333</td>\n",
              "      <td>61.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>78.250000</td>\n",
              "      <td>98.412088</td>\n",
              "      <td>62.00</td>\n",
              "      <td>84.750000</td>\n",
              "      <td>66.619048</td>\n",
              "      <td>70.854167</td>\n",
              "      <td>69.033333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer  Sonar  Ionosphere       Bupa  \\\n",
              "0   AdaBoost  88.166667      97.082418  70.25   85.303571  65.190476   \n",
              "1  GradBoost  87.000000      96.434066  61.00   88.928571  67.619048   \n",
              "2   CatBoost  91.750000      98.258242  62.05   90.607143  65.380952   \n",
              "3   LightGBM  44.916667      55.824176  67.45   80.714286  52.380952   \n",
              "4    XGBoost  78.250000      98.412088  62.00   84.750000  66.619048   \n",
              "\n",
              "        Pima      Heart  \n",
              "0  70.733333  75.266667  \n",
              "1  71.004167  75.100000  \n",
              "2  74.283333  78.166667  \n",
              "3  52.333333  61.000000  \n",
              "4  70.854167  69.033333  "
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Heart'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Liver**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "liver_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\AprendizadoDeMaquina\\Seminario\\Datasets\\Liver\\Liver.data', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (207, 7)\n",
            "Validation data shape: (69, 7)\n",
            "Test data shape: (69, 7)\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.6  # 60% for training\n",
        "val_ratio = 0.2    # 20% for validation\n",
        "test_ratio = 0.2   # 20% for testing\n",
        "\n",
        "# First, split the data into training and temporary sets\n",
        "temp_data, test_data = train_test_split(liver_df, test_size=test_ratio, random_state=42)\n",
        "\n",
        "# Then, split the temporary data into training and validation sets\n",
        "train_data, val_data = train_test_split(temp_data, test_size=val_ratio / (1 - test_ratio), random_state=42)\n",
        "\n",
        "# Now, you have your training, validation, and test sets\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: ((207, 6), (207,))\n",
            "Validation data shape: ((69, 6), (69,))\n",
            "Test data shape: ((69, 6), (69,))\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "X_train = train_data.iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "print(f\"Training data shape: {X_train.shape, y_train.shape}\")\n",
        "\n",
        "# Validation\n",
        "X_val = val_data.iloc[:, :-1]\n",
        "y_val = val_data.iloc[:, -1]\n",
        "print(f\"Validation data shape: {X_val.shape, y_val.shape}\")\n",
        "\n",
        "# Test\n",
        "X_test = test_data.iloc[:, :-1]\n",
        "y_test = test_data.iloc[:, -1]\n",
        "print(f\"Test data shape: {X_test.shape, y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_val = le.fit_transform(y_val)\n",
        "y_test = le.fit_transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X = liver_df.iloc[:, :-1]\n",
        "# y = liver_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:41<00:00,  1.21trial/s, best loss: -0.7681159420289855]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 200.0, 'learning_rate': 0.028745858811684347, 'max_depth': 2.0, 'max_features': 'log2', 'min_samples_leaf': 2.0, 'min_samples_split': 8.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:13<00:00,  3.65trial/s, best loss: -0.7681159420289855]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': 'sqrt', 'n_estimators': 100, 'learning_rate': 0.04102652661864284, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.0, 'min_impurity_decrease': 1.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:50<00:00,  1.01s/trial, best loss: -0.782608695652174] \n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 500, 'learning_rate': 0.01002981253798441, 'min_child_samples': 2, 'max_depth': 10, 'reg_lambda': 2.6701577095887252, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 47.29trial/s, best loss: -0.7391304347826086]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'gbdt', 'num_leaves': 45, 'learning_rate': 0.04824163099686186, 'min_child_samples': 40, 'reg_alpha': 0.4714117799670238, 'reg_lambda': 3.6517162746818896, 'colsample_by_tree': 0.6717443359974058, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:05<00:00,  9.84trial/s, best loss: -0.7971014492753623]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'gbtree', 'learning_rate': 0.011777426690454684, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'colsample_bytree': 0.6642423404208758, 'colsample_bylevel': 0.8389604376670141, 'colsample_bynode': 0.46801910869053165, 'reg_alpha': 1.3842922617481603, 'reg_lambda': 0.25127542856871243, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return -accuracy_score(y_val, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['AdaBoost', 'GradBoost', 'CatBoost', 'LightGBM', 'XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Liver Dataset ---------\n",
            "[0.57142857 0.57142857 0.28571429 0.71428571 0.71428571 0.71428571\n",
            " 0.28571429 0.57142857 0.85714286 0.66666667 0.71428571 0.57142857\n",
            " 0.71428571 0.71428571 0.71428571 0.71428571 0.42857143 0.85714286\n",
            " 0.28571429 0.66666667 0.57142857 0.42857143 0.57142857 0.85714286\n",
            " 0.57142857 0.71428571 0.57142857 0.71428571 0.57142857 0.66666667\n",
            " 0.57142857 0.85714286 0.57142857 0.71428571 0.57142857 0.71428571\n",
            " 0.71428571 0.42857143 0.71428571 0.83333333 0.57142857 0.71428571\n",
            " 0.57142857 0.85714286 0.42857143 0.71428571 0.57142857 0.57142857\n",
            " 0.57142857 0.83333333 0.57142857 0.85714286 0.57142857 0.85714286\n",
            " 0.71428571 0.85714286 0.57142857 0.85714286 0.42857143 0.5\n",
            " 0.71428571 0.42857143 0.85714286 0.57142857 0.71428571 0.57142857\n",
            " 0.71428571 0.57142857 0.71428571 0.83333333 0.71428571 0.85714286\n",
            " 0.57142857 0.85714286 0.42857143 0.85714286 0.42857143 0.71428571\n",
            " 0.71428571 0.66666667 0.57142857 0.71428571 0.85714286 0.57142857\n",
            " 0.71428571 0.85714286 0.42857143 0.71428571 0.57142857 0.83333333\n",
            " 0.71428571 0.71428571 0.71428571 0.71428571 0.42857143 0.57142857\n",
            " 0.42857143 0.57142857 0.71428571 0.83333333]\n",
            "Accuracy: 65.19% (14.43%)\n",
            "Execution Time: 22.49 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Liver Dataset ---------\n",
            "[0.71428571 0.71428571 0.57142857 0.71428571 0.71428571 0.57142857\n",
            " 0.71428571 0.71428571 1.         0.66666667 0.85714286 0.71428571\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.71428571 1.\n",
            " 0.85714286 0.66666667 0.42857143 0.57142857 0.57142857 0.85714286\n",
            " 0.71428571 0.71428571 0.57142857 0.85714286 0.71428571 0.83333333\n",
            " 0.57142857 0.57142857 0.71428571 0.57142857 0.57142857 0.57142857\n",
            " 0.71428571 0.71428571 0.71428571 0.83333333 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.71428571 0.57142857 0.71428571 0.71428571\n",
            " 0.57142857 0.83333333 0.57142857 0.71428571 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.42857143 0.85714286 0.85714286 0.5\n",
            " 0.71428571 0.57142857 0.85714286 0.57142857 0.71428571 0.57142857\n",
            " 0.71428571 0.57142857 0.71428571 0.83333333 0.85714286 0.71428571\n",
            " 0.57142857 0.71428571 0.57142857 0.57142857 0.57142857 0.85714286\n",
            " 0.71428571 0.83333333 0.71428571 0.57142857 0.71428571 0.71428571\n",
            " 0.85714286 0.57142857 0.57142857 0.71428571 0.71428571 0.66666667\n",
            " 0.85714286 0.85714286 0.57142857 0.71428571 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.85714286 0.66666667]\n",
            "Accuracy: 67.62% (11.87%)\n",
            "Execution Time: 3.51 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Liver Dataset ---------\n",
            "[0.57142857 0.85714286 0.57142857 0.71428571 0.28571429 0.71428571\n",
            " 0.71428571 0.57142857 0.71428571 0.5        0.85714286 0.57142857\n",
            " 0.57142857 0.71428571 0.57142857 0.57142857 0.57142857 0.85714286\n",
            " 0.57142857 0.66666667 0.57142857 0.57142857 0.71428571 0.71428571\n",
            " 0.71428571 0.71428571 0.85714286 0.71428571 0.42857143 0.5\n",
            " 0.42857143 0.85714286 0.85714286 0.57142857 0.71428571 0.57142857\n",
            " 0.57142857 0.57142857 0.71428571 0.66666667 0.71428571 0.71428571\n",
            " 0.42857143 0.85714286 0.42857143 0.71428571 0.28571429 0.71428571\n",
            " 0.57142857 1.         0.71428571 0.57142857 0.57142857 0.85714286\n",
            " 0.57142857 0.85714286 0.57142857 0.85714286 0.28571429 0.5\n",
            " 0.57142857 0.85714286 0.85714286 0.42857143 0.42857143 0.85714286\n",
            " 0.71428571 0.57142857 0.71428571 0.83333333 0.85714286 0.85714286\n",
            " 0.42857143 0.71428571 0.28571429 1.         0.42857143 0.85714286\n",
            " 0.57142857 0.66666667 0.57142857 0.85714286 0.71428571 0.42857143\n",
            " 0.85714286 0.71428571 0.28571429 0.71428571 0.71428571 0.66666667\n",
            " 0.85714286 0.85714286 1.         0.57142857 0.57142857 0.57142857\n",
            " 0.42857143 0.57142857 0.71428571 0.66666667]\n",
            "Accuracy: 65.38% (16.60%)\n",
            "Execution Time: 143.05 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Liver Dataset ---------\n",
            "[0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.28571429 0.28571429 0.66666667 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.28571429\n",
            " 0.28571429 0.66666667 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.57142857 0.28571429 0.28571429 0.66666667\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.28571429 0.28571429 0.66666667 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.28571429\n",
            " 0.28571429 0.66666667 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.57142857 0.28571429 0.28571429 0.66666667\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.28571429 0.28571429 0.66666667 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.28571429\n",
            " 0.28571429 0.66666667 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.57142857 0.57142857 0.28571429 0.28571429 0.66666667\n",
            " 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857 0.57142857\n",
            " 0.57142857 0.28571429 0.28571429 0.66666667]\n",
            "Accuracy: 52.38% (12.23%)\n",
            "Execution Time: 0.58 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Liver Dataset ---------\n",
            "[0.71428571 0.71428571 0.42857143 0.71428571 0.57142857 0.57142857\n",
            " 0.71428571 0.71428571 1.         0.66666667 0.57142857 0.71428571\n",
            " 0.57142857 0.42857143 0.57142857 0.57142857 0.57142857 1.\n",
            " 0.71428571 0.66666667 0.42857143 0.57142857 0.57142857 0.71428571\n",
            " 0.71428571 0.57142857 0.85714286 0.71428571 0.71428571 0.66666667\n",
            " 0.71428571 0.71428571 0.71428571 0.57142857 0.57142857 0.85714286\n",
            " 0.57142857 0.71428571 0.57142857 0.83333333 0.57142857 0.57142857\n",
            " 0.57142857 0.85714286 0.57142857 0.71428571 0.71428571 0.71428571\n",
            " 0.71428571 0.66666667 0.71428571 0.71428571 0.57142857 0.57142857\n",
            " 0.57142857 0.71428571 0.28571429 0.71428571 0.85714286 0.66666667\n",
            " 0.71428571 0.85714286 0.57142857 0.57142857 0.71428571 0.71428571\n",
            " 0.57142857 0.57142857 0.57142857 0.83333333 0.85714286 0.57142857\n",
            " 0.71428571 0.57142857 0.57142857 0.57142857 0.57142857 0.71428571\n",
            " 0.85714286 0.83333333 0.57142857 0.57142857 0.85714286 0.57142857\n",
            " 0.85714286 0.57142857 0.57142857 0.71428571 0.85714286 0.83333333\n",
            " 0.71428571 0.71428571 0.71428571 0.71428571 0.42857143 0.57142857\n",
            " 0.57142857 0.71428571 0.71428571 0.66666667]\n",
            "Accuracy: 66.62% (12.21%)\n",
            "Execution Time: 2.37 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "liver_scores = []\n",
        "liver_mean = []\n",
        "liver_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X_test, y_test, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    liver_scores.append(results)\n",
        "    liver_mean.append(results.mean()*100)\n",
        "    liver_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Liver Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Liver scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(liver_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Liver'] = liver_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>Bupa</th>\n",
              "      <th>Pima</th>\n",
              "      <th>Heart</th>\n",
              "      <th>Liver</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>88.166667</td>\n",
              "      <td>97.082418</td>\n",
              "      <td>70.25</td>\n",
              "      <td>85.303571</td>\n",
              "      <td>65.190476</td>\n",
              "      <td>70.733333</td>\n",
              "      <td>75.266667</td>\n",
              "      <td>65.190476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>96.434066</td>\n",
              "      <td>61.00</td>\n",
              "      <td>88.928571</td>\n",
              "      <td>67.619048</td>\n",
              "      <td>71.004167</td>\n",
              "      <td>75.100000</td>\n",
              "      <td>67.619048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>91.750000</td>\n",
              "      <td>98.258242</td>\n",
              "      <td>62.05</td>\n",
              "      <td>90.607143</td>\n",
              "      <td>65.380952</td>\n",
              "      <td>74.283333</td>\n",
              "      <td>78.166667</td>\n",
              "      <td>65.380952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>44.916667</td>\n",
              "      <td>55.824176</td>\n",
              "      <td>67.45</td>\n",
              "      <td>80.714286</td>\n",
              "      <td>52.380952</td>\n",
              "      <td>52.333333</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>52.380952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>78.250000</td>\n",
              "      <td>98.412088</td>\n",
              "      <td>62.00</td>\n",
              "      <td>84.750000</td>\n",
              "      <td>66.619048</td>\n",
              "      <td>70.854167</td>\n",
              "      <td>69.033333</td>\n",
              "      <td>66.619048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer  Sonar  Ionosphere       Bupa  \\\n",
              "0   AdaBoost  88.166667      97.082418  70.25   85.303571  65.190476   \n",
              "1  GradBoost  87.000000      96.434066  61.00   88.928571  67.619048   \n",
              "2   CatBoost  91.750000      98.258242  62.05   90.607143  65.380952   \n",
              "3   LightGBM  44.916667      55.824176  67.45   80.714286  52.380952   \n",
              "4    XGBoost  78.250000      98.412088  62.00   84.750000  66.619048   \n",
              "\n",
              "        Pima      Heart      Liver  \n",
              "0  70.733333  75.266667  65.190476  \n",
              "1  71.004167  75.100000  67.619048  \n",
              "2  74.283333  78.166667  65.380952  \n",
              "3  52.333333  61.000000  52.380952  \n",
              "4  70.854167  69.033333  66.619048  "
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_results_tr = Algo_results.set_index('Names').T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_results_tr.to_csv('E:\\Cursos\\MestradoCienciaComputação\\AprendizadoDeMaquina\\Seminario\\Code\\AlgorithmComparison\\AlgorithmComparison\\TunedAlgoResults.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Names</th>\n",
              "      <th>AdaBoost</th>\n",
              "      <th>GradBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>XGBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Wine</th>\n",
              "      <td>88.166667</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>91.750000</td>\n",
              "      <td>44.916667</td>\n",
              "      <td>78.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <td>97.082418</td>\n",
              "      <td>96.434066</td>\n",
              "      <td>98.258242</td>\n",
              "      <td>55.824176</td>\n",
              "      <td>98.412088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sonar</th>\n",
              "      <td>70.250000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>62.050000</td>\n",
              "      <td>67.450000</td>\n",
              "      <td>62.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ionosphere</th>\n",
              "      <td>85.303571</td>\n",
              "      <td>88.928571</td>\n",
              "      <td>90.607143</td>\n",
              "      <td>80.714286</td>\n",
              "      <td>84.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bupa</th>\n",
              "      <td>65.190476</td>\n",
              "      <td>67.619048</td>\n",
              "      <td>65.380952</td>\n",
              "      <td>52.380952</td>\n",
              "      <td>66.619048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pima</th>\n",
              "      <td>70.733333</td>\n",
              "      <td>71.004167</td>\n",
              "      <td>74.283333</td>\n",
              "      <td>52.333333</td>\n",
              "      <td>70.854167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Heart</th>\n",
              "      <td>75.266667</td>\n",
              "      <td>75.100000</td>\n",
              "      <td>78.166667</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>69.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Liver</th>\n",
              "      <td>65.190476</td>\n",
              "      <td>67.619048</td>\n",
              "      <td>65.380952</td>\n",
              "      <td>52.380952</td>\n",
              "      <td>66.619048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Names           AdaBoost  GradBoost   CatBoost   LightGBM    XGBoost\n",
              "Wine           88.166667  87.000000  91.750000  44.916667  78.250000\n",
              "Breast_Cancer  97.082418  96.434066  98.258242  55.824176  98.412088\n",
              "Sonar          70.250000  61.000000  62.050000  67.450000  62.000000\n",
              "Ionosphere     85.303571  88.928571  90.607143  80.714286  84.750000\n",
              "Bupa           65.190476  67.619048  65.380952  52.380952  66.619048\n",
              "Pima           70.733333  71.004167  74.283333  52.333333  70.854167\n",
              "Heart          75.266667  75.100000  78.166667  61.000000  69.033333\n",
              "Liver          65.190476  67.619048  65.380952  52.380952  66.619048"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Tuned_Algo_results_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Liver'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>Bupa</th>\n",
              "      <th>Pima</th>\n",
              "      <th>Heart</th>\n",
              "      <th>Liver</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>1.013227</td>\n",
              "      <td>34.874733</td>\n",
              "      <td>172.098242</td>\n",
              "      <td>146.819920</td>\n",
              "      <td>22.174649</td>\n",
              "      <td>151.680956</td>\n",
              "      <td>66.337747</td>\n",
              "      <td>22.492951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.996717</td>\n",
              "      <td>12.208340</td>\n",
              "      <td>49.973433</td>\n",
              "      <td>86.928549</td>\n",
              "      <td>3.489784</td>\n",
              "      <td>45.408070</td>\n",
              "      <td>3.617876</td>\n",
              "      <td>3.512265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>53.566599</td>\n",
              "      <td>270.585441</td>\n",
              "      <td>67.879931</td>\n",
              "      <td>65.282004</td>\n",
              "      <td>141.794038</td>\n",
              "      <td>48.567595</td>\n",
              "      <td>87.458145</td>\n",
              "      <td>143.050683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>0.755169</td>\n",
              "      <td>0.595133</td>\n",
              "      <td>1.031106</td>\n",
              "      <td>1.427489</td>\n",
              "      <td>0.559126</td>\n",
              "      <td>0.566139</td>\n",
              "      <td>0.581154</td>\n",
              "      <td>0.584130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>17.993899</td>\n",
              "      <td>10.222295</td>\n",
              "      <td>4.016673</td>\n",
              "      <td>11.443568</td>\n",
              "      <td>2.322521</td>\n",
              "      <td>11.136394</td>\n",
              "      <td>2.121266</td>\n",
              "      <td>2.370531</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer       Sonar  Ionosphere        Bupa  \\\n",
              "0   AdaBoost   1.013227      34.874733  172.098242  146.819920   22.174649   \n",
              "1  GradBoost  98.996717      12.208340   49.973433   86.928549    3.489784   \n",
              "2   CatBoost  53.566599     270.585441   67.879931   65.282004  141.794038   \n",
              "3   LightGBM   0.755169       0.595133    1.031106    1.427489    0.559126   \n",
              "4    XGBoost  17.993899      10.222295    4.016673   11.443568    2.322521   \n",
              "\n",
              "         Pima      Heart       Liver  \n",
              "0  151.680956  66.337747   22.492951  \n",
              "1   45.408070   3.617876    3.512265  \n",
              "2   48.567595  87.458145  143.050683  \n",
              "3    0.566139   0.581154    0.584130  \n",
              "4   11.136394   2.121266    2.370531  "
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_time_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_time_results_tr = Algo_time_results.set_index('Names').T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_time_results_tr.to_csv('E:\\Cursos\\MestradoCienciaComputação\\AprendizadoDeMaquina\\Seminario\\Code\\AlgorithmComparison\\AlgorithmComparison\\TunedAlgoTimeResults.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Friedman's Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-posthocs in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (0.7.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (1.25.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (1.11.2)\n",
            "Requirement already satisfied: statsmodels in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (0.14.0)\n",
            "Requirement already satisfied: pandas>=0.20.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (2.1.0)\n",
            "Requirement already satisfied: seaborn in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (0.12.2)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (3.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.20.0->scikit-posthocs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.20.0->scikit-posthocs) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.20.0->scikit-posthocs) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (10.0.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (6.0.1)\n",
            "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from statsmodels->scikit-posthocs) (0.5.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->scikit-posthocs) (3.16.2)\n",
            "Requirement already satisfied: six in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from patsy>=0.5.2->statsmodels->scikit-posthocs) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-posthocs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scikit_posthocs as sp\n",
        "from scipy.stats import friedmanchisquare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_results_tr = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\AprendizadoDeMaquina\\Seminario\\Code\\AlgorithmComparison\\AlgorithmComparison\\TunedAlgoResults.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AdaBoost</th>\n",
              "      <th>GradBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>XGBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>88.166667</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>91.750000</td>\n",
              "      <td>44.916667</td>\n",
              "      <td>78.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>97.082418</td>\n",
              "      <td>96.434066</td>\n",
              "      <td>98.258242</td>\n",
              "      <td>55.824176</td>\n",
              "      <td>98.412088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70.250000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>62.050000</td>\n",
              "      <td>67.450000</td>\n",
              "      <td>62.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>85.303571</td>\n",
              "      <td>88.928571</td>\n",
              "      <td>90.607143</td>\n",
              "      <td>80.714286</td>\n",
              "      <td>84.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>65.190476</td>\n",
              "      <td>67.619048</td>\n",
              "      <td>65.380952</td>\n",
              "      <td>52.380952</td>\n",
              "      <td>66.619048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>70.733333</td>\n",
              "      <td>71.004167</td>\n",
              "      <td>74.283333</td>\n",
              "      <td>52.333333</td>\n",
              "      <td>70.854167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>75.266667</td>\n",
              "      <td>75.100000</td>\n",
              "      <td>78.166667</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>69.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>65.190476</td>\n",
              "      <td>67.619048</td>\n",
              "      <td>65.380952</td>\n",
              "      <td>52.380952</td>\n",
              "      <td>66.619048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    AdaBoost  GradBoost   CatBoost   LightGBM    XGBoost\n",
              "0  88.166667  87.000000  91.750000  44.916667  78.250000\n",
              "1  97.082418  96.434066  98.258242  55.824176  98.412088\n",
              "2  70.250000  61.000000  62.050000  67.450000  62.000000\n",
              "3  85.303571  88.928571  90.607143  80.714286  84.750000\n",
              "4  65.190476  67.619048  65.380952  52.380952  66.619048\n",
              "5  70.733333  71.004167  74.283333  52.333333  70.854167\n",
              "6  75.266667  75.100000  78.166667  61.000000  69.033333\n",
              "7  65.190476  67.619048  65.380952  52.380952  66.619048"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Tuned_Algo_results_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "stat, p_value = friedmanchisquare(Tuned_Algo_results_tr['AdaBoost'], Tuned_Algo_results_tr['GradBoost'], Tuned_Algo_results_tr['CatBoost'], Tuned_Algo_results_tr['LightGBM'], Tuned_Algo_results_tr['XGBoost'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.01127579394733179"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are significant differences among the models.\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print('There are significant differences among the models.')\n",
        "else:\n",
        "    print('There are no significant differences among the models.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Nemenyi test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "nemenyi_results = sp.posthoc_nemenyi_friedman(a=Tuned_Algo_results_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AdaBoost</th>\n",
              "      <th>GradBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>XGBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AdaBoost</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.687481</td>\n",
              "      <td>0.174719</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GradBoost</th>\n",
              "      <td>0.900000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.866947</td>\n",
              "      <td>0.084282</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CatBoost</th>\n",
              "      <td>0.687481</td>\n",
              "      <td>0.866947</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.004591</td>\n",
              "      <td>0.597744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LightGBM</th>\n",
              "      <td>0.174719</td>\n",
              "      <td>0.084282</td>\n",
              "      <td>0.004591</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.239603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost</th>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.597744</td>\n",
              "      <td>0.239603</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           AdaBoost  GradBoost  CatBoost  LightGBM   XGBoost\n",
              "AdaBoost   1.000000   0.900000  0.687481  0.174719  0.900000\n",
              "GradBoost  0.900000   1.000000  0.866947  0.084282  0.900000\n",
              "CatBoost   0.687481   0.866947  1.000000  0.004591  0.597744\n",
              "LightGBM   0.174719   0.084282  0.004591  1.000000  0.239603\n",
              "XGBoost    0.900000   0.900000  0.597744  0.239603  1.000000"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nemenyi_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models 1 and 2 are not significantly different (p-value = 0.9000).\n",
            "Models 1 and 3 are not significantly different (p-value = 0.6875).\n",
            "Models 1 and 4 are not significantly different (p-value = 0.1747).\n",
            "Models 1 and 5 are not significantly different (p-value = 0.9000).\n",
            "Models 2 and 3 are not significantly different (p-value = 0.8669).\n",
            "Models 2 and 4 are not significantly different (p-value = 0.0843).\n",
            "Models 2 and 5 are not significantly different (p-value = 0.9000).\n",
            "Models 3 and 4 are significantly different (p-value = 0.0046).\n",
            "Models 3 and 5 are not significantly different (p-value = 0.5977).\n",
            "Models 4 and 5 are not significantly different (p-value = 0.2396).\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.05\n",
        "\n",
        "# Print p-values for all pairs of models\n",
        "for i in range(nemenyi_results.shape[0]):\n",
        "    for j in range(i + 1, nemenyi_results.shape[1]):\n",
        "        model1 = i + 1\n",
        "        model2 = j + 1\n",
        "        p_value = nemenyi_results.iloc[i, j]\n",
        "\n",
        "        if p_value < alpha:\n",
        "            print(f\"Models {model1} and {model2} are significantly different (p-value = {p_value:.4f}).\")\n",
        "        else:\n",
        "            print(f\"Models {model1} and {model2} are not significantly different (p-value = {p_value:.4f}).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Algorithms running time Friedman's Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_results_tr = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\AprendizadoDeMaquina\\Seminario\\Code\\AlgorithmComparison\\AlgorithmComparison\\TunedAlgoTimeResults.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AdaBoost</th>\n",
              "      <th>GradBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>XGBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.013227</td>\n",
              "      <td>98.996717</td>\n",
              "      <td>53.566599</td>\n",
              "      <td>0.755169</td>\n",
              "      <td>17.993899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>34.874733</td>\n",
              "      <td>12.208340</td>\n",
              "      <td>270.585441</td>\n",
              "      <td>0.595133</td>\n",
              "      <td>10.222295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>172.098242</td>\n",
              "      <td>49.973433</td>\n",
              "      <td>67.879931</td>\n",
              "      <td>1.031106</td>\n",
              "      <td>4.016673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>146.819920</td>\n",
              "      <td>86.928549</td>\n",
              "      <td>65.282004</td>\n",
              "      <td>1.427489</td>\n",
              "      <td>11.443568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>22.174649</td>\n",
              "      <td>3.489784</td>\n",
              "      <td>141.794038</td>\n",
              "      <td>0.559126</td>\n",
              "      <td>2.322521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>151.680956</td>\n",
              "      <td>45.408070</td>\n",
              "      <td>48.567595</td>\n",
              "      <td>0.566139</td>\n",
              "      <td>11.136394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>66.337747</td>\n",
              "      <td>3.617876</td>\n",
              "      <td>87.458145</td>\n",
              "      <td>0.581154</td>\n",
              "      <td>2.121266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>22.492951</td>\n",
              "      <td>3.512265</td>\n",
              "      <td>143.050683</td>\n",
              "      <td>0.584130</td>\n",
              "      <td>2.370531</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     AdaBoost  GradBoost    CatBoost  LightGBM    XGBoost\n",
              "0    1.013227  98.996717   53.566599  0.755169  17.993899\n",
              "1   34.874733  12.208340  270.585441  0.595133  10.222295\n",
              "2  172.098242  49.973433   67.879931  1.031106   4.016673\n",
              "3  146.819920  86.928549   65.282004  1.427489  11.443568\n",
              "4   22.174649   3.489784  141.794038  0.559126   2.322521\n",
              "5  151.680956  45.408070   48.567595  0.566139  11.136394\n",
              "6   66.337747   3.617876   87.458145  0.581154   2.121266\n",
              "7   22.492951   3.512265  143.050683  0.584130   2.370531"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Tuned_Algo_results_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "stat, p_value = friedmanchisquare(Tuned_Algo_results_tr['AdaBoost'], Tuned_Algo_results_tr['GradBoost'], Tuned_Algo_results_tr['CatBoost'], Tuned_Algo_results_tr['LightGBM'], Tuned_Algo_results_tr['XGBoost'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.472289952954606e-05"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are significant running time differences among the algorithm.\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print('There are significant running time differences among the algorithm.')\n",
        "else:\n",
        "    print('There are no significant running time differences among the algorithm.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Algorithms running time Nemenyi test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "nemenyi_results = sp.posthoc_nemenyi_friedman(a=Tuned_Algo_results_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AdaBoost</th>\n",
              "      <th>GradBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>XGBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AdaBoost</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.866947</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.084282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GradBoost</th>\n",
              "      <td>0.866947</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.687481</td>\n",
              "      <td>0.022374</td>\n",
              "      <td>0.508007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CatBoost</th>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.687481</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.035854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LightGBM</th>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.022374</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.597744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost</th>\n",
              "      <td>0.084282</td>\n",
              "      <td>0.508007</td>\n",
              "      <td>0.035854</td>\n",
              "      <td>0.597744</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           AdaBoost  GradBoost  CatBoost  LightGBM   XGBoost\n",
              "AdaBoost   1.000000   0.866947  0.900000  0.001000  0.084282\n",
              "GradBoost  0.866947   1.000000  0.687481  0.022374  0.508007\n",
              "CatBoost   0.900000   0.687481  1.000000  0.001000  0.035854\n",
              "LightGBM   0.001000   0.022374  0.001000  1.000000  0.597744\n",
              "XGBoost    0.084282   0.508007  0.035854  0.597744  1.000000"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nemenyi_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Algorithm 1 and 2 are not significantly different (p-value = 0.8669).\n",
            "Algorithm 1 and 3 are not significantly different (p-value = 0.9000).\n",
            "Algorithm 1 and 4 are significantly different (p-value = 0.0010).\n",
            "Algorithm 1 and 5 are not significantly different (p-value = 0.0843).\n",
            "Algorithm 2 and 3 are not significantly different (p-value = 0.6875).\n",
            "Algorithm 2 and 4 are significantly different (p-value = 0.0224).\n",
            "Algorithm 2 and 5 are not significantly different (p-value = 0.5080).\n",
            "Algorithm 3 and 4 are significantly different (p-value = 0.0010).\n",
            "Algorithm 3 and 5 are significantly different (p-value = 0.0359).\n",
            "Algorithm 4 and 5 are not significantly different (p-value = 0.5977).\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.05\n",
        "\n",
        "# Print p-values for all pairs of models\n",
        "for i in range(nemenyi_results.shape[0]):\n",
        "    for j in range(i + 1, nemenyi_results.shape[1]):\n",
        "        model1 = i + 1\n",
        "        model2 = j + 1\n",
        "        p_value = nemenyi_results.iloc[i, j]\n",
        "\n",
        "        if p_value < alpha:\n",
        "            print(f\"Algorithm {model1} and {model2} are significantly different (p-value = {p_value:.4f}).\")\n",
        "        else:\n",
        "            print(f\"Algorithm {model1} and {model2} are not significantly different (p-value = {p_value:.4f}).\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMVO8koMTTTdYQJS3YoNuih",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
