{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RochaErik/AlgorithmComparison/blob/main/AlgorithmComparison3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eemeaAaCsyS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7sPV5hJCeJ2"
      },
      "source": [
        "# **Evaluating algorithms with hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knwYV1QmCuEU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnwsW6x9w1QO",
        "outputId": "fad3ad0d-f838-4cfe-e95c-f8295d5fd365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (1.2.1)\n",
            "Requirement already satisfied: graphviz in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (3.7.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (2.1.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (1.11.2)\n",
            "Requirement already satisfied: plotly in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (5.16.1)\n",
            "Requirement already satisfied: six in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (10.0.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->catboost) (6.0.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from plotly->catboost) (8.2.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.16.2)\n",
            "Requirement already satisfied: lightgbm in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (4.0.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from lightgbm) (1.25.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from lightgbm) (1.11.2)\n",
            "Requirement already satisfied: xgboost in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (1.7.6)\n",
            "Requirement already satisfied: numpy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from xgboost) (1.11.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost\n",
        "!pip install lightgbm\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sp9bGvxdqiOw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "import scipy.stats as stats\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB5jc4iDCWZI",
        "outputId": "a417925a-0c0b-4a02-e9a2-ca428226ab51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hyperopt in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (1.25.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (1.11.2)\n",
            "Requirement already satisfied: six in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (3.1)\n",
            "Requirement already satisfied: future in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (0.18.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (4.66.1)\n",
            "Requirement already satisfied: cloudpickle in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (2.2.1)\n",
            "Requirement already satisfied: py4j in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from hyperopt) (0.10.9.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from tqdm->hyperopt) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install hyperopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pnmKn_fsDTha"
      },
      "outputs": [],
      "source": [
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByCnDDmkDayW"
      },
      "source": [
        "# **Wine Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "23mGy-W6DZLy"
      },
      "outputs": [],
      "source": [
        "wine_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Wine\\wine.data', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C0N1S4LWDnbw"
      },
      "outputs": [],
      "source": [
        "X = wine_df.iloc[:, 1:]\n",
        "y = wine_df.iloc[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "omlj8qxkDoM1"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bEtKdQvTEsAR"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcZuN-z4CdXh",
        "outputId": "ee31c32a-6b6b-467e-f741-153da73f7c60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:24<00:00,  2.06trial/s, best loss: -1.0]              \n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 200.0, 'learning_rate': 0.06659352635164861, 'max_depth': 4.0, 'max_features': 'sqrt', 'min_samples_leaf': 3.0, 'min_samples_split': 2.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [01:11<00:00,  1.43s/trial, best loss: -1.0]              \n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': 'sqrt', 'n_estimators': 100, 'learning_rate': 0.04102652661864284, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.0, 'min_impurity_decrease': 1.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [02:25<00:00,  2.92s/trial, best loss: -1.0]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 550, 'learning_rate': 0.0479901225935416, 'min_child_samples': 1, 'max_depth': 6, 'reg_lambda': 3.3766279624518107, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 30.09trial/s, best loss: -0.9722222222222222]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'gbdt', 'num_leaves': 55, 'learning_rate': 0.04496177447997528, 'min_child_samples': 10, 'reg_alpha': 0.3916912792044354, 'reg_lambda': 1.4941077467431771, 'colsample_by_tree': 0.379259630420579, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:16<00:00,  3.05trial/s, best loss: -1.0]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'dart', 'learning_rate': 0.09292666170093178, 'gamma': 4, 'max_depth': 4, 'min_child_weight': 1, 'colsample_bytree': 0.8943278668489419, 'colsample_bylevel': 0.2640104690942444, 'colsample_bynode': 0.8937107554719765, 'reg_alpha': 0.056770729092546546, 'reg_lambda': 4.219736540591216, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 200.0,\n",
              " 'learning_rate': 0.06659352635164861,\n",
              " 'max_depth': 4.0,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_leaf': 3.0,\n",
              " 'min_samples_split': 2.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['AdaBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'criterion': 'friedman_mse',\n",
              " 'max_features': 'sqrt',\n",
              " 'n_estimators': 100,\n",
              " 'learning_rate': 0.04102652661864284,\n",
              " 'max_depth': 3,\n",
              " 'min_samples_split': 7,\n",
              " 'min_samples_leaf': 7,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'min_impurity_decrease': 1.0,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['GradBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 550,\n",
              " 'learning_rate': 0.0479901225935416,\n",
              " 'min_child_samples': 1,\n",
              " 'max_depth': 6,\n",
              " 'reg_lambda': 3.3766279624518107,\n",
              " 'silent': True,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['CatBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'class_weight': 'balanced',\n",
              " 'boosting_type': 'gbdt',\n",
              " 'num_leaves': 55,\n",
              " 'learning_rate': 0.04496177447997528,\n",
              " 'min_child_samples': 10,\n",
              " 'reg_alpha': 0.3916912792044354,\n",
              " 'reg_lambda': 1.4941077467431771,\n",
              " 'colsample_by_tree': 0.379259630420579,\n",
              " 'verbosity': -1,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['LightGBM']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'booster': 'dart',\n",
              " 'learning_rate': 0.09292666170093178,\n",
              " 'gamma': 4,\n",
              " 'max_depth': 4,\n",
              " 'min_child_weight': 1,\n",
              " 'colsample_bytree': 0.8943278668489419,\n",
              " 'colsample_bylevel': 0.2640104690942444,\n",
              " 'colsample_bynode': 0.8937107554719765,\n",
              " 'reg_alpha': 0.056770729092546546,\n",
              " 'reg_lambda': 4.219736540591216,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AiGBWUhXmjty"
      },
      "outputs": [],
      "source": [
        "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['AdaBoost', 'GradBoost', 'CatBoost', 'LightGBM', 'XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x7JQf94WmaZT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Wine Dataset ---------\n",
            "[0.94444444 0.94444444 0.94444444 1.         0.94444444 1.\n",
            " 1.         0.83333333 1.         1.         1.         1.\n",
            " 0.94444444 1.         0.94444444 1.         1.         0.88888889\n",
            " 1.         0.94117647 1.         1.         0.88888889 0.94444444\n",
            " 1.         1.         1.         1.         0.88235294 1.\n",
            " 0.94444444 1.         1.         0.94444444 1.         0.94444444\n",
            " 1.         1.         0.94117647 0.88235294 0.88888889 1.\n",
            " 1.         0.94444444 1.         1.         1.         1.\n",
            " 0.82352941 0.94117647 1.         0.94444444 1.         0.94444444\n",
            " 1.         1.         0.94444444 0.94444444 0.94117647 0.94117647\n",
            " 1.         1.         0.94444444 1.         1.         0.88888889\n",
            " 0.94444444 0.94444444 0.88235294 1.         0.94444444 1.\n",
            " 1.         0.94444444 1.         1.         0.94444444 0.94444444\n",
            " 0.94117647 0.94117647 1.         1.         1.         0.94444444\n",
            " 1.         0.94444444 0.94444444 1.         0.94117647 1.\n",
            " 0.94444444 0.94444444 1.         0.94444444 1.         0.88888889\n",
            " 0.94444444 0.94444444 0.94117647 1.        ]\n",
            "Accuracy: 96.55% (4.09%)\n",
            "Execution Time: 22.98 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Wine Dataset ---------\n",
            "[1.         0.94444444 1.         1.         1.         1.\n",
            " 1.         0.88888889 1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         0.94444444 0.94444444\n",
            " 1.         0.94117647 1.         1.         0.88888889 0.94444444\n",
            " 1.         1.         1.         1.         0.94117647 0.88235294\n",
            " 0.94444444 1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         0.82352941 1.         1.\n",
            " 1.         0.94444444 1.         0.94444444 1.         1.\n",
            " 0.94117647 1.         1.         0.94444444 0.94444444 1.\n",
            " 1.         1.         1.         0.94444444 1.         0.94117647\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         1.         1.\n",
            " 1.         0.94117647 1.         1.         1.         0.94444444\n",
            " 1.         0.88888889 1.         1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         0.94444444 1.\n",
            " 0.94444444 0.94444444 0.94117647 1.        ]\n",
            "Accuracy: 98.08% (3.44%)\n",
            "Execution Time: 15.55 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Wine Dataset ---------\n",
            "[1.         0.94444444 1.         1.         0.94444444 1.\n",
            " 1.         0.88888889 1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         0.94444444 0.94444444\n",
            " 1.         0.94117647 1.         1.         0.94444444 0.94444444\n",
            " 1.         1.         1.         1.         0.94117647 0.94117647\n",
            " 0.94444444 1.         1.         0.94444444 1.         1.\n",
            " 1.         1.         0.94117647 0.94117647 0.94444444 1.\n",
            " 1.         0.94444444 1.         1.         1.         1.\n",
            " 0.94117647 1.         1.         0.94444444 1.         0.94444444\n",
            " 1.         1.         1.         0.94444444 1.         0.94117647\n",
            " 1.         1.         0.88888889 1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         1.         1.\n",
            " 1.         0.94117647 1.         1.         1.         0.94444444\n",
            " 1.         0.88888889 1.         1.         0.94117647 1.\n",
            " 1.         0.94444444 1.         0.94444444 0.94444444 1.\n",
            " 0.94444444 1.         0.94117647 1.        ]\n",
            "Accuracy: 97.97% (3.03%)\n",
            "Execution Time: 98.04 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Wine Dataset ---------\n",
            "[1.         0.94444444 0.94444444 1.         0.94444444 1.\n",
            " 1.         0.77777778 1.         1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         0.94444444 0.94444444\n",
            " 1.         1.         1.         1.         0.94444444 0.94444444\n",
            " 1.         1.         1.         1.         0.88235294 0.94117647\n",
            " 0.94444444 1.         1.         1.         1.         1.\n",
            " 1.         0.88888889 0.94117647 0.94117647 0.94444444 1.\n",
            " 1.         0.94444444 1.         0.94444444 1.         1.\n",
            " 0.88235294 0.94117647 1.         0.94444444 1.         0.94444444\n",
            " 1.         1.         1.         0.94444444 1.         0.94117647\n",
            " 0.94444444 1.         0.94444444 1.         1.         1.\n",
            " 1.         0.94444444 0.88235294 1.         0.94444444 1.\n",
            " 1.         0.94444444 1.         1.         1.         0.94444444\n",
            " 0.94117647 0.94117647 1.         1.         1.         0.88888889\n",
            " 1.         0.88888889 1.         0.94444444 1.         1.\n",
            " 1.         0.94444444 1.         0.94444444 0.88888889 1.\n",
            " 1.         0.94444444 0.94117647 1.        ]\n",
            "Accuracy: 97.12% (4.03%)\n",
            "Execution Time: 3.60 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Wine Dataset ---------\n",
            "[1.         0.94444444 1.         1.         0.94444444 1.\n",
            " 1.         0.88888889 1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         0.94444444 0.88888889\n",
            " 1.         0.94117647 1.         1.         0.94444444 0.94444444\n",
            " 1.         1.         1.         1.         0.94117647 0.88235294\n",
            " 0.94444444 1.         1.         1.         1.         1.\n",
            " 1.         1.         0.94117647 0.88235294 0.94444444 1.\n",
            " 1.         0.94444444 1.         0.94444444 1.         1.\n",
            " 0.94117647 1.         1.         0.94444444 0.94444444 1.\n",
            " 1.         1.         1.         1.         1.         0.94117647\n",
            " 1.         1.         0.88888889 1.         1.         1.\n",
            " 0.94444444 0.94444444 1.         1.         1.         1.\n",
            " 1.         0.94444444 1.         1.         1.         0.94444444\n",
            " 1.         0.94117647 1.         1.         1.         0.94444444\n",
            " 1.         0.88888889 1.         0.94444444 1.         1.\n",
            " 1.         0.94444444 1.         1.         0.94444444 1.\n",
            " 1.         0.94444444 0.94117647 1.        ]\n",
            "Accuracy: 97.80% (3.38%)\n",
            "Execution Time: 41.52 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "wine_scores = []\n",
        "wine_scores_mean = []\n",
        "wine_scores_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "        \n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    wine_scores.append(results)\n",
        "    wine_scores_mean.append(results.mean()*100)\n",
        "    wine_scores_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "\n",
        "    print(f'--------- {algorithm_name} on Wine Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Wine scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(wine_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results = pd.DataFrame()\n",
        "Algo_results['Names'] = names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Wine'] = wine_scores_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine\n",
              "0   AdaBoost  96.552288\n",
              "1  GradBoost  98.075163\n",
              "2   CatBoost  97.967320\n",
              "3   LightGBM  97.120915\n",
              "4    XGBoost  97.797386"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results = pd.DataFrame()\n",
        "Algo_time_results['Names'] = names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Wine'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>22.984195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>15.547652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>98.037048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>3.596812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>41.520385</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine\n",
              "0   AdaBoost  22.984195\n",
              "1  GradBoost  15.547652\n",
              "2   CatBoost  98.037048\n",
              "3   LightGBM   3.596812\n",
              "4    XGBoost  41.520385"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_time_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Breast Cancer Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "breast_cancer_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\BreastCancer\\Breast.dat', sep=',', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = breast_cancer_df.iloc[:, :-1]\n",
        "y = breast_cancer_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:40<00:00,  1.23trial/s, best loss: -0.9927007299270073]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 450.0, 'learning_rate': 0.01199453123793802, 'max_depth': 4.0, 'max_features': 'log2', 'min_samples_leaf': 4.0, 'min_samples_split': 6.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:16<00:00,  3.01trial/s, best loss: -0.9927007299270073]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': 'sqrt', 'n_estimators': 550, 'learning_rate': 0.0611622198189229, 'max_depth': 2, 'min_samples_split': 7, 'min_samples_leaf': 5, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 1.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:31<00:00,  1.57trial/s, best loss: -0.9927007299270073]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 300, 'learning_rate': 0.013094250183297027, 'min_child_samples': 1, 'max_depth': 5, 'reg_lambda': 4.710165866797953, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 44.00trial/s, best loss: -0.9854014598540146]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'goss', 'num_leaves': 70, 'learning_rate': 0.06466735422122151, 'min_child_samples': 60, 'reg_alpha': 1.7712918439651535, 'reg_lambda': 0.09630512995808138, 'colsample_by_tree': 0.9773212695265424, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:08<00:00,  6.01trial/s, best loss: -0.9927007299270073]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'dart', 'learning_rate': 0.01958151011028328, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 3, 'colsample_bytree': 0.17755828466772988, 'colsample_bylevel': 0.2750454258060418, 'colsample_bynode': 0.5404751722067938, 'reg_alpha': 1.5842037921731331, 'reg_lambda': 1.1583230510671016, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 450.0,\n",
              " 'learning_rate': 0.01199453123793802,\n",
              " 'max_depth': 4.0,\n",
              " 'max_features': 'log2',\n",
              " 'min_samples_leaf': 4.0,\n",
              " 'min_samples_split': 6.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['AdaBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'criterion': 'friedman_mse',\n",
              " 'max_features': 'sqrt',\n",
              " 'n_estimators': 550,\n",
              " 'learning_rate': 0.0611622198189229,\n",
              " 'max_depth': 2,\n",
              " 'min_samples_split': 7,\n",
              " 'min_samples_leaf': 5,\n",
              " 'min_weight_fraction_leaf': 0.1,\n",
              " 'min_impurity_decrease': 1.0,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['GradBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 300,\n",
              " 'learning_rate': 0.013094250183297027,\n",
              " 'min_child_samples': 1,\n",
              " 'max_depth': 5,\n",
              " 'reg_lambda': 4.710165866797953,\n",
              " 'silent': True,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['CatBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'class_weight': 'balanced',\n",
              " 'boosting_type': 'goss',\n",
              " 'num_leaves': 70,\n",
              " 'learning_rate': 0.06466735422122151,\n",
              " 'min_child_samples': 60,\n",
              " 'reg_alpha': 1.7712918439651535,\n",
              " 'reg_lambda': 0.09630512995808138,\n",
              " 'colsample_by_tree': 0.9773212695265424,\n",
              " 'verbosity': -1,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['LightGBM']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'booster': 'dart',\n",
              " 'learning_rate': 0.01958151011028328,\n",
              " 'gamma': 5,\n",
              " 'max_depth': 3,\n",
              " 'min_child_weight': 3,\n",
              " 'colsample_bytree': 0.17755828466772988,\n",
              " 'colsample_bylevel': 0.2750454258060418,\n",
              " 'colsample_bynode': 0.5404751722067938,\n",
              " 'reg_alpha': 1.5842037921731331,\n",
              " 'reg_lambda': 1.1583230510671016,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Breast Cancer Dataset ---------\n",
            "[0.95652174 0.97101449 0.92753623 0.95588235 0.97058824 1.\n",
            " 0.97058824 0.97058824 0.98529412 0.98529412 0.97101449 0.95652174\n",
            " 0.95652174 0.98529412 0.95588235 0.95588235 1.         0.97058824\n",
            " 0.98529412 0.97058824 1.         0.95652174 0.97101449 0.98529412\n",
            " 0.95588235 0.98529412 0.97058824 0.98529412 0.97058824 0.97058824\n",
            " 0.98550725 0.94202899 0.98550725 0.97058824 0.98529412 1.\n",
            " 0.97058824 0.98529412 0.94117647 0.95588235 0.95652174 0.98550725\n",
            " 0.98550725 1.         0.98529412 0.97058824 0.97058824 0.98529412\n",
            " 0.95588235 0.94117647 0.98550725 0.92753623 1.         1.\n",
            " 0.94117647 0.98529412 0.97058824 0.98529412 0.97058824 0.97058824\n",
            " 0.95652174 0.98550725 0.98550725 0.91176471 0.98529412 0.98529412\n",
            " 0.94117647 0.97058824 0.92647059 1.         0.95652174 1.\n",
            " 0.97101449 0.95588235 0.98529412 0.94117647 0.98529412 0.98529412\n",
            " 0.95588235 0.97058824 0.97101449 0.97101449 0.97101449 0.97058824\n",
            " 0.98529412 0.95588235 0.97058824 0.97058824 0.98529412 0.97058824\n",
            " 1.         0.95652174 0.98550725 0.97058824 0.95588235 1.\n",
            " 0.97058824 0.98529412 0.97058824 0.94117647]\n",
            "Accuracy: 97.16% (1.86%)\n",
            "Execution Time: 61.19 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Breast Cancer Dataset ---------\n",
            "[0.95652174 0.98550725 0.98550725 0.95588235 0.97058824 0.98529412\n",
            " 0.95588235 0.97058824 0.95588235 0.98529412 0.95652174 0.97101449\n",
            " 0.97101449 0.97058824 0.95588235 0.92647059 1.         0.92647059\n",
            " 0.97058824 0.97058824 1.         0.94202899 0.95652174 0.97058824\n",
            " 0.97058824 0.98529412 0.97058824 0.95588235 0.97058824 0.98529412\n",
            " 0.95652174 0.95652174 0.97101449 0.97058824 0.97058824 1.\n",
            " 0.98529412 0.98529412 0.89705882 0.97058824 0.95652174 0.98550725\n",
            " 0.97101449 0.98529412 0.97058824 0.94117647 0.95588235 0.98529412\n",
            " 0.94117647 0.95588235 1.         0.91304348 1.         0.98529412\n",
            " 0.89705882 0.97058824 0.95588235 0.97058824 0.98529412 0.98529412\n",
            " 0.95652174 0.98550725 0.97101449 0.95588235 0.97058824 1.\n",
            " 0.94117647 0.97058824 0.97058824 0.98529412 0.98550725 0.98550725\n",
            " 0.97101449 0.95588235 0.98529412 0.94117647 0.97058824 0.94117647\n",
            " 0.95588235 0.95588235 0.94202899 0.95652174 0.94202899 0.95588235\n",
            " 0.95588235 0.95588235 0.97058824 1.         0.98529412 0.97058824\n",
            " 0.98550725 0.94202899 0.97101449 0.98529412 0.95588235 1.\n",
            " 0.97058824 0.92647059 0.97058824 0.94117647]\n",
            "Accuracy: 96.65% (2.09%)\n",
            "Execution Time: 21.19 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Breast Cancer Dataset ---------\n",
            "[0.94202899 0.97101449 0.98550725 0.95588235 0.97058824 1.\n",
            " 0.97058824 0.97058824 0.97058824 0.98529412 0.98550725 0.97101449\n",
            " 0.97101449 0.97058824 0.97058824 0.95588235 1.         0.95588235\n",
            " 1.         0.95588235 1.         0.94202899 0.95652174 0.97058824\n",
            " 0.95588235 1.         0.95588235 0.98529412 0.97058824 0.98529412\n",
            " 0.97101449 0.95652174 0.98550725 0.98529412 0.98529412 1.\n",
            " 0.98529412 0.97058824 0.94117647 0.97058824 0.97101449 0.98550725\n",
            " 1.         1.         0.98529412 0.95588235 0.98529412 0.97058824\n",
            " 0.94117647 0.95588235 1.         0.95652174 1.         0.98529412\n",
            " 0.94117647 0.98529412 0.95588235 0.97058824 0.97058824 0.98529412\n",
            " 0.95652174 0.98550725 0.98550725 0.92647059 0.98529412 1.\n",
            " 0.94117647 0.97058824 0.98529412 0.98529412 1.         1.\n",
            " 0.97101449 0.95588235 0.98529412 0.97058824 0.97058824 0.95588235\n",
            " 0.95588235 0.95588235 0.98550725 0.98550725 0.94202899 0.97058824\n",
            " 0.98529412 0.97058824 0.97058824 1.         0.98529412 0.98529412\n",
            " 0.98550725 0.95652174 0.97101449 1.         0.97058824 1.\n",
            " 0.97058824 0.97058824 0.97058824 0.92647059]\n",
            "Accuracy: 97.38% (1.80%)\n",
            "Execution Time: 43.54 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Breast Cancer Dataset ---------\n",
            "[0.97101449 0.97101449 0.95652174 0.95588235 0.97058824 1.\n",
            " 0.95588235 0.97058824 0.98529412 0.98529412 0.98550725 0.97101449\n",
            " 0.98550725 0.98529412 0.97058824 0.92647059 1.         0.95588235\n",
            " 1.         0.95588235 1.         0.94202899 0.95652174 0.97058824\n",
            " 0.95588235 1.         0.97058824 0.98529412 0.97058824 0.97058824\n",
            " 0.97101449 0.95652174 0.97101449 0.97058824 0.98529412 0.97058824\n",
            " 0.98529412 1.         0.94117647 0.97058824 0.95652174 1.\n",
            " 0.98550725 0.98529412 0.98529412 0.95588235 0.98529412 0.97058824\n",
            " 0.95588235 0.95588235 1.         0.91304348 1.         0.98529412\n",
            " 0.89705882 0.97058824 0.97058824 0.98529412 1.         0.98529412\n",
            " 0.95652174 1.         0.98550725 0.92647059 0.98529412 1.\n",
            " 0.94117647 0.97058824 0.98529412 0.95588235 1.         1.\n",
            " 0.97101449 0.95588235 0.98529412 0.94117647 0.97058824 0.98529412\n",
            " 0.98529412 0.97058824 0.94202899 0.98550725 0.95652174 0.98529412\n",
            " 0.98529412 0.97058824 0.98529412 1.         0.98529412 0.97058824\n",
            " 1.         0.97101449 0.98550725 0.98529412 0.97058824 0.98529412\n",
            " 0.94117647 1.         0.97058824 0.94117647]\n",
            "Accuracy: 97.33% (2.05%)\n",
            "Execution Time: 1.56 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Breast Cancer Dataset ---------\n",
            "[0.95652174 0.98550725 0.98550725 0.95588235 0.95588235 0.98529412\n",
            " 0.95588235 0.97058824 0.95588235 0.97058824 0.98550725 0.97101449\n",
            " 0.95652174 0.97058824 0.97058824 0.95588235 1.         0.94117647\n",
            " 0.95588235 0.97058824 1.         0.92753623 0.95652174 0.95588235\n",
            " 0.95588235 1.         0.97058824 0.97058824 0.97058824 0.97058824\n",
            " 0.95652174 0.97101449 0.97101449 0.97058824 0.97058824 1.\n",
            " 0.98529412 1.         0.91176471 0.95588235 0.97101449 0.98550725\n",
            " 0.97101449 1.         0.97058824 0.92647059 0.98529412 0.97058824\n",
            " 0.94117647 0.94117647 1.         0.92753623 0.97101449 0.97058824\n",
            " 0.92647059 0.98529412 0.95588235 0.97058824 1.         0.97058824\n",
            " 0.95652174 0.98550725 0.97101449 0.94117647 0.97058824 1.\n",
            " 0.92647059 0.97058824 0.97058824 0.98529412 1.         0.98550725\n",
            " 0.95652174 0.92647059 0.98529412 0.95588235 0.95588235 0.97058824\n",
            " 0.98529412 0.95588235 0.95652174 0.97101449 0.94202899 0.95588235\n",
            " 0.97058824 0.97058824 0.98529412 0.98529412 0.98529412 0.97058824\n",
            " 1.         0.94202899 0.98550725 0.98529412 0.95588235 1.\n",
            " 0.97058824 0.95588235 0.95588235 0.92647059]\n",
            "Accuracy: 96.79% (2.02%)\n",
            "Execution Time: 19.88 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "breast_cancer_scores = []\n",
        "breast_cancer_mean = []\n",
        "breast_cancer_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    breast_cancer_scores.append(results)\n",
        "    breast_cancer_mean.append(results.mean()*100)\n",
        "    breast_cancer_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "\n",
        "    print(f'--------- {algorithm_name} on Breast Cancer Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Breast Cancer scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(breast_cancer_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Breast_Cancer'] = breast_cancer_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "      <td>97.159847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>96.646633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.378303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.334612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "      <td>96.792626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer\n",
              "0   AdaBoost  96.552288      97.159847\n",
              "1  GradBoost  98.075163      96.646633\n",
              "2   CatBoost  97.967320      97.378303\n",
              "3   LightGBM  97.120915      97.334612\n",
              "4    XGBoost  97.797386      96.792626"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Breast_Cancer'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Sonar Dataset** #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "sonar_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Sonar\\Sonar.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = sonar_df.iloc[:, :-1]\n",
        "y = sonar_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [02:39<00:00,  3.19s/trial, best loss: -0.9523809523809523]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 1250.0, 'learning_rate': 0.011066661922600281, 'max_depth': 2.0, 'max_features': None, 'min_samples_leaf': 5.0, 'min_samples_split': 6.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.04s/trial, best loss: -0.8571428571428571]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': 'sqrt', 'n_estimators': 100, 'learning_rate': 0.04102652661864284, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.0, 'min_impurity_decrease': 1.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [04:59<00:00,  5.99s/trial, best loss: -0.8809523809523809]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 1300, 'learning_rate': 0.014023863721779927, 'min_child_samples': 9, 'max_depth': 7, 'reg_lambda': 0.2645130637158699, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 49.44trial/s, best loss: -0.7857142857142857]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'gbdt', 'num_leaves': 80, 'learning_rate': 0.088633625625231, 'min_child_samples': 60, 'reg_alpha': 0.20114179877735983, 'reg_lambda': 0.021920717955273894, 'colsample_by_tree': 0.9439502913709372, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:09<00:00,  5.44trial/s, best loss: -0.8571428571428571]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'dart', 'learning_rate': 0.044025607478991216, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 1, 'colsample_bytree': 0.3050569452640979, 'colsample_bylevel': 0.914172295823844, 'colsample_bynode': 0.2986672398458319, 'reg_alpha': 0.7657116989618611, 'reg_lambda': 0.8092636688928795, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 1250.0,\n",
              " 'learning_rate': 0.011066661922600281,\n",
              " 'max_depth': 2.0,\n",
              " 'max_features': None,\n",
              " 'min_samples_leaf': 5.0,\n",
              " 'min_samples_split': 6.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['AdaBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'criterion': 'friedman_mse',\n",
              " 'max_features': 'sqrt',\n",
              " 'n_estimators': 100,\n",
              " 'learning_rate': 0.04102652661864284,\n",
              " 'max_depth': 3,\n",
              " 'min_samples_split': 7,\n",
              " 'min_samples_leaf': 7,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'min_impurity_decrease': 1.0,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['GradBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 1300,\n",
              " 'learning_rate': 0.014023863721779927,\n",
              " 'min_child_samples': 9,\n",
              " 'max_depth': 7,\n",
              " 'reg_lambda': 0.2645130637158699,\n",
              " 'silent': True,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['CatBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'class_weight': 'balanced',\n",
              " 'boosting_type': 'gbdt',\n",
              " 'num_leaves': 80,\n",
              " 'learning_rate': 0.088633625625231,\n",
              " 'min_child_samples': 60,\n",
              " 'reg_alpha': 0.20114179877735983,\n",
              " 'reg_lambda': 0.021920717955273894,\n",
              " 'colsample_by_tree': 0.9439502913709372,\n",
              " 'verbosity': -1,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['LightGBM']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'booster': 'dart',\n",
              " 'learning_rate': 0.044025607478991216,\n",
              " 'gamma': 2,\n",
              " 'max_depth': 5,\n",
              " 'min_child_weight': 1,\n",
              " 'colsample_bytree': 0.3050569452640979,\n",
              " 'colsample_bylevel': 0.914172295823844,\n",
              " 'colsample_bynode': 0.2986672398458319,\n",
              " 'reg_alpha': 0.7657116989618611,\n",
              " 'reg_lambda': 0.8092636688928795,\n",
              " 'random_state': 42}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparams['XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Sonar Dataset ---------\n",
            "[0.95238095 0.80952381 0.85714286 0.95238095 0.80952381 1.\n",
            " 0.9047619  0.85714286 0.85       1.         0.9047619  0.9047619\n",
            " 0.95238095 0.9047619  0.80952381 0.9047619  0.9047619  0.9047619\n",
            " 0.85       0.75       0.76190476 0.80952381 1.         0.9047619\n",
            " 0.80952381 0.9047619  0.85714286 0.76190476 0.85       0.9\n",
            " 0.95238095 0.85714286 0.9047619  0.71428571 0.85714286 0.80952381\n",
            " 0.80952381 0.9047619  0.85       0.8        0.9047619  0.76190476\n",
            " 0.9047619  0.71428571 0.76190476 0.85714286 0.80952381 0.71428571\n",
            " 0.9        0.95       0.9047619  0.85714286 0.85714286 0.85714286\n",
            " 0.80952381 0.9047619  0.95238095 0.9047619  0.9        0.8\n",
            " 0.66666667 0.85714286 0.9047619  0.9047619  0.76190476 0.9047619\n",
            " 0.80952381 0.9047619  0.65       1.         0.85714286 0.9047619\n",
            " 0.80952381 0.9047619  0.95238095 0.76190476 0.9047619  0.85714286\n",
            " 0.85       0.9        0.76190476 0.76190476 0.76190476 1.\n",
            " 0.95238095 0.95238095 0.9047619  0.9047619  0.9        0.9\n",
            " 0.9047619  0.9047619  0.80952381 0.85714286 0.9047619  0.9047619\n",
            " 0.76190476 0.9047619  0.9        0.8       ]\n",
            "Accuracy: 86.35% (7.35%)\n",
            "Execution Time: 684.58 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Sonar Dataset ---------\n",
            "[0.80952381 0.66666667 0.71428571 0.95238095 0.57142857 0.9047619\n",
            " 0.71428571 0.80952381 0.75       0.95       0.71428571 0.71428571\n",
            " 0.80952381 0.95238095 0.85714286 0.76190476 0.85714286 0.76190476\n",
            " 0.9        0.6        0.76190476 0.61904762 0.9047619  0.85714286\n",
            " 0.71428571 0.71428571 0.85714286 0.80952381 0.7        0.8\n",
            " 0.85714286 0.9047619  0.66666667 0.66666667 0.66666667 0.80952381\n",
            " 0.71428571 0.80952381 0.75       0.8        0.9047619  0.76190476\n",
            " 0.85714286 0.66666667 0.61904762 0.76190476 0.76190476 0.61904762\n",
            " 0.9        0.9        0.76190476 0.71428571 0.76190476 0.61904762\n",
            " 0.71428571 0.76190476 0.9047619  0.85714286 0.95       0.75\n",
            " 0.71428571 0.66666667 0.9047619  0.80952381 0.85714286 0.71428571\n",
            " 0.85714286 0.80952381 0.65       0.75       0.9047619  0.85714286\n",
            " 0.76190476 0.76190476 0.85714286 0.71428571 0.76190476 0.76190476\n",
            " 0.8        0.85       0.80952381 0.57142857 0.71428571 0.76190476\n",
            " 0.76190476 0.85714286 0.80952381 0.80952381 0.85       0.85\n",
            " 0.80952381 0.71428571 0.76190476 0.80952381 0.76190476 0.80952381\n",
            " 0.85714286 0.80952381 0.7        0.85      ]\n",
            "Accuracy: 78.15% (8.83%)\n",
            "Execution Time: 8.57 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Sonar Dataset ---------\n",
            "[0.95238095 0.76190476 0.85714286 1.         0.71428571 0.95238095\n",
            " 0.9047619  0.85714286 0.85       0.95       0.76190476 0.9047619\n",
            " 0.95238095 0.95238095 0.85714286 0.9047619  0.85714286 0.76190476\n",
            " 0.95       0.75       0.9047619  0.71428571 1.         0.95238095\n",
            " 0.76190476 0.80952381 0.85714286 0.9047619  0.95       0.9\n",
            " 0.95238095 0.95238095 0.85714286 0.76190476 0.85714286 0.80952381\n",
            " 0.9047619  0.9047619  0.85       0.85       0.9047619  0.76190476\n",
            " 0.95238095 0.76190476 0.9047619  0.80952381 0.85714286 0.9047619\n",
            " 0.9        0.9        0.95238095 0.85714286 0.80952381 0.85714286\n",
            " 0.71428571 0.9047619  0.9047619  0.95238095 0.9        0.85\n",
            " 0.85714286 0.80952381 0.9047619  0.85714286 0.80952381 0.95238095\n",
            " 0.95238095 0.9047619  0.65       0.9        0.85714286 0.95238095\n",
            " 0.85714286 0.95238095 0.9047619  0.71428571 0.85714286 0.80952381\n",
            " 0.85       0.9        0.85714286 0.76190476 0.71428571 0.9047619\n",
            " 0.85714286 0.9047619  0.95238095 0.95238095 0.95       0.9\n",
            " 0.9047619  0.80952381 0.71428571 0.85714286 1.         0.9047619\n",
            " 0.85714286 0.9047619  0.85       1.        ]\n",
            "Accuracy: 87.08% (7.54%)\n",
            "Execution Time: 760.84 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Sonar Dataset ---------\n",
            "[0.85714286 0.66666667 0.66666667 0.85714286 0.71428571 0.85714286\n",
            " 0.95238095 0.80952381 0.9        0.95       0.66666667 0.76190476\n",
            " 0.85714286 0.80952381 0.80952381 0.9047619  0.76190476 0.85714286\n",
            " 0.9        0.75       0.85714286 0.80952381 0.95238095 0.80952381\n",
            " 0.80952381 0.80952381 0.85714286 0.9047619  0.75       0.7\n",
            " 1.         0.9047619  0.80952381 0.71428571 0.71428571 0.85714286\n",
            " 0.80952381 0.76190476 0.8        0.9        0.9047619  0.80952381\n",
            " 0.9047619  0.66666667 0.76190476 0.80952381 0.9047619  0.71428571\n",
            " 0.85       0.85       0.95238095 0.71428571 0.80952381 0.80952381\n",
            " 0.76190476 0.80952381 0.85714286 0.95238095 0.9        0.9\n",
            " 0.85714286 0.85714286 0.85714286 0.9047619  0.85714286 0.80952381\n",
            " 0.80952381 0.80952381 0.65       0.75       0.9047619  0.76190476\n",
            " 0.80952381 0.80952381 0.9047619  0.85714286 0.76190476 0.80952381\n",
            " 0.75       0.75       0.85714286 0.61904762 0.71428571 0.85714286\n",
            " 0.80952381 0.85714286 0.85714286 0.95238095 0.95       0.85\n",
            " 0.80952381 0.76190476 0.66666667 0.85714286 0.85714286 0.9047619\n",
            " 0.80952381 0.85714286 0.8        0.95      ]\n",
            "Accuracy: 82.36% (7.92%)\n",
            "Execution Time: 1.45 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Sonar Dataset ---------\n",
            "[0.9047619  0.80952381 0.76190476 0.95238095 0.71428571 0.95238095\n",
            " 0.80952381 0.85714286 0.8        0.95       0.71428571 0.71428571\n",
            " 0.95238095 0.95238095 0.95238095 0.85714286 0.80952381 0.76190476\n",
            " 0.95       0.7        0.85714286 0.61904762 0.95238095 0.9047619\n",
            " 0.80952381 0.80952381 0.85714286 0.9047619  0.85       0.9\n",
            " 0.95238095 0.9047619  0.85714286 0.80952381 0.80952381 0.85714286\n",
            " 0.71428571 0.76190476 0.8        0.75       0.9047619  0.80952381\n",
            " 0.85714286 0.76190476 0.66666667 0.85714286 0.76190476 0.76190476\n",
            " 0.85       0.9        0.95238095 0.71428571 0.80952381 0.71428571\n",
            " 0.71428571 0.9047619  0.85714286 0.9047619  0.9        0.8\n",
            " 0.76190476 0.76190476 0.9047619  0.9047619  0.9047619  0.85714286\n",
            " 0.95238095 0.85714286 0.65       0.85       0.9047619  0.95238095\n",
            " 0.80952381 0.85714286 0.85714286 0.76190476 0.80952381 0.85714286\n",
            " 0.8        0.85       0.9047619  0.66666667 0.76190476 0.80952381\n",
            " 0.80952381 0.9047619  0.85714286 0.95238095 0.95       0.85\n",
            " 0.85714286 0.80952381 0.66666667 0.80952381 1.         0.85714286\n",
            " 0.9047619  0.85714286 0.8        0.95      ]\n",
            "Accuracy: 83.80% (8.27%)\n",
            "Execution Time: 22.49 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "sonar_scores = []\n",
        "sonar_mean = []\n",
        "sonar_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "        \n",
        "    start_time = time.time()\n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    sonar_scores.append(results)\n",
        "    sonar_mean.append(results.mean()*100)\n",
        "    sonar_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "\n",
        "    print(f'--------- {algorithm_name} on Sonar Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Sonar scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(sonar_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Sonar'] = sonar_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "      <td>97.159847</td>\n",
              "      <td>86.347619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>78.145238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>87.076190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>82.361905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "      <td>96.792626</td>\n",
              "      <td>83.802381</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer      Sonar\n",
              "0   AdaBoost  96.552288      97.159847  86.347619\n",
              "1  GradBoost  98.075163      96.646633  78.145238\n",
              "2   CatBoost  97.967320      97.378303  87.076190\n",
              "3   LightGBM  97.120915      97.334612  82.361905\n",
              "4    XGBoost  97.797386      96.792626  83.802381"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Sonar'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Ionosphere Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "ionosphere_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Ionosphere\\ionosphere.data', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(351, 35)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ionosphere_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = ionosphere_df.iloc[:, :-1]\n",
        "y = ionosphere_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:23<00:00,  1.66s/trial, best loss: -0.971830985915493] \n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 50.0, 'learning_rate': 0.046035205781861564, 'max_depth': 6.0, 'max_features': 'log2', 'min_samples_leaf': 5.0, 'min_samples_split': 2.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.06s/trial, best loss: -0.9295774647887324]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': None, 'n_estimators': 400, 'learning_rate': 0.021178191623985942, 'max_depth': 4, 'min_samples_split': 3, 'min_samples_leaf': 8, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 2.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [06:36<00:00,  7.93s/trial, best loss: -0.9577464788732394]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 750, 'learning_rate': 0.02983152512960275, 'min_child_samples': 3, 'max_depth': 5, 'reg_lambda': 3.8771604915102147, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 43.89trial/s, best loss: -0.9436619718309859]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'gbdt', 'num_leaves': 75, 'learning_rate': 0.09085691661731564, 'min_child_samples': 20, 'reg_alpha': 0.8776705363565946, 'reg_lambda': 2.021006183231964, 'colsample_by_tree': 0.5358470999804816, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:07<00:00,  6.49trial/s, best loss: -0.9436619718309859]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'gbtree', 'learning_rate': 0.011777426690454684, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'colsample_bytree': 0.6642423404208758, 'colsample_bylevel': 0.8389604376670141, 'colsample_bynode': 0.46801910869053165, 'reg_alpha': 1.3842922617481603, 'reg_lambda': 0.25127542856871243, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Ionosphere Dataset ---------\n",
            "[1.         0.91428571 0.91428571 0.94285714 0.97142857 0.91428571\n",
            " 0.94285714 0.97142857 0.97142857 0.88571429 0.97222222 0.94285714\n",
            " 0.97142857 0.97142857 1.         0.94285714 0.91428571 0.88571429\n",
            " 0.91428571 0.88571429 0.86111111 0.85714286 0.97142857 0.94285714\n",
            " 0.97142857 0.94285714 1.         0.94285714 0.88571429 1.\n",
            " 0.91666667 0.97142857 0.94285714 1.         0.91428571 0.91428571\n",
            " 0.85714286 0.91428571 0.94285714 0.88571429 0.97222222 1.\n",
            " 0.91428571 0.91428571 0.97142857 0.82857143 1.         0.97142857\n",
            " 0.91428571 0.85714286 0.94444444 0.97142857 0.91428571 1.\n",
            " 0.94285714 0.97142857 0.97142857 0.91428571 0.91428571 0.88571429\n",
            " 0.97222222 0.91428571 0.91428571 0.94285714 0.97142857 0.91428571\n",
            " 1.         0.88571429 1.         0.94285714 0.97222222 0.88571429\n",
            " 0.91428571 0.94285714 0.88571429 0.88571429 1.         0.91428571\n",
            " 0.97142857 1.         0.91666667 0.94285714 0.94285714 0.97142857\n",
            " 0.8        1.         0.91428571 1.         0.91428571 0.91428571\n",
            " 0.91666667 0.88571429 1.         0.88571429 0.94285714 0.94285714\n",
            " 0.94285714 1.         0.94285714 0.97142857]\n",
            "Accuracy: 93.82% (4.34%)\n",
            "Execution Time: 12.88 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Ionosphere Dataset ---------\n",
            "[0.94444444 0.97142857 0.94285714 0.85714286 0.91428571 0.85714286\n",
            " 0.88571429 0.91428571 0.82857143 0.91428571 0.83333333 0.88571429\n",
            " 0.91428571 0.94285714 0.91428571 0.94285714 0.94285714 0.94285714\n",
            " 0.85714286 0.91428571 0.80555556 0.8        1.         0.97142857\n",
            " 1.         0.85714286 0.91428571 0.88571429 0.94285714 0.97142857\n",
            " 1.         1.         1.         0.97142857 0.85714286 0.94285714\n",
            " 0.82857143 0.94285714 0.82857143 0.77142857 0.94444444 0.97142857\n",
            " 0.91428571 0.94285714 0.97142857 0.82857143 0.85714286 0.88571429\n",
            " 0.91428571 0.88571429 0.97222222 0.94285714 0.97142857 0.91428571\n",
            " 0.85714286 0.91428571 0.94285714 0.82857143 0.91428571 0.82857143\n",
            " 0.88888889 0.97142857 0.88571429 0.82857143 0.94285714 0.85714286\n",
            " 0.85714286 0.94285714 0.94285714 0.91428571 0.88888889 0.85714286\n",
            " 0.91428571 0.77142857 0.88571429 0.94285714 1.         0.94285714\n",
            " 0.88571429 0.88571429 0.91666667 0.88571429 0.88571429 0.97142857\n",
            " 0.85714286 1.         0.88571429 0.94285714 0.91428571 0.94285714\n",
            " 0.88888889 0.91428571 1.         0.91428571 0.85714286 0.91428571\n",
            " 0.82857143 0.91428571 0.85714286 0.94285714]\n",
            "Accuracy: 90.85% (5.39%)\n",
            "Execution Time: 103.12 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Ionosphere Dataset ---------\n",
            "[0.91666667 0.91428571 0.97142857 0.94285714 0.97142857 0.88571429\n",
            " 0.91428571 1.         0.97142857 0.91428571 0.94444444 0.97142857\n",
            " 1.         0.97142857 0.85714286 0.91428571 0.97142857 0.91428571\n",
            " 0.88571429 0.91428571 0.94444444 0.85714286 0.97142857 0.94285714\n",
            " 0.97142857 0.91428571 0.94285714 0.91428571 0.94285714 0.97142857\n",
            " 1.         1.         1.         1.         0.88571429 0.91428571\n",
            " 0.82857143 0.97142857 0.88571429 0.85714286 0.94444444 1.\n",
            " 1.         0.94285714 0.94285714 0.82857143 0.97142857 1.\n",
            " 0.91428571 0.88571429 0.94444444 0.97142857 0.94285714 0.94285714\n",
            " 0.94285714 0.97142857 0.97142857 0.88571429 0.88571429 0.85714286\n",
            " 0.94444444 0.97142857 0.91428571 0.88571429 0.94285714 0.85714286\n",
            " 0.91428571 0.94285714 1.         1.         0.97222222 0.88571429\n",
            " 0.91428571 0.88571429 0.94285714 0.91428571 0.97142857 0.91428571\n",
            " 0.97142857 0.97142857 0.91666667 0.94285714 0.97142857 0.97142857\n",
            " 0.88571429 1.         0.94285714 0.97142857 0.94285714 0.88571429\n",
            " 0.94444444 0.94285714 1.         0.91428571 0.97142857 0.91428571\n",
            " 0.91428571 0.94285714 0.94285714 0.97142857]\n",
            "Accuracy: 93.82% (4.22%)\n",
            "Execution Time: 198.15 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Ionosphere Dataset ---------\n",
            "[0.94444444 0.91428571 0.94285714 0.91428571 0.94285714 0.88571429\n",
            " 0.94285714 0.97142857 0.88571429 0.88571429 0.91666667 0.94285714\n",
            " 0.94285714 0.97142857 0.85714286 0.91428571 0.97142857 0.94285714\n",
            " 0.88571429 0.97142857 0.86111111 0.8        0.97142857 0.97142857\n",
            " 0.97142857 0.94285714 0.94285714 0.91428571 0.94285714 0.97142857\n",
            " 1.         1.         1.         0.97142857 0.91428571 0.91428571\n",
            " 0.82857143 0.94285714 0.94285714 0.82857143 0.94444444 0.97142857\n",
            " 0.97142857 0.94285714 0.97142857 0.82857143 0.91428571 0.97142857\n",
            " 0.94285714 0.85714286 0.94444444 0.94285714 0.94285714 0.94285714\n",
            " 0.88571429 0.97142857 0.97142857 0.85714286 0.88571429 0.88571429\n",
            " 0.88888889 0.94285714 0.91428571 0.88571429 0.94285714 0.85714286\n",
            " 0.85714286 0.91428571 1.         1.         0.97222222 0.94285714\n",
            " 0.88571429 0.82857143 0.94285714 0.88571429 0.97142857 0.94285714\n",
            " 0.94285714 0.97142857 0.88888889 0.88571429 0.97142857 0.97142857\n",
            " 0.85714286 1.         0.88571429 0.97142857 0.91428571 0.94285714\n",
            " 0.91666667 0.94285714 1.         0.91428571 0.91428571 0.91428571\n",
            " 0.91428571 0.97142857 0.88571429 0.97142857]\n",
            "Accuracy: 92.85% (4.53%)\n",
            "Execution Time: 3.21 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Ionosphere Dataset ---------\n",
            "[0.91666667 0.91428571 0.97142857 0.94285714 0.94285714 0.88571429\n",
            " 0.88571429 1.         0.85714286 0.88571429 0.94444444 0.94285714\n",
            " 1.         0.97142857 0.88571429 0.91428571 0.97142857 0.94285714\n",
            " 0.88571429 0.88571429 0.91666667 0.85714286 1.         0.94285714\n",
            " 0.97142857 0.91428571 0.94285714 0.91428571 0.94285714 0.97142857\n",
            " 1.         1.         0.97142857 1.         0.88571429 0.94285714\n",
            " 0.82857143 0.97142857 0.85714286 0.82857143 0.91666667 0.97142857\n",
            " 1.         0.94285714 0.94285714 0.82857143 0.91428571 0.94285714\n",
            " 0.91428571 0.88571429 0.94444444 0.94285714 0.94285714 0.94285714\n",
            " 0.91428571 0.97142857 0.97142857 0.88571429 0.88571429 0.88571429\n",
            " 0.94444444 0.97142857 0.85714286 0.88571429 0.94285714 0.85714286\n",
            " 0.91428571 0.94285714 1.         0.97142857 0.91666667 0.88571429\n",
            " 0.94285714 0.82857143 0.94285714 0.94285714 0.97142857 0.94285714\n",
            " 0.94285714 1.         0.94444444 0.91428571 0.94285714 0.97142857\n",
            " 0.85714286 1.         0.91428571 0.94285714 0.91428571 0.88571429\n",
            " 0.94444444 0.91428571 1.         0.91428571 0.94285714 0.94285714\n",
            " 0.85714286 0.94285714 0.88571429 0.97142857]\n",
            "Accuracy: 92.96% (4.43%)\n",
            "Execution Time: 5.81 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "ionosphere_scores = []\n",
        "ionosphere_mean = []\n",
        "ionosphere_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "        \n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    ionosphere_scores.append(results)\n",
        "    ionosphere_mean.append(results.mean()*100)\n",
        "    ionosphere_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Ionosphere Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAHNCAYAAADMjHveAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABT3UlEQVR4nO3deVwVZd8G8OuAcNhBQVYJFFzABRSEkMw9zCXNTMpUJKVcconK1ErcycztTY00l8elMtcWjUzUV1Oe8FGoVMQVtQTcEgQVlPN7//BlHo+AchAckOv7+ZyPcp97Zu6ZOXPmOjP3zGhEREBERESkEiO1G0BEREQ1G8MIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCD0SjUaDyZMnq92MEnl6eqJHjx5qN+OJ0L59e7Rv3175Oz09HRqNBitXrtSrFx8fD39/f5iZmUGj0eDatWsAgNWrV6NJkyYwMTGBnZ3dY2s3lV/79u3RrFkztZtBNQTDyCM6deoU3nzzTTRo0ABmZmawsbFBaGgoFixYgJs3b6rdPKpAN27cwOTJk7F79261m1IlXblyBf369YO5uTkWLVqE1atXw9LSEseOHcPgwYPh5eWFpUuXYsmSJWo3tVRHjx7F5MmTkZ6eXqb6kydPhkajweXLlyu3YURPuFpqN6A627p1K15++WVotVoMGjQIzZo1Q0FBAX799Ve89957OHLkSJX+4q0IN2/eRK1aNeNjdOPGDUyZMgUA9I4S1EQeHh64efMmTExMlLIDBw7g+vXrmDZtGjp37qyU7969GzqdDgsWLIC3t7cazS2zo0ePYsqUKWjfvj08PT3Vbg5RjVEz9iKV4MyZM3jllVfg4eGBnTt3wsXFRXlv5MiROHnyJLZu3apiCyuPTqdDQUEBzMzMYGZmpnZzSAUajabYur948SIAFDsNU1r5o8jLy4OlpWWFjY/Uce93SXX2pMyHmniappw++eQT5ObmYtmyZXpBpIi3tzfGjBmj/H3nzh1MmzYNXl5e0Gq18PT0xMSJE5Gfn683XFE/h927dyMwMBDm5uZo3ry5cmpg06ZNaN68OczMzBAQEIDk5GS94QcPHgwrKyucPn0aYWFhsLS0hKurK6ZOnYr7H9D86aefok2bNrC3t4e5uTkCAgKwYcOGYvOi0Wjw1ltvYe3atWjatCm0Wi3i4+OV9+7tM3L9+nWMHTsWnp6e0Gq1cHR0RJcuXXDo0CG9ca5fvx4BAQEwNzeHg4MDBgwYgL///rvEefn777/Ru3dvWFlZoW7dunj33XdRWFhYypopbvv27Uo/Bl9fX2zatKlYnWvXrmHs2LFwd3eHVquFt7c3Zs2aBZ1OB+BuH4m6desCAKZMmQKNRqPM+/fffw+NRoM//vhDGd/GjRuh0WjQp08fven4+PggPDxcr2zNmjXKsqhTpw5eeeUVnD9/vlgbf/vtN3Tt2hW2trawsLBAu3btsG/fPr06RacNTp48icGDB8POzg62traIjIzEjRs3yrS8lixZAi8vL5ibmyMoKAh79+4tVuf+PiPt27dHREQEAKB169bQaDQYPHgwPD09ERMTAwCoW7dusc/LTz/9hLZt28LS0hLW1tbo3r07jhw5ojetos/BqVOn0K1bN1hbW+O1114DcHcnMH/+fDRt2hRmZmZwcnLCm2++iX/++UdvHEXb1a+//oqgoCCYmZmhQYMGWLVqlVJn5cqVePnllwEAHTp0UNZxeU7LVfTnOy8vD++8847y+WzcuDE+/fTTYtv0L7/8gmeeeQZ2dnawsrJC48aNMXHiROX93bt3Q6PRYN26dZg4cSKcnZ1haWmJF154ocTPHHD3aFGHDh1gYWEBNzc3fPLJJ8Xq5OfnIyYmBt7e3tBqtXB3d8e4ceOKfb896Lvk77//xuuvvw4nJydotVo0bdoUy5cvL9Pyfth8A8CtW7cwefJkNGrUCGZmZnBxcUGfPn1w6tQpg5dzRczHZ599hqZNm8LCwgK1a9dGYGAgvvrqqzLN7xNJqFzc3NykQYMGZa4fEREhAKRv376yaNEiGTRokACQ3r1769Xz8PCQxo0bi4uLi0yePFnmzZsnbm5uYmVlJWvWrJGnnnpKPv74Y/n444/F1tZWvL29pbCwUG86ZmZm0rBhQxk4cKAsXLhQevToIQDko48+0ptWvXr1ZMSIEbJw4UKZO3euBAUFCQD58ccf9eoBEB8fH6lbt65MmTJFFi1aJMnJycp7MTExSt3+/fuLqampREdHy5dffimzZs2Snj17ypo1a5Q6K1asEADSunVrmTdvnowfP17Mzc3F09NT/vnnn2Lz0rRpU3n99dfl888/l5deekkAyOLFix+6zD08PKRRo0ZiZ2cn48ePl7lz50rz5s3FyMhItm/frtTLy8uTFi1aiL29vUycOFHi4uJk0KBBotFoZMyYMSIikpubK59//rkAkBdffFFWr14tq1evlt9//12uXLkiGo1GPvvsM2WcY8aMESMjI6lbt65SdvHiRQEgCxcuVMqmT58uGo1GwsPDZfHixTJlyhRxcHAotiwSEhLE1NRUQkJCZM6cOTJv3jxp0aKFmJqaym+//abUi4mJEQDSsmVL6dOnjyxevFiGDh0qAGTcuHEPXWZffvmlAJA2bdrI//zP/8jYsWPFzs5OGjRoIO3atVPqnTlzRgDIihUrRERk+/bt8sYbbwgAmTp1qqxevVr2798vmzdvlhdffFEAyOeff64sMxGRVatWiUajka5du8pnn30ms2bNEk9PT7Gzs5MzZ84o04qIiBCtViteXl4SEREhcXFxsmrVKhERGTp0qNSqVUuioqIkLi5O3n//fbG0tJTWrVtLQUGB3mehcePG4uTkJBMnTpSFCxdKq1atRKPRyOHDh0VE5NSpUzJ69GgBIBMnTlTWcWZmZqnLq2h5X7p0SSmr6M+3TqeTjh07ikajkaFDh8rChQulZ8+eAkDGjh2r1Dt8+LCYmppKYGCgLFiwQOLi4uTdd9+VZ599Vqmza9cuASDNmzeXFi1ayNy5c2X8+PFiZmYmjRo1khs3bih127VrJ66uruLu7i5jxoyRxYsXS8eOHQWAbNu2TalXWFgozz33nFhYWMjYsWPliy++kLfeektq1aolvXr10ltepX2XZGZmSr169cTd3V2mTp0qn3/+ubzwwgsCQObNm1fq8i/rfN+5c0c6deokAOSVV16RhQsXSmxsrHTs2FG2bNli0HKuiPlYsmSJsj/44osvZMGCBTJkyBAZPXr0A+f1ScYwUg7Z2dkCoNiGVpqUlBQBIEOHDtUrf/fddwWA7Ny5Uynz8PAQALJ//36l7OeffxYAYm5uLmfPnlXKv/jiCwEgu3btUsqKQs+oUaOUMp1OJ927dxdTU1O9L817v3hERAoKCqRZs2bSsWNHvXIAYmRkJEeOHCk2b/eHEVtbWxk5cmSpy6KgoEAcHR2lWbNmcvPmTaX8xx9/FAAyadKkYvMydepUvXG0bNlSAgICSp1GkaJluXHjRqUsOztbXFxcpGXLlkrZtGnTxNLSUo4fP643/Pjx48XY2FjOnTsnIiKXLl0qNr9FmjZtKv369VP+btWqlbz88ssCQFJTU0VEZNOmTQJA2Rmnp6eLsbGxzJgxQ29cf/75p9SqVUsp1+l00rBhQwkLCxOdTqfUu3HjhtSvX1+6dOmilBXtHF9//XW9cb744otib2//wOVVtG78/f0lPz9fKS/64nxQGBH57074wIEDeuMtaYd9/fp1sbOzk6ioKL26mZmZYmtrq1de9DkYP368Xt29e/cKAFm7dq1eeXx8fLHyos/Cnj17lLKLFy+KVquVd955Rylbv359sW3qQe6ft8r4fG/ZskUAyPTp0/Xq9e3bVzQajZw8eVJERObNm1dsOd+vKIy4ublJTk6OUv7tt98KAFmwYIFS1q5dOwGgBD8Rkfz8fHF2dpaXXnpJKVu9erUYGRnJ3r179aYVFxcnAGTfvn1KWWnfJUOGDBEXFxe5fPmyXvkrr7witra2xb6r7lWW+V6+fLkAkLlz5xZ7r2ibKutyroj56NWrlzRt2rTU9tZEPE1TDjk5OQAAa2vrMtXftm0bACA6Olqv/J133gGAYn1LfH19ERISovwdHBwMAOjYsSOeeuqpYuWnT58uNs233npL+X/RIcWCggLs2LFDKTc3N1f+/88//yA7Oxtt27YtdkoFANq1awdfX9+HzOndfgG//fYbLly4UOL7//nPf3Dx4kWMGDFC7/xq9+7d0aRJkxL72QwbNkzv77Zt25Y4zyVxdXXFiy++qPxtY2ODQYMGITk5GZmZmQDuHlJv27YtateujcuXLyuvzp07o7CwEHv27HnodNq2bauczrh+/Tp+//13vPHGG3BwcFDK9+7dCzs7O+VyyU2bNkGn06Ffv35603V2dkbDhg2xa9cuAEBKSgpOnDiB/v3748qVK0q9vLw8dOrUCXv27FFOJz1omV25ckX57JakaN0MGzYMpqamSvngwYNha2v70GVgiF9++QXXrl3Dq6++qjfvxsbGCA4OVub9XsOHD9f7e/369bC1tUWXLl30xhEQEAArK6ti4/D19UXbtm2Vv+vWrYvGjRuX+bNUFpXx+d62bRuMjY0xevRovXrvvPMORAQ//fQTgP/2yfnuu++KfR7uN2jQIL3vr759+8LFxUX5ripiZWWFAQMGKH+bmpoiKChIr33r16+Hj48PmjRporceOnbsCADF1sP93yUigo0bN6Jnz54QEb1xhIWFITs7u8TvpCJlme+NGzfCwcEBo0aNKvaeRqMBUPblXBHzYWdnh7/++gsHDhwodb5qGnZgLQcbGxsAd3c6ZXH27FkYGRkVu5LA2dkZdnZ2OHv2rF75vYEDgLIjcHd3L7H8/vPjRkZGaNCggV5Zo0aNAEDvksUff/wR06dPR0pKit653aKN817169cvdf7u9cknnyAiIgLu7u4ICAhAt27dMGjQIKU9RfPauHHjYsM2adIEv/76q16ZmZmZ0lejSO3atYvNc2m8vb2Lzc+9y8LZ2RknTpzAH3/8UWw6RYo6YD5I27ZtERcXh5MnT+LUqVPQaDQICQlRQkpUVBT27t2L0NBQGBnd/Q1w4sQJiAgaNmxY4jiLrlQ5ceIEACh9MkqSnZ2N2rVrK3/f/xkqeu+ff/5RPr/3K1o397fHxMSk2OfpURXNU9EO6373t7FWrVqoV69esXFkZ2fD0dGxxHHcv97uXyaAYZ+lsqiMz/fZs2fh6upa7MePj4+P3jTDw8Px5ZdfYujQoRg/fjw6deqEPn36oG/fvspnrsj961ij0cDb27vYJc316tUrtv3Url1br3/UiRMnkJqaWubt5/7vkkuXLuHatWtYsmRJqVcfPmgbLMt8nzp1Co0bN37glX9lXc4VMR/vv/8+duzYgaCgIHh7e+O5555D//79ERoaWmr7nnQMI+VgY2MDV1dXHD582KDhStrJl8TY2Nigcrmvc1VZ7N27Fy+88AKeffZZLF68GC4uLjAxMcGKFStK7ER171GUB+nXrx/atm2LzZs3Y/v27Zg9ezZmzZqFTZs24fnnnze4naXNc0XS6XTo0qULxo0bV+L7ReHlQZ555hkAwJ49e3D69Gm0atUKlpaWaNu2Lf7nf/4Hubm5SE5OxowZM/Smq9Fo8NNPP5U4n1ZWVko9AJg9ezb8/f1LnH5R3SIV+VmpDEXztHr1ajg7Oxd7//6dhlarLbZD1el0cHR0xNq1a0ucxv07x6q4TCry821ubo49e/Zg165d2Lp1K+Lj47Fu3Tp07NgR27dvL9e0yrLMdDodmjdvjrlz55ZY9/4fUfd/lxR9FgYMGFBq4G7RokWpbayM+S6LR5kPHx8fpKWl4ccff0R8fDw2btyIxYsXY9KkScrtA2oahpFy6tGjB5YsWYLExES9Uyol8fDwgE6nw4kTJ5SUDQBZWVm4du0aPDw8KrRtOp0Op0+f1tuJHj9+HACUeyds3LgRZmZm+Pnnn6HVapV6K1aseOTpu7i4YMSIERgxYgQuXryIVq1aYcaMGXj++eeVeU1LSyv2qzgtLa3Cl8XJkychInpB8P5l4eXlhdzcXL17Y5TkQWHyqaeewlNPPYW9e/fi9OnTyumAZ599FtHR0Vi/fj0KCwvx7LPPKsN4eXlBRFC/fv0HBh4vLy8Ad0Pww9r4KIqW/YkTJ/TWze3bt3HmzBn4+flV2LSK5snR0bHc8+Tl5YUdO3YgNDS0zGH5Ycr6g6E0lfH59vDwwI4dO3D9+nW9X+3Hjh3TmyZw96hop06d0KlTJ8ydOxczZ87EBx98gF27dukt56IjU0VEBCdPnnzgTr80Xl5e+P3339GpU6dyLb+6devC2toahYWF5f4sPGy+vby88Ntvv+H27dt698a5lyHLuSLmw9LSEuHh4QgPD0dBQQH69OmDGTNmYMKECTXyEmH2GSmncePGwdLSEkOHDkVWVlax90+dOoUFCxYAALp16wYAmD9/vl6dol8S3bt3r/D2LVy4UPm/iGDhwoUwMTFBp06dANz9xaPRaPQuIUxPT8eWLVvKPc3CwkJkZ2frlTk6OsLV1VU5DRQYGAhHR0fExcXpnRr66aefkJqaWuHL4sKFC9i8ebPyd05ODlatWgV/f3/lF3m/fv2QmJiIn3/+udjw165dw507dwAAFhYWSllJ2rZti507dyIpKUkJI/7+/rC2tsbHH3+sXD5dpE+fPjA2NsaUKVOK/ToXEVy5cgUAEBAQAC8vL3z66afIzc0tNt1Lly6VdXE8UGBgIOrWrYu4uDgUFBQo5StXrix1nssrLCwMNjY2mDlzJm7fvl3s/bLMU79+/VBYWIhp06YVe+/OnTvlanPRvUvKO7+V8fnu1q0bCgsL9bZpAJg3bx40Go1yxPHq1avFhi06knb/JbarVq3SO828YcMGZGRklOvoZb9+/fD3339j6dKlxd67efMm8vLyHji8sbExXnrpJWzcuLHEo80P+yyUZb5feuklXL58udgyBP57lKesy7ki5qNo2y5iamoKX19fiEiJ20NNwCMj5eTl5YWvvvoK4eHh8PHx0bsD6/79+7F+/XoMHjwYAODn54eIiAgsWbIE165dQ7t27ZCUlIR//etf6N27Nzp06FChbTMzM0N8fDwiIiIQHByMn376CVu3bsXEiROVQ9fdu3fH3Llz0bVrV/Tv3x8XL17EokWL4O3trXc+2BDXr19HvXr10LdvX/j5+cHKygo7duzAgQMHMGfOHAB3+x/MmjULkZGRaNeuHV599VVkZWVhwYIF8PT0xNtvv11hywG4e4plyJAhOHDgAJycnLB8+XJkZWXpHQF677338P3336NHjx4YPHgwAgICkJeXhz///BMbNmxAeno6HBwcYG5uDl9fX6xbtw6NGjVCnTp10KxZM6VDatu2bbF27VpoNBrltI2xsTHatGmDn3/+Ge3bt9frGOrl5YXp06djwoQJSE9PR+/evWFtbY0zZ85g8+bNeOONN/Duu+/CyMgIX375JZ5//nk0bdoUkZGRcHNzw99//41du3bBxsYGP/zwwyMvKxMTE0yfPh1vvvkmOnbsiPDwcJw5cwYrVqyo8D4jNjY2+PzzzzFw4EC0atUKr7zyCurWrYtz585h69atCA0NLXHHca927drhzTffRGxsLFJSUvDcc8/BxMQEJ06cwPr167FgwQL07dvXoHb5+/vD2NgYs2bNQnZ2NrRaLTp27Fhqv5T7Vcbnu2fPnujQoQM++OADpKenw8/PD9u3b8d3332HsWPHKkeZpk6dij179qB79+7w8PDAxYsXsXjxYtSrV0/5PBapU6cOnnnmGURGRiIrKwvz58+Ht7c3oqKiDG7fwIED8e2332LYsGHYtWsXQkNDUVhYiGPHjuHbb7/Fzz//jMDAwAeO4+OPP8auXbsQHByMqKgo+Pr64urVqzh06BB27NhRYuAoUpb5HjRoEFatWoXo6Gjlx0JeXh527NiBESNGoFevXmVezhUxH8899xycnZ0RGhoKJycnpKamYuHChejevXuZL4x44jzmq3eeOMePH5eoqCjx9PQUU1NTsba2ltDQUPnss8/k1q1bSr3bt2/LlClTpH79+mJiYiLu7u4yYcIEvToidy9B7N69e7HpACh2yWzR5ZWzZ89WyiIiIsTS0lJOnTqlXPvv5OQkMTExevcjERFZtmyZNGzYULRarTRp0kRWrFihXKr4sGnf+17Rpa75+fny3nvviZ+fn1hbW4ulpaX4+fmVeE+QdevWScuWLUWr1UqdOnXktddek7/++kuvTtG83K+kNpakaFn+/PPP0qJFC2U+169fX6zu9evXZcKECeLt7S2mpqbi4OAgbdq0kU8//VTvfhX79++XgIAAMTU1LXaZ75EjR5T7D9xr+vTpJd7npcjGjRvlmWeeEUtLS7G0tJQmTZrIyJEjJS0tTa9ecnKy9OnTR+zt7UWr1YqHh4f069dPEhISii2b+y9zLLrs9t77d5Rm8eLFUr9+fdFqtRIYGCh79uyRdu3aVeilvUV27dolYWFhYmtrK2ZmZuLl5SWDBw+W//znP0qd0j4HRZYsWSIBAQFibm4u1tbW0rx5cxk3bpxcuHBBqVPadnX/fImILF26VBo0aCDGxsYPvcy3tHmr6M/39evX5e233xZXV1cxMTGRhg0byuzZs/Uu9U5ISJBevXqJq6urmJqaiqurq7z66qt6l6wXXdr79ddfy4QJE8TR0VHMzc2le/fuercNKFo2JV1+GhERIR4eHnplBQUFMmvWLGnatKlotVqpXbu2BAQEyJQpUyQ7O1up96DvkqysLBk5cqS4u7uLiYmJODs7S6dOnWTJkiUl1jdkvkXuXgr/wQcfKN/Bzs7O0rdvXzl16pRBy7ki5uOLL76QZ599VtmWvby85L333tNbVjWNRqSK9GijCjF48GBs2LChxMP5RFSz7d69Gx06dMD69esNPmpEVJnYZ4SIiIhUxTBCREREqmIYISIiIlWxzwgRERGpikdGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqgwOI3v27EHPnj3h6uoKjUaDLVu2PHSY3bt3o1WrVtBqtfD29sbKlSvL0VQiIiJ6EhkcRvLy8uDn54dFixaVqf6ZM2fQvXt3dOjQASkpKRg7diyGDh2Kn3/+2eDGEhER0ZNHIyJS7oE1GmzevBm9e/cutc7777+PrVu34vDhw0rZK6+8gmvXriE+Pr68kyYiIqInRKX3GUlMTETnzp31ysLCwpCYmFjZkyYiIqJqoFZlTyAzMxNOTk56ZU5OTsjJycHNmzdhbm5ebJj8/Hzk5+crf+t0Oly9ehX29vbQaDSV3WQiIiKqACKC69evw9XVFUZGpR//qPQwUh6xsbGYMmWK2s0gIiKiCnD+/HnUq1ev1PcrPYw4OzsjKytLrywrKws2NjYlHhUBgAkTJiA6Olr5Ozs7G0899RTOnz8PGxubSm1vWd24cQPHjx8vc/20tDS88cYbWLJkCRo3bmzQtBo1agQLCwtDm1hjGLougPKvD66LB+O6qFoe1/cU18XD1dR9Rk5ODtzd3WFtbf3AepUeRkJCQrBt2za9sl9++QUhISGlDqPVaqHVaouV29jYVJkwYmNjA2dn5zLXt7KyAgAEBASgVatWldWsGsnQdQFwfVQWrouqhd9TVUdNXxcP62JhcAfW3NxcpKSkICUlBcDdS3dTUlJw7tw5AHePagwaNEipP2zYMJw+fRrjxo3DsWPHsHjxYnz77bd4++23DZ00ERERPYEMDiP/+c9/0LJlS7Rs2RIAEB0djZYtW2LSpEkAgIyMDCWYAED9+vWxdetW/PLLL/Dz88OcOXPw5ZdfIiwsrIJmgYiIiKozg0/TtG/fHg+6NUlJd1dt3749kpOTDZ0UERER1QB8Ng0RERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSVZV8aq9aTpw4gevXr1fKuFNTU/X+rSzW1tZo2LBhpU6DapbK3C4AbhuG4LqoOrguKpZGHnQ71SoiJycHtra2yM7OrrQH5Z04cQKNGjWqlHE/bsePH68SH66q7NChQwgICMDBgwefiIdQVZYnabsAqve2wXVRdXBdlF1Z9988MvL/ihLumjVr4OPjU+Hjv3nzJtLT0+Hp6Qlzc/MKHz9wN0EPGDCgUtM61SyVvV0A3DbKiuui6uC6qHgMI/fx8fGptF/KoaGhlTJeospWmdsFwG3DEFwXVQfXRcVhB1YiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYeQxSbyQiF5beiHxQqLaTSGqUrhtEBGfTfP/NHduoaWzEcyvHQcuVGxGExEsSIrF6ZwzWPBbLJ4OmgKNRlOh0wAA82vH0dLZCJo7typ83I/bk/B47qryaO5HUZnbBcBtwxCVvS4eF66LqqMqrQuNiIjajXiYsj6C+FGk7vwGPnverJRx7zM3wzBnR+XvuMyLCL1ZeSs/9dkv4NPxlUobf2V7kh7PXZ0fkw5U7nYBcNswRGWvi8eN66LqqMx1Udb9N4+M/L9bVk+h1Re5WLt2LXyaNKmw8YoIPkuKgVHOWeiggxGM8FmjYLSphF+AqceO4bXXXsOybk9V6Hgftyfh8dxV6dHcj6KytguA24ahKnNdPE5cF1VHVVoXDCP/T2qZITlTh5t2jQBX/wob7/6/9+FIzhnlbx10OJJzBvtxA6GuFft46JuZOiRn6iC1zCp0vGrh47nVV1nbBcBtw1CVuS4eJ66LqqMqrYvqe7KrGhARfJb8GYw0+ovZSGOEz5I/QzU4Q0ZUKbhtVE3sTFx11LR1wTBSifZf2I8jV45AJzq9cp3ocOTKEey/sF+llhGpi9tG1SMiWHBoAU5nn8aCQwsYCFVUE9cFw0glKfrlp0HJ57410PAXINVI3DaqpqKACICBUGU1cV0wjFSS27rbyMzLhKDkL1SBIDMvE7d1tx9zy4jUxW2j6rn/tBlPl6mnpq4LdmCtJKbGpvimxze4eutqqXXqmNWBqbHpY2wVkfq4bVQ99/4SB/RPl4W6saP341RT1wXDSCVytnSGs6Wz2s0gqnK4bVQd9/4Sv7cPT9Ev8jaubSrlRnRUXE1eFzxNQ0RUg7EzcdVRk9cFwwgRUQ3FzsRVR01fFwwjREQ1FDsTVx01fV2wzwgRUQ3FzsRVR01fFwwjREQ1GDsTVx01eV0wjPy/GzduAAAOHTpUKeOv7AezAXcfzvYkeByP5068chgfp63G+MYDEWLfrMLHX5Uezf0oKnu7ALhtEBHDiOLYsWMAgKioKJVb8uisra3VbsIjMcs9h0NvWgF73gT2VPz4BcACVyec1mqxIHEqnr6QVUqXsfLzAXDoTSuk5p4D0KaCx/74PEnbBVD9tw2iJxXDyP/r3bs3AKBJkyawsLCo8PEXPVJ+zZo18PHxqfDxF7G2tkbDhg0rbfyPQ2U/nnv/5T9wJHk2AOCIVov9fT5DqEOLCp1GVXo096Oo7O0C4LZBRAwjCgcHBwwdOrTSp+Pj44NWrVpV+nSqs8p8PLeI4LNDHys3FTLSGOGzc9vQpvnACr2ZUFV6NPejeFzbBcBtg6gmK9cJ+UWLFsHT0xNmZmYIDg5GUlJSqXVv376NqVOnwsvLC2ZmZvDz80N8fHy5G0z0KO6/qVBNuJkQEVFVZ3AYWbduHaKjoxETE4NDhw7Bz88PYWFhuHjxYon1P/zwQ3zxxRf47LPPcPToUQwbNgwvvvgikpOTH7nxRIa4/wFURWrKg6iIiKoqg8PI3LlzERUVhcjISPj6+iIuLg4WFhZYvnx5ifVXr16NiRMnolu3bmjQoAGGDx+Obt26Yc6cOY/ceCJD1ORbLRMRVWUGhZGCggIcPHgQnTt3/u8IjIzQuXNnJCYmljhMfn4+zMz0z5ubm5vj119/LXU6+fn5yMnJ0XsRPYqafqtlIqKqzKAwcvnyZRQWFsLJyUmv3MnJCZmZmSUOExYWhrlz5+LEiRPQ6XT45ZdfsGnTJmRkZJQ6ndjYWNja2iovd3d3Q5pJVExNv9UyEVFVVulX0yxYsABRUVFo0qQJNBoNvLy8EBkZWeppHQCYMGECoqOjlb9zcnIYSOiR1PRbLRMRVWUGhREHBwcYGxsjKytLrzwrKwvOziXfwrZu3brYsmULbt26hStXrsDV1RXjx49HgwYNSp2OVquFVqs1pGlED1WTb7VMRFSVGXSaxtTUFAEBAUhISFDKdDodEhISEBIS8sBhzczM4Obmhjt37mDjxo3o1atX+VpMRERETxSDT9NER0cjIiICgYGBCAoKwvz585GXl4fIyEgAwKBBg+Dm5obY2FgAwG+//Ya///4b/v7++PvvvzF58mTodDqMGzeuYueEiIiIqiWDw0h4eDguXbqESZMmITMzE/7+/oiPj1c6tZ47dw5GRv894HLr1i18+OGHOH36NKysrNCtWzesXr0adnZ2FTYTREREVH2VqwPrW2+9hbfeeqvE93bv3q33d7t27XD06NHyTIaIiP4fn6BMTzI+m4aIqBrgE5SrDgbDiscwQkRUDfAJylUHg2HFYxghIqoG+ATlqoPBsOIxjBARERmAwbDiGfygPCIiIqKKxDBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJV8dk0VOU8CY/nrkqP5iYiquoYRqjKeZIez10VHs1NRFTVMYxQlfOkPJ67qjyam4ioqmMYoSqHj+cmIqpZ2IGViIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVfHZNOV048YN5emyZVH0SPnyPFq+Mh8YR1SRDN0ugPJvG9wuqDrhPuPBGEbK6dixYwgICDB4uAEDBhg8zMGDB/kwN6oWyrtdAIZvG9wuqDrhPuPBGEbKqUmTJjh48GCZ69+8eRPp6enw9PSEubm5wdMiqg4M3S6A8m8b3C6oOuE+48EYRsrJwsLC4OQZGhpaSa0hqhrKs10A3Dboycd9xoOxAysRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQq3mfkMSgsLMTevXuRkZEBFxcXtG3bFsbGxmo3i4iIqEoo15GRRYsWwdPTE2ZmZggODkZSUtID68+fPx+NGzeGubk53N3d8fbbb+PWrVvlanB1s2nTJnh7e6NDhw7o378/OnToAG9vb2zatEntphEREVUJBoeRdevWITo6GjExMTh06BD8/PwQFhaGixcvllj/q6++wvjx4xETE4PU1FQsW7YM69atw8SJEx+58VXdpk2b0LdvXzRv3hyJiYm4fv06EhMT0bx5c/Tt25eBhIiICOUII3PnzkVUVBQiIyPh6+uLuLg4WFhYYPny5SXW379/P0JDQ9G/f394enriueeew6uvvvrQoynVXWFhId555x306NEDW7ZswdNPPw0rKys8/fTT2LJlC3r06IF3330XhYWFajeViIhIVQb1GSkoKMDBgwcxYcIEpczIyAidO3dGYmJiicO0adMGa9asQVJSEoKCgnD69Gls27YNAwcOLHU6+fn5yM/PV/7OyckxpJlVwt69e5Geno6vv/4aRkb6mc/IyAgTJkxAmzZtsHfvXrRv316dRhLRE+1xPba+Oj6ynqoWg8LI5cuXUVhYCCcnJ71yJyenUj/w/fv3x+XLl/HMM89ARHDnzh0MGzbsgadpYmNjMWXKFEOaVuVkZGQAAJo1a1bi+0XlRfWIiCra43psfXV8ZD1VLZV+Nc3u3bsxc+ZMLF68GMHBwTh58iTGjBmDadOm4aOPPipxmAkTJiA6Olr5OycnB+7u7pXd1Arl4uICADh8+DCefvrpYu8fPnxYrx4RUUV7XI+tr46PrKeqxaAw4uDgAGNjY2RlZemVZ2VlwdnZucRhPvroIwwcOBBDhw4FADRv3hx5eXl444038MEHHxQ7hQEAWq0WWq3WkKZVOW3btoWnpydmzpyJLVu26M2nTqdDbGws6tevj7Zt26rYSiJ6kvGx9VRdGNSB1dTUFAEBAUhISFDKdDodEhISEBISUuIwN27cKBY4iu6xISKGtrfaMDY2xpw5c/Djjz+id+/eelfT9O7dGz/++CM+/fRT3m+EiIhqPINP00RHRyMiIgKBgYEICgrC/PnzkZeXh8jISADAoEGD4ObmhtjYWABAz549MXfuXLRs2VI5TfPRRx+hZ8+eT/yOuE+fPtiwYQPeeecdtGnTRimvX78+NmzYgD59+qjYOiIioqrB4DASHh6OS5cuYdKkScjMzIS/vz/i4+OVTq3nzp3TOxLy4YcfQqPR4MMPP8Tff/+NunXromfPnpgxY0bFzUUV1qdPH/Tq1Yt3YCUiIiqFRqrBuZKcnBzY2toiOzsbNjY2ajeHngCHDh1CQEAArwIgIqpEZd1/80F5REREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqir9QXlElc3Qx6QDfFQ6EVFVwjBC1V55H5MO8FHpRERVAcMIVXuGPiYd4KPSiYiqEt4OnoiIiCoFbwdPRERE1QLDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVeUKI4sWLYKnpyfMzMwQHByMpKSkUuu2b98eGo2m2Kt79+7lbjQRERE9OQwOI+vWrUN0dDRiYmJw6NAh+Pn5ISwsDBcvXiyx/qZNm5CRkaG8Dh8+DGNjY7z88suP3HgiIiKq/gwOI3PnzkVUVBQiIyPh6+uLuLg4WFhYYPny5SXWr1OnDpydnZXXL7/8AgsLC4YRIiIiAmBgGCkoKMDBgwfRuXPn/47AyAidO3dGYmJimcaxbNkyvPLKK7C0tCy1Tn5+PnJycvReRERE9GQyKIxcvnwZhYWFcHJy0it3cnJCZmbmQ4dPSkrC4cOHMXTo0AfWi42Nha2trfJyd3c3pJlERERUjTzWq2mWLVuG5s2bIygo6IH1JkyYgOzsbOV1/vz5x9RCIiIietxqGVLZwcEBxsbGyMrK0ivPysqCs7PzA4fNy8vDN998g6lTpz50OlqtFlqt1pCmERERUTVl0JERU1NTBAQEICEhQSnT6XRISEhASEjIA4ddv3498vPzMWDAgPK1lIiIiJ5IBh0ZAYDo6GhEREQgMDAQQUFBmD9/PvLy8hAZGQkAGDRoENzc3BAbG6s33LJly9C7d2/Y29tXTMuJiIjoiWBwGAkPD8elS5cwadIkZGZmwt/fH/Hx8Uqn1nPnzsHISP+AS1paGn799Vds3769YlpNRERETwyNiIjajXiYnJwc2NraIjs7GzY2Nmo3h4iIiMqgrPtvPpuGiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqipXGFm0aBE8PT1hZmaG4OBgJCUlPbD+tWvXMHLkSLi4uECr1aJRo0bYtm1buRpMRERET5Zahg6wbt06REdHIy4uDsHBwZg/fz7CwsKQlpYGR0fHYvULCgrQpUsXODo6YsOGDXBzc8PZs2dhZ2dXEe0nIiKiak4jImLIAMHBwWjdujUWLlwIANDpdHB3d8eoUaMwfvz4YvXj4uIwe/ZsHDt2DCYmJuVqZE5ODmxtbZGdnQ0bG5tyjYOIiIger7Luvw06TVNQUICDBw+ic+fO/x2BkRE6d+6MxMTEEof5/vvvERISgpEjR8LJyQnNmjXDzJkzUVhYWOp08vPzkZOTo/ciIiKiJ5NBYeTy5csoLCyEk5OTXrmTkxMyMzNLHOb06dPYsGEDCgsLsW3bNnz00UeYM2cOpk+fXup0YmNjYWtrq7zc3d0NaSYRERFVI5V+NY1Op4OjoyOWLFmCgIAAhIeH44MPPkBcXFypw0yYMAHZ2dnK6/z585XdTCIiIlKJQR1YHRwcYGxsjKysLL3yrKwsODs7lziMi4sLTExMYGxsrJT5+PggMzMTBQUFMDU1LTaMVquFVqs1pGlERERUTRl0ZMTU1BQBAQFISEhQynQ6HRISEhASElLiMKGhoTh58iR0Op1Sdvz4cbi4uJQYRIiIiKhmMfg0TXR0NJYuXYp//etfSE1NxfDhw5GXl4fIyEgAwKBBgzBhwgSl/vDhw3H16lWMGTMGx48fx9atWzFz5kyMHDmy4uaCiIiIqi2D7zMSHh6OS5cuYdKkScjMzIS/vz/i4+OVTq3nzp2DkdF/M467uzt+/vlnvP3222jRogXc3NwwZswYvP/++xU3F0RERFRtGXyfETXwPiNERETVT6XcZ4SIiIioojGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVlSuMLFq0CJ6enjAzM0NwcDCSkpJKrbty5UpoNBq9l5mZWbkbTERERE8Wg8PIunXrEB0djZiYGBw6dAh+fn4ICwvDxYsXSx3GxsYGGRkZyuvs2bOP1GgiIiJ6chgcRubOnYuoqChERkbC19cXcXFxsLCwwPLly0sdRqPRwNnZWXk5OTk9UqOJiIjoyWFQGCkoKMDBgwfRuXPn/47AyAidO3dGYmJiqcPl5ubCw8MD7u7u6NWrF44cOVL+FhMREdETxaAwcvnyZRQWFhY7suHk5ITMzMwSh2ncuDGWL1+O7777DmvWrIFOp0ObNm3w119/lTqd/Px85OTk6L2IiIjoyVTpV9OEhIRg0KBB8Pf3R7t27bBp0ybUrVsXX3zxRanDxMbGwtbWVnm5u7tXdjOJiIhIJQaFEQcHBxgbGyMrK0uvPCsrC87OzmUah4mJCVq2bImTJ0+WWmfChAnIzs5WXufPnzekmURERFSNGBRGTE1NERAQgISEBKVMp9MhISEBISEhZRpHYWEh/vzzT7i4uJRaR6vVwsbGRu9FRERET6Zahg4QHR2NiIgIBAYGIigoCPPnz0deXh4iIyMBAIMGDYKbmxtiY2MBAFOnTsXTTz8Nb29vXLt2DbNnz8bZs2cxdOjQip0TIiIiqpYMDiPh4eG4dOkSJk2ahMzMTPj7+yM+Pl7p1Hru3DkYGf33gMs///yDqKgoZGZmonbt2ggICMD+/fvh6+tbcXNBRERE1ZZGRETtRjxMTk4ObG1tkZ2dzVM2RERE1URZ9998Ng0RERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUVa4wsmjRInh6esLMzAzBwcFISkoq03DffPMNNBoNevfuXZ7JEhER0RPI4DCybt06REdHIyYmBocOHYKfnx/CwsJw8eLFBw6Xnp6Od999F23bti13Y4mIiOjJY3AYmTt3LqKiohAZGQlfX1/ExcXBwsICy5cvL3WYwsJCvPbaa5gyZQoaNGjwSA0mIiKiJ4tBYaSgoAAHDx5E586d/zsCIyN07twZiYmJpQ43depUODo6YsiQIWWaTn5+PnJycvReRERE9GQyKIxcvnwZhYWFcHJy0it3cnJCZmZmicP8+uuvWLZsGZYuXVrm6cTGxsLW1lZ5ubu7G9JMIiIiqkYq9Wqa69evY+DAgVi6dCkcHBzKPNyECROQnZ2tvM6fP1+JrSQiIiI11TKksoODA4yNjZGVlaVXnpWVBWdn52L1T506hfT0dPTs2VMp0+l0dydcqxbS0tLg5eVVbDitVgutVmtI04iIiKiaMujIiKmpKQICApCQkKCU6XQ6JCQkICQkpFj9Jk2a4M8//0RKSoryeuGFF9ChQwekpKTw9AsREREZdmQEAKKjoxEREYHAwEAEBQVh/vz5yMvLQ2RkJABg0KBBcHNzQ2xsLMzMzNCsWTO94e3s7ACgWDkRERHVTAaHkfDwcFy6dAmTJk1CZmYm/P39ER8fr3RqPXfuHIyMeGNXIiIiKhuNiIjajXiYnJwc2NraIjs7GzY2Nmo3h4iIiMqgrPtvHsIgIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqsoVRhYtWgRPT0+YmZkhODgYSUlJpdbdtGkTAgMDYWdnB0tLS/j7+2P16tXlbjARERE9WQwOI+vWrUN0dDRiYmJw6NAh+Pn5ISwsDBcvXiyxfp06dfDBBx8gMTERf/zxByIjIxEZGYmff/75kRtPRERE1Z9GRMSQAYKDg9G6dWssXLgQAKDT6eDu7o5Ro0Zh/PjxZRpHq1at0L17d0ybNq1M9XNycmBra4vs7GzY2NgY0lwiIiJSSVn337UMGWlBQQEOHjyICRMmKGVGRkbo3LkzEhMTHzq8iGDnzp1IS0vDrFmzSq2Xn5+P/Px85e/s7GwAd2eKiIiIqoei/fbDjnsYFEYuX76MwsJCODk56ZU7OTnh2LFjpQ6XnZ0NNzc35Ofnw9jYGIsXL0aXLl1KrR8bG4spU6YUK3d3dzekuURERFQFXL9+Hba2tqW+b1AYKS9ra2ukpKQgNzcXCQkJiI6ORoMGDdC+ffsS60+YMAHR0dHK3zqdDlevXoW9vT00Gs3jaHKFy8nJgbu7O86fP89TTVUA10fVwXVRdXBdVB1PyroQEVy/fh2urq4PrGdQGHFwcICxsTGysrL0yrOysuDs7FzqcEZGRvD29gYA+Pv7IzU1FbGxsaWGEa1WC61Wq1dmZ2dnSFOrLBsbm2r9wXrScH1UHVwXVQfXRdXxJKyLBx0RKWLQ1TSmpqYICAhAQkKCUqbT6ZCQkICQkJAyj0en0+n1CSEiIqKay+DTNNHR0YiIiEBgYCCCgoIwf/585OXlITIyEgAwaNAguLm5ITY2FsDd/h+BgYHw8vJCfn4+tm3bhtWrV+Pzzz+v2DkhIiKiasngMBIeHo5Lly5h0qRJyMzMhL+/P+Lj45VOrefOnYOR0X8PuOTl5WHEiBH466+/YG5ujiZNmmDNmjUIDw+vuLmoBrRaLWJiYoqdfiJ1cH1UHVwXVQfXRdVR09aFwfcZISIiIqpIfDYNERERqYphhIiIiFTFMEJERESqYhh5iMmTJ8Pf31/tZtAjGDx4MHr37q12M4gemUajwZYtW8pcf/fu3dBoNLh27VqltYmoItTIMJKYmAhjY2N07969Usbv6ekJjUYDjUYDY2NjuLq6YsiQIfjnn38qZXolqcpfQpmZmRgzZgy8vb1hZmYGJycnhIaG4vPPP8eNGzcqffqDBw9W1o9Go4G9vT26du2KP/74o9KnfS9DdyyPS2ZmJkaNGoUGDRpAq9XC3d0dPXv21Lu/0IOsXLmyxJsUtm/fXm+5Ozk54eWXX8bZs2creA5Kl56eDo1Gg5SUlMc2TUM9KDxnZGTg+eefr9DpPegHV3JyMsLDw+Hi4gKtVgsPDw/06NEDP/zwg/KskaJlWvQyNTWFt7c3pk+frvc8ksmTJ0Oj0aBr167FpjN79mxoNJpSb4RZFRQWFqJNmzbo06ePXnl2djbc3d3xwQcfKGUbN25Ex44dUbt2bZibm6Nx48Z4/fXXkZycrNRZuXKl3nKzsrJCQEAANm3a9NjmCbi7XY4dO/axTrMkNTKMLFu2DKNGjcKePXtw4cKFSpnG1KlTkZGRgXPnzmHt2rXYs2cPRo8eXSnTqk5Onz6Nli1bYvv27Zg5cyaSk5ORmJiIcePG4ccff8SOHTtKHO727dsV2o6uXbsiIyMDGRkZSEhIQK1atdCjR48KnUZ1lJ6ejoCAAOzcuROzZ8/Gn3/+ifj4eHTo0AEjR4585PFHRUUhIyMDFy5cwHfffYfz589jwIABFdDymsHZ2fmxXer53Xff4emnn0Zubi7+9a9/ITU1FfHx8XjxxRfx4YcfKg8wLbJjxw5kZGTgxIkTmDJlCmbMmIHly5fr1XFxccGuXbvw119/6ZUvX74cTz31VKXP06MwNjbGypUrER8fj7Vr1yrlo0aNQp06dRATEwMAeP/99xEeHg5/f398//33SEtLw1dffYUGDRroPWQWuHt31aLvoeTkZISFhaFfv35IS0t7rPNWJUgNc/36dbGyspJjx45JeHi4zJgxQ+/92NhYcXR0FCsrK3n99dfl/fffFz8/P+X9pKQk6dy5s9jb24uNjY08++yzcvDgQb1xeHh4yLx58/TKpk2bJr6+vnplGzZsEF9fXzE1NRUPDw/59NNP9d6/evWqDBw4UOzs7MTc3Fy6du0qx48fV95PT0+XHj16iJ2dnVhYWIivr69s3bpVzpw5IwD0XhEREeVfaBUoLCxM6tWrJ7m5uSW+r9PpREQEgCxevFh69uwpFhYWEhMTI3fu3JHXX39dPD09xczMTBo1aiTz58/XG/7OnTvy9ttvi62trdSpU0fee+89GTRokPTq1UupExERofe3iMjevXsFgFy8eFEp++OPP6RDhw5iZmYmderUkaioKLl+/bryfmFhoUyZMkXc3NzE1NRU/Pz85KefflLez8/Pl5EjR4qzs7NotVp56qmnZObMmSJy9zNy7/rx8PAoz+KscM8//7y4ubmVuH7++ecfERGZM2eONGvWTCwsLKRevXoyfPhwZbns2rWr2GcvJiZGRETatWsnY8aM0Rvn6tWrxcLCQq9s9+7d0rp1azE1NRVnZ2d5//335fbt28r7t27dklGjRkndunVFq9VKaGioJCUlKe9fvXpV+vfvLw4ODmJmZibe3t6yfPlyEZFibWvXrt0jLrGKV9LnswgA2bx5s/L3vn37xM/PT7RarQQEBMjmzZsFgCQnJ4vIf9fHjh07JCAgQMzNzSUkJESOHTsmIiIrVqwotkxWrFghubm5Ym9vLy+++GKp7SzaVou+b4qmWaRTp04yYsQI5e+YmBjx8/OTHj16yPTp0/XmwcHBQYYPH14l18f9FixYILVr15YLFy7Ili1bxMTERFJSUkREJDExUQDIggULShy2aJmJ3F32tra2eu8XFhaKiYmJfPvtt0rZw/YDIg/flyxatEi8vb1Fq9WKo6OjvPTSSyJy97N2//o/c+ZMeRfNI6lxYWTZsmUSGBgoIiI//PCDeHl5KR+QdevWiVarlS+//FKOHTsmH3zwgVhbW+uFkYSEBFm9erWkpqbK0aNHZciQIeLk5CQ5OTlKnfvDyF9//SVBQUESGRmplP3nP/8RIyMjmTp1qqSlpcmKFSvE3NxcVqxYodR54YUXxMfHR/bs2SMpKSkSFhYm3t7eUlBQICIi3bt3ly5dusgff/whp06dkh9++EH+93//V+7cuSMbN24UAJKWliYZGRly7dq1Sliahrl8+bJoNBqJjY19aF0A4ujoKMuXL5dTp07J2bNnpaCgQCZNmiQHDhyQ06dPy5o1a8TCwkLWrVunDDdr1iypXbu2bNy4UVk/1tbWDwwj169flzfffFO8vb2lsLBQRERyc3PFxcVF+vTpI3/++ackJCRI/fr19ULd3LlzxcbGRr7++ms5duyYjBs3TkxMTJQvitmzZ4u7u7vs2bNH0tPTZe/evfLVV1+JiMjFixeVL/6MjAy9EKSWK1euiEajUQJTaebNmyc7d+6UM2fOSEJCgjRu3FiGDx8uIncD2Pz588XGxkYyMjIkIyNDCSr3h5ErV65Iz549pUOHDkrZX3/9JRYWFjJixAhJTU2VzZs3i4ODgxJoRERGjx4trq6usm3bNjly5IhERERI7dq15cqVKyIiMnLkSPH395cDBw7ImTNn5JdffpHvv/9eRO7+mCjaOWdkZCjDVCVlDSPZ2dlSp04dGTBggBw5ckS2bdsmjRo1KjGMBAcHy+7du+XIkSPStm1badOmjYiI3LhxQ9555x1p2rSpsr5u3LghmzZtEgCSmJj40PaWFEYOHDggdnZ28q9//UspKwojmzZtEm9vb6V8yJAhMmbMGBkzZky1CCM6nU7at28vnTp1EkdHR5k2bZry3ujRo8XKykovPJfm/jBy584dWb58uZiYmMjJkyeV8oftBx62Lzlw4IAYGxvLV199Jenp6XLo0CElLF27dk1CQkIkKipKWf937typgKVkuBoXRtq0aaP8mr59+7Y4ODjIrl27REQkJCREL8mLiAQHB+uFkfsVFhaKtbW1/PDDD0qZh4eHmJqaiqWlpZiZmSlfBkW/LEVE+vfvL126dNEb13vvvaccPTl+/LgAkH379invX758WczNzZXU3Lx5c5k8eXKJ7Sr6Erp3mmr797//LQBk06ZNeuX29vZiaWkplpaWMm7cOBG5+6U7duzYh45z5MiRSsoXEXFxcZFPPvlE+fv27dtSr169YmHE2NhYmSYAcXFx0TvCtWTJEqldu7beEYKtW7eKkZGRZGZmioiIq6trsSNrrVu3Vj5Do0aNko4dO+r9GrrX/b9y1fbbb7+VuH4eZv369WJvb6/8XdIvPpG7YcTExEQsLS3FwsJCAEijRo30folNnDhRGjdurLfMFi1aJFZWVlJYWCi5ubliYmIia9euVd4vKCgQV1dXZb337NlTL/jfq7Rf8VVJWcPI559/Lvb29nLz5k3l/aVLl5Z6ZKTI1q1bBYAyXFFIuNfHH38sAOTq1atKWVJSkrLNWFpaKt95RcvU3NxcLC0txcTERADIG2+8oTfOoukUFBSIo6Oj/O///q/k5uaKtbW1/P7779UmjIiIpKamCgBp3ry5XvDo2rWrtGjRQq/unDlz9JZb0Q/DoqNSReVGRkai1Wr1fpCWZT/wsH3Jxo0bxcbGRu8H871KOmKphhrVZyQtLQ1JSUl49dVXAQC1atVCeHg4li1bBgBITU1FcHCw3jD3PwAwKysLUVFRaNiwIWxtbWFjY4Pc3FycO3dOr957772HlJQU/PHHH0rHv+7du6OwsFCZVmhoqN4woaGhOHHiBAoLC5GamopatWrptcfe3h6NGzdGamoqAGD06NGYPn06QkNDERMT89g7YFaUpKQkpKSkoGnTpnoPUAwMDCxWd9GiRQgICEDdunVhZWWFJUuWKMs+OzsbGRkZesusVq1aJY6nQ4cOSElJQUpKCpKSkhAWFobnn39e6UyZmpoKPz8/WFpaKsOEhoZCp9MhLS0NOTk5uHDhQonrsGj9DB48GCkpKWjcuDFGjx6N7du3P8JSqnxSxpsx79ixA506dYKbmxusra0xcOBAXLlypUydj1977TWkpKTg999/x6+//gpvb28899xzuH79OoC7yz0kJAQajUYZJjQ0FLm5ufjrr79w6tQp3L59W2+5m5iYICgoSFnuw4cPxzfffAN/f3+MGzcO+/fvN2QxVBtpaWlo0aIFzMzMlLKgoKAS67Zo0UL5v4uLCwDg4sWLBk2vRYsWyjaTl5eHO3fu6L2/bt06Zd1+++23+O677zB+/Phi4zExMcGAAQOwYsUKrF+/Ho0aNdJrX3WwfPlyWFhY4MyZM8X6v9zv9ddfR0pKCr744gvk5eXpbWfW1tbKMk1OTsbMmTMxbNgw/PDDDwBQpv3Aw/YlXbp0gYeHBxo0aICBAwdi7dq1j+VCAUPVqDCybNky3LlzB66urqhVqxZq1aqFzz//HBs3bizWGas0ERERSElJwYIFC7B//36kpKTA3t4eBQUFevUcHBzg7e2Nhg0bomPHjpg/fz7279+PXbt2Vdj8DB06FKdPn8bAgQPx559/IjAwEJ999lmFjb+ieXt7Q6PRFOuc1aBBA3h7e8Pc3Fyv/N4gAADffPMN3n33XQwZMgTbt29HSkoKIiMjiy37srC0tIS3tze8vb3RunVrfPnll8jLy8PSpUsNn7FStGrVCmfOnMG0adNw8+ZN9OvXD3379q2w8Ve0hg0bQqPR4NixY6XWSU9PR48ePdCiRQts3LgRBw8exKJFiwCgTOvB1tZWWe6hoaFYtmwZTpw4gXXr1lXYfBSFyrfffhsXLlxAp06d8O6771bY+KsjExMT5f9FQU+n05Vav2HDhgCgt61qtVpl3ZXE3d0d3t7e8PHxwcsvv4yxY8dizpw5uHXrVrG6r7/+OtavX49Fixbh9ddfL9c8qWX//v2YN28efvzxRwQFBWHIkCFKwGjYsCFOnz6t1+Hezs4O3t7ecHNzKzYuIyMjZZm2aNEC0dHRaN++PWbNmlVh7bW2tsahQ4fw9ddfw8XFBZMmTYKfn1+Vu9KyxoSRO3fuYNWqVZgzZ46SRItSvKurK77++mv4+Pjgt99+0xvu3//+t97f+/btw+jRo9GtWzc0bdoUWq0Wly9ffuj0jY2NAQA3b94EAPj4+GDfvn3Fxt2oUSMYGxvDx8cHd+7c0WvPlStXkJaWBl9fX6XM3d0dw4YNw6ZNm/DOO+8oO1NTU1MAUI7EVAX29vbo0qULFi5ciLy8PIOH37dvH9q0aYMRI0agZcuW8Pb2xqlTp5T3bW1t4eLiorfM7ty5g4MHDz503BqNBkZGRnrr5/fff9dr5759+2BkZITGjRvDxsYGrq6uJa7De9ePjY0NwsPDsXTpUqxbtw4bN27E1atXAdzdQVSl9VOnTh2EhYVh0aJFJa6fa9eu4eDBg9DpdJgzZw6efvppNGrUqNgVaaampmWer5K2i8TERL1fj/v27YO1tTXq1asHLy8vmJqa6i3327dv48CBA3rLvW7duoiIiMCaNWswf/58LFmyRGkbULW2i/Jq3Lgx/vzzT72jiQcOHDB4PCWtr+eeew516tR5pJ2isbEx7ty5U2JIbdq0KZo2bYrDhw+jf//+5Z7G43bjxg0MHjwYw4cPR4cOHbBs2TIkJSUhLi4OAPDqq68iNzcXixcvLvc0jI2N9baHh+0HHrYvAe4eIe7cuTM++eQT/PHHH0hPT8fOnTsBGLa9Vip1zxI9Pps3bxZTU9MSO3KOGzdOAgMD5ZtvvhEzMzNZvny5pKWlyaRJk4p1YG3ZsqV06dJFjh49Kv/+97+lbdu2Ym5urtdh1cPDQ6ZOnSoZGRly4cIF+e2336Rdu3ZSt25duXz5soiIHDx4UK/T0cqVK4t1YO3Vq5f4+vrK3r17JSUlRbp27arXcWnMmDESHx8vp0+floMHD0pwcLD069dPRO52BNRoNLJy5Uq5ePGi3lUgajp58qQ4OTlJkyZN5JtvvpGjR4/KsWPHZPXq1eLk5CTR0dEiUnJ/igULFoiNjY3Ex8dLWlqafPjhh2JjY6O3fj7++GOpU6eObN68WVJTUyUqKqrEDqxdu3ZVOmwdPXpURowYIRqNRuk/lJeXJy4uLvLSSy/Jn3/+KTt37pQGDRrodWCdN2+e2NjYyDfffCPHjh2T999/X68D65w5c+Srr76S1NRUSUtLkyFDhoizs7PSSbZhw4YyfPhwycjI0Ds3r6ZTp06Js7Oz+Pr6yoYNG+T48eNy9OhRWbBggTRp0kRSUlIEgMyfP19OnTolq1atEjc3N73+Sfv27VP6KVy6dEny8vJE5O656Xs7yqWkpMhLL70kZmZmytUdRR1YR44cKampqbJly5ZiHVjHjBkjrq6u8tNPP+l1YC1ahh999JFs2bJFTpw4IYcPH5YePXpIUFCQiNztQ2Rubi7Tp0+XzMzMKtGx+34RERHSvn17SU5O1nudO3euxA6sgwYNkqNHj0p8fLw0adJEAChXd5TUdyw5OVnvqom1a9eKpaWlJCcny6VLl+TWrVsiIrJp0yYxMTGRbt26SXx8vJw6dUp+//13mTVrlgBQOgUX9Rkp6hR8/vx52bZtm7i5uel1Tr6/b0pubq5eu6pDn5HRo0eLt7e38pkWEYmLixMrKytleb7zzjtibGwsb7/9tuzdu1fS09MlMTFRBgwYIBqNRrKzs0Xkbp+Rezt6nz59Wr744gsxNjaWKVOmKON/2H7gYfuSH374QRYsWCDJycmSnp4uixcvFiMjIzl8+LCIiERFRUnr1q3lzJkzcunSJeX76XGrMWGkR48e0q1btxLfK+q49/vvv8uMGTPEwcFBrKysJCIiQsaNG6e3AR06dEgCAwPFzMxMGjZsKOvXry929cz9l23WrVtXunXrVqzTXNHlWCYmJvLUU0/J7Nmz9d4vuqTL1tZWzM3NJSwsTO+Srrfeeku8vLxEq9VK3bp1ZeDAgUrYERGZOnWqODs7i0ajqTKX9oqIXLhwQd566y2pX7++mJiYiJWVlQQFBcns2bOVjbykMHLr1i0ZPHiw2Nraip2dnQwfPlzGjx+vt35u374tY8aMERsbG7Gzs5Po6OgSL+29d/1YW1tL69atZcOGDXrTK8ulvZMnTxY3NzcxMTEpdmnvkiVLxN/fXywtLcXGxkY6deokhw4dUt7//vvvxdvbW2rVqlVlLu0Vubt+Ro4cqXTEdnNzkxdeeEEJanPnzhUXFxflM7lq1apiO7xhw4aJvb19sUt7713utWvXlnbt2snOnTv1pv+wS3tv3rwpo0aNEgcHhxIv7Z02bZr4+PiIubm51KlTR3r16iWnT59W3l+6dKm4u7uLkZFRldz5lXS5JQAZMmRIiZf2tmjRQkxNTSUgIEC++uorAaCEu7KEkVu3bslLL70kdnZ2yhVeRQ4cOCB9+/YVR0dHqVWrltjb20tYWJh88803xS7tLXoZGxtLvXr1JCoqSu8qsZI6yt6rqoeR3bt3i7Gxsezdu7fYe88995xeZ/V169ZJ+/btxdbWVkxMTKRevXrSv39/+fe//60Mc/9l1VqtVho1aiQzZszQu6LlYfsBkQfvS/bu3Svt2rWT2rVri7m5ubRo0ULvCsS0tDR5+umnxdzcXNVLezUiZey1RkREVdratWsRGRmJ7OzsYn2wiKqyWmo3gIiIymfVqlVo0KAB3Nzc8Pvvv+P9999Hv379GESo2mEYISKqpjIzMzFp0iRkZmbCxcUFL7/8MmbMmKF2s4gMxtM0REREpKoac2kvERERVU0MI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhV/we7R19mAchd7QAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Ionosphere scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(ionosphere_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Ionosphere'] = ionosphere_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "      <td>97.159847</td>\n",
              "      <td>86.347619</td>\n",
              "      <td>93.815873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>78.145238</td>\n",
              "      <td>90.854762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>87.076190</td>\n",
              "      <td>93.815079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>82.361905</td>\n",
              "      <td>92.849206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "      <td>96.792626</td>\n",
              "      <td>83.802381</td>\n",
              "      <td>92.960317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer      Sonar  Ionosphere\n",
              "0   AdaBoost  96.552288      97.159847  86.347619   93.815873\n",
              "1  GradBoost  98.075163      96.646633  78.145238   90.854762\n",
              "2   CatBoost  97.967320      97.378303  87.076190   93.815079\n",
              "3   LightGBM  97.120915      97.334612  82.361905   92.849206\n",
              "4    XGBoost  97.797386      96.792626  83.802381   92.960317"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Ionosphere'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Bupa**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "bupa_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Bupa\\Bupa.data', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(345, 7)"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bupa_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = bupa_df.iloc[:, :-1]\n",
        "y = bupa_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:11<00:00,  1.43s/trial, best loss: -0.8695652173913043]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 1150.0, 'learning_rate': 0.036566586849114326, 'max_depth': 6.0, 'max_features': 'sqrt', 'min_samples_leaf': 3.0, 'min_samples_split': 2.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:20<00:00,  2.49trial/s, best loss: -0.8115942028985508]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': None, 'n_estimators': 650, 'learning_rate': 0.06856648459048352, 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 0.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [01:06<00:00,  1.33s/trial, best loss: -0.7971014492753623]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 1000, 'learning_rate': 0.07885766008379519, 'min_child_samples': 8, 'max_depth': 2, 'reg_lambda': 2.215819236413667, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 44.96trial/s, best loss: -0.7391304347826086]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'goss', 'num_leaves': 50, 'learning_rate': 0.030743334125495195, 'min_child_samples': 20, 'reg_alpha': 1.2374175460929842, 'reg_lambda': 2.7904588669270254, 'colsample_by_tree': 0.6043264075687251, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:07<00:00,  6.92trial/s, best loss: -0.7971014492753623]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'dart', 'learning_rate': 0.04329402990235971, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 2, 'colsample_bytree': 0.8839036553414338, 'colsample_bylevel': 0.13572776354954574, 'colsample_bynode': 0.32883430164648214, 'reg_alpha': 0.43424116154739917, 'reg_lambda': 1.9753629991445285, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Bupa Dataset ---------\n",
            "[0.77142857 0.74285714 0.71428571 0.71428571 0.77142857 0.88235294\n",
            " 0.67647059 0.64705882 0.67647059 0.61764706 0.65714286 0.57142857\n",
            " 0.68571429 0.8        0.68571429 0.70588235 0.70588235 0.76470588\n",
            " 0.73529412 0.67647059 0.65714286 0.71428571 0.8        0.74285714\n",
            " 0.77142857 0.70588235 0.79411765 0.73529412 0.70588235 0.67647059\n",
            " 0.77142857 0.77142857 0.74285714 0.82857143 0.74285714 0.79411765\n",
            " 0.64705882 0.70588235 0.70588235 0.67647059 0.74285714 0.8\n",
            " 0.65714286 0.68571429 0.77142857 0.73529412 0.73529412 0.64705882\n",
            " 0.67647059 0.70588235 0.65714286 0.77142857 0.74285714 0.62857143\n",
            " 0.77142857 0.70588235 0.70588235 0.70588235 0.58823529 0.58823529\n",
            " 0.74285714 0.77142857 0.6        0.57142857 0.68571429 0.73529412\n",
            " 0.61764706 0.82352941 0.85294118 0.67647059 0.8        0.65714286\n",
            " 0.82857143 0.74285714 0.65714286 0.79411765 0.76470588 0.70588235\n",
            " 0.67647059 0.73529412 0.8        0.68571429 0.71428571 0.82857143\n",
            " 0.68571429 0.70588235 0.76470588 0.64705882 0.73529412 0.64705882\n",
            " 0.82857143 0.74285714 0.74285714 0.6        0.65714286 0.82352941\n",
            " 0.64705882 0.76470588 0.61764706 0.67647059]\n",
            "Accuracy: 71.67% (6.58%)\n",
            "Execution Time: 177.65 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Bupa Dataset ---------\n",
            "[0.88571429 0.71428571 0.71428571 0.71428571 0.74285714 0.82352941\n",
            " 0.67647059 0.70588235 0.67647059 0.67647059 0.6        0.68571429\n",
            " 0.65714286 0.77142857 0.68571429 0.67647059 0.76470588 0.61764706\n",
            " 0.64705882 0.79411765 0.62857143 0.68571429 0.71428571 0.71428571\n",
            " 0.74285714 0.79411765 0.82352941 0.61764706 0.67647059 0.73529412\n",
            " 0.71428571 0.8        0.71428571 0.8        0.77142857 0.64705882\n",
            " 0.64705882 0.70588235 0.70588235 0.58823529 0.68571429 0.71428571\n",
            " 0.74285714 0.68571429 0.74285714 0.76470588 0.64705882 0.61764706\n",
            " 0.64705882 0.70588235 0.68571429 0.8        0.65714286 0.65714286\n",
            " 0.77142857 0.67647059 0.67647059 0.70588235 0.73529412 0.64705882\n",
            " 0.62857143 0.74285714 0.62857143 0.62857143 0.6        0.70588235\n",
            " 0.44117647 0.91176471 0.70588235 0.70588235 0.82857143 0.65714286\n",
            " 0.68571429 0.71428571 0.68571429 0.73529412 0.73529412 0.61764706\n",
            " 0.61764706 0.70588235 0.71428571 0.71428571 0.65714286 0.8\n",
            " 0.77142857 0.64705882 0.55882353 0.61764706 0.64705882 0.61764706\n",
            " 0.68571429 0.74285714 0.8        0.48571429 0.6        0.79411765\n",
            " 0.73529412 0.64705882 0.67647059 0.76470588]\n",
            "Accuracy: 69.78% (7.34%)\n",
            "Execution Time: 53.80 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Bupa Dataset ---------\n",
            "[0.85714286 0.77142857 0.71428571 0.68571429 0.65714286 0.82352941\n",
            " 0.58823529 0.55882353 0.61764706 0.70588235 0.74285714 0.6\n",
            " 0.62857143 0.71428571 0.65714286 0.64705882 0.73529412 0.61764706\n",
            " 0.73529412 0.73529412 0.65714286 0.62857143 0.65714286 0.74285714\n",
            " 0.68571429 0.67647059 0.79411765 0.64705882 0.73529412 0.70588235\n",
            " 0.77142857 0.68571429 0.68571429 0.77142857 0.82857143 0.88235294\n",
            " 0.61764706 0.64705882 0.70588235 0.64705882 0.77142857 0.68571429\n",
            " 0.74285714 0.65714286 0.77142857 0.67647059 0.70588235 0.61764706\n",
            " 0.70588235 0.70588235 0.71428571 0.68571429 0.68571429 0.65714286\n",
            " 0.8        0.73529412 0.70588235 0.76470588 0.67647059 0.64705882\n",
            " 0.74285714 0.68571429 0.62857143 0.6        0.77142857 0.73529412\n",
            " 0.52941176 0.82352941 0.85294118 0.61764706 0.77142857 0.57142857\n",
            " 0.71428571 0.82857143 0.71428571 0.79411765 0.73529412 0.58823529\n",
            " 0.64705882 0.64705882 0.74285714 0.74285714 0.6        0.77142857\n",
            " 0.65714286 0.61764706 0.76470588 0.64705882 0.64705882 0.70588235\n",
            " 0.65714286 0.65714286 0.74285714 0.6        0.71428571 0.85294118\n",
            " 0.64705882 0.64705882 0.55882353 0.79411765]\n",
            "Accuracy: 69.85% (7.31%)\n",
            "Execution Time: 104.23 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Bupa Dataset ---------\n",
            "[0.8        0.74285714 0.65714286 0.65714286 0.6        0.73529412\n",
            " 0.55882353 0.73529412 0.58823529 0.76470588 0.6        0.68571429\n",
            " 0.6        0.8        0.77142857 0.73529412 0.79411765 0.64705882\n",
            " 0.73529412 0.73529412 0.6        0.54285714 0.68571429 0.68571429\n",
            " 0.82857143 0.82352941 0.85294118 0.55882353 0.76470588 0.67647059\n",
            " 0.71428571 0.68571429 0.71428571 0.82857143 0.71428571 0.76470588\n",
            " 0.52941176 0.73529412 0.79411765 0.64705882 0.62857143 0.77142857\n",
            " 0.71428571 0.68571429 0.77142857 0.73529412 0.70588235 0.55882353\n",
            " 0.58823529 0.70588235 0.65714286 0.77142857 0.65714286 0.54285714\n",
            " 0.82857143 0.79411765 0.82352941 0.76470588 0.70588235 0.64705882\n",
            " 0.6        0.8        0.71428571 0.74285714 0.68571429 0.58823529\n",
            " 0.61764706 0.85294118 0.67647059 0.61764706 0.68571429 0.68571429\n",
            " 0.68571429 0.71428571 0.74285714 0.79411765 0.70588235 0.58823529\n",
            " 0.58823529 0.79411765 0.74285714 0.74285714 0.8        0.8\n",
            " 0.62857143 0.70588235 0.58823529 0.76470588 0.58823529 0.64705882\n",
            " 0.62857143 0.68571429 0.74285714 0.51428571 0.71428571 0.73529412\n",
            " 0.70588235 0.67647059 0.76470588 0.58823529]\n",
            "Accuracy: 69.79% (8.14%)\n",
            "Execution Time: 1.90 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Bupa Dataset ---------\n",
            "[0.91428571 0.8        0.71428571 0.71428571 0.62857143 0.79411765\n",
            " 0.64705882 0.76470588 0.70588235 0.73529412 0.71428571 0.74285714\n",
            " 0.65714286 0.82857143 0.77142857 0.70588235 0.85294118 0.67647059\n",
            " 0.67647059 0.79411765 0.77142857 0.68571429 0.82857143 0.71428571\n",
            " 0.85714286 0.76470588 0.79411765 0.61764706 0.67647059 0.73529412\n",
            " 0.77142857 0.77142857 0.71428571 0.85714286 0.77142857 0.76470588\n",
            " 0.61764706 0.76470588 0.73529412 0.73529412 0.8        0.74285714\n",
            " 0.71428571 0.74285714 0.71428571 0.85294118 0.67647059 0.67647059\n",
            " 0.73529412 0.76470588 0.77142857 0.8        0.74285714 0.65714286\n",
            " 0.8        0.76470588 0.79411765 0.70588235 0.76470588 0.70588235\n",
            " 0.74285714 0.85714286 0.71428571 0.71428571 0.71428571 0.70588235\n",
            " 0.61764706 0.85294118 0.88235294 0.67647059 0.82857143 0.65714286\n",
            " 0.82857143 0.82857143 0.74285714 0.76470588 0.73529412 0.61764706\n",
            " 0.61764706 0.70588235 0.77142857 0.74285714 0.8        0.82857143\n",
            " 0.74285714 0.70588235 0.79411765 0.70588235 0.61764706 0.76470588\n",
            " 0.68571429 0.77142857 0.85714286 0.6        0.68571429 0.88235294\n",
            " 0.76470588 0.76470588 0.76470588 0.70588235]\n",
            "Accuracy: 74.48% (6.77%)\n",
            "Execution Time: 20.48 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "bupa_scores = []\n",
        "bupa_mean = []\n",
        "bupa_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    bupa_scores.append(results)\n",
        "    bupa_mean.append(results.mean()*100)\n",
        "    bupa_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Bupa Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAHNCAYAAADMjHveAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABYTUlEQVR4nO3deVgVZf8G8PtwhMMOKjuiKKjgBoob+pJaGubyhmZapqImmVsWlWn1uqZkLulrqGkupZamopUWVqg/KSkNpdIAV9IScAdBBeF8f3/4MnkElIMHh+X+XBeXnuc8M88zM2e5z8wzMxoRERARERGpxEztDhAREVHNxjBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQpWeRqPB9OnT1e5Giby9vdGnTx+1u1EtdO3aFV27dlUep6WlQaPRYO3atQb1YmNjERgYCEtLS2g0Gly9ehUAsG7dOvj5+cHc3ByOjo4Prd9E9OAYRqqAkydPYvTo0WjUqBEsLS1hb2+Pzp07Y/Hixbhx44ba3SMTun79OqZPn469e/eq3ZVK6dKlSxg4cCCsrKwQHR2NdevWwcbGBikpKRg+fDh8fHywcuVKrFixQu2uluqPP/7A9OnTkZaWVqb606dPh0ajUf7MzMzg7u6OPn364KeffqrYzhI9JLXU7gDd286dO/H0009Dp9Nh2LBhaNGiBfLz8/HDDz/g9ddfx9GjRyv1B68p3LhxA7Vq1YyX6vXr1zFjxgwAMNhLUBM1aNAAN27cgLm5uVJ28OBBXLt2DbNmzUL37t2V8r1790Kv12Px4sXw9fVVo7tl9scff2DGjBno2rUrvL29yzzdsmXLYGtrC71ej7Nnz2LlypV45JFHcODAAQQGBlZYf4kehprxCV9FnT59Gs888wwaNGiA3bt3w93dXXlu3LhxOHHiBHbu3KliDyuOXq9Hfn4+LC0tYWlpqXZ3SAUajabYtj9//jwAFDsMU1r5g8jNzYWNjY3J5vegBgwYACcnJ+VxWFgYWrRogc2bNzOM3IOI4ObNm7CyslK7K3QPPExTib333nvIycnBqlWrDIJIEV9fX0ycOFF5XFBQgFmzZsHHxwc6nQ7e3t548803kZeXZzBd0TiHvXv3om3btrCyskLLli2VQwMxMTFo2bIlLC0tERQUhMOHDxtMP3z4cNja2uLUqVMIDQ2FjY0NPDw8MHPmTNx9E+j58+ejU6dOqFu3LqysrBAUFIQtW7YUWxaNRoPx48djw4YNaN68OXQ6HWJjY5Xn7hwzcu3aNbz88svw9vaGTqeDi4sLevTogUOHDhnMc/PmzQgKCoKVlRWcnJwwZMgQ/P333yUuy99//42wsDDY2trC2dkZr732GgoLC0vZMsV9++23yjiGZs2aISYmplidq1ev4uWXX4aXlxd0Oh18fX0xd+5c6PV6ALfHSDg7OwMAZsyYoeyWnz59Or788ktoNBr89ttvyvy2bt0KjUaD/v37G7Tj7++PQYMGGZStX79eWRd16tTBM888g7Nnzxbr488//4yePXvCwcEB1tbW6NKlC3788UeDOkWHDU6cOIHhw4fD0dERDg4OGDFiBK5fv16m9bVixQr4+PjAysoK7du3R3x8fLE6d48Z6dq1K8LDwwEA7dq1g0ajwfDhw+Ht7Y1p06YBAJydnYu9Xr755huEhITAxsYGdnZ26N27N44ePWrQVtHr4OTJk+jVqxfs7Ozw3HPPAbgdjBctWoTmzZvD0tISrq6uGD16NK5cuWIwj6L31Q8//ID27dvD0tISjRo1wieffKLUWbt2LZ5++mkAQLdu3ZRtXJ7Dcm5ubgBgsNdw7dq10Gg0xQ4B7d27t1g7Xbt2RYsWLZCYmIhOnTrBysoKDRs2xPLlyw2mzc/Px9SpUxEUFAQHBwfY2NggJCQEe/bsKVM/f/nlF4SGhsLJyUlpY+TIkQZ1ivZqFX3uODs7o2fPnvjll1+UOsZ+vu3atUv5fPvwww8B3P89WGTjxo0ICgqCnZ0d7O3t0bJlSyxevLhMy0vlJFRpeXp6SqNGjcpcPzw8XADIgAEDJDo6WoYNGyYAJCwszKBegwYNpGnTpuLu7i7Tp0+X999/Xzw9PcXW1lbWr18v9evXl3fffVfeffddcXBwEF9fXyksLDRox9LSUho3bixDhw6VDz74QPr06SMA5D//+Y9BW/Xq1ZOxY8fKBx98IAsXLpT27dsLANmxY4dBPQDi7+8vzs7OMmPGDImOjpbDhw8rz02bNk2pO3jwYLGwsJDIyEj56KOPZO7cudK3b19Zv369UmfNmjUCQNq1ayfvv/++TJ48WaysrMTb21uuXLlSbFmaN28uI0eOlGXLlslTTz0lAGTp0qX3XecNGjSQJk2aiKOjo0yePFkWLlwoLVu2FDMzM/n222+Verm5udKqVSupW7euvPnmm7J8+XIZNmyYaDQamThxooiI5OTkyLJlywSA9OvXT9atWyfr1q2TX3/9VS5duiQajUaWLFmizHPixIliZmYmzs7OStn58+cFgHzwwQdK2TvvvCMajUYGDRokS5culRkzZoiTk1OxdREXFycWFhYSHBwsCxYskPfff19atWolFhYW8vPPPyv1pk2bJgCkdevW0r9/f1m6dKmMGjVKAMikSZPuu84++ugjASCdOnWS//73v/Lyyy+Lo6OjNGrUSLp06aLUO336tACQNWvWiIjIt99+Ky+88IIAkJkzZ8q6detk//79sm3bNunXr58AkGXLlinrTETkk08+EY1GIz179pQlS5bI3LlzxdvbWxwdHeX06dNKW+Hh4aLT6cTHx0fCw8Nl+fLl8sknn4iIyKhRo6RWrVoSEREhy5cvlzfeeENsbGykXbt2kp+fb/BaaNq0qbi6usqbb74pH3zwgbRp00Y0Go0cOXJEREROnjwpL730kgCQN998U9nGGRkZpa6vovWdmpoqFy5ckMzMTDl06JD069dPLC0tlXmL/PO6v3PZRET27NkjAGTPnj1KWZcuXcTDw0NcXFxk/Pjx8t///lf+9a9/CQBZtWqVUu/ChQvi7u4ukZGRsmzZMnnvvfekadOmYm5urrxHS5OZmSm1a9eWJk2ayLx582TlypXy1ltvib+/v0G94cOHCwB54oknZNGiRTJ//nx58sknDV7vxny++fr6Su3atWXy5MmyfPly2bNnT5negyK3X2cA5LHHHpPo6GiJjo6W8ePHy9NPP33PZaUHwzBSSWVlZQkAefLJJ8tUPykpSQDIqFGjDMpfe+01ASC7d+9Wyho0aCAAZP/+/UrZrl27BIBYWVnJn3/+qZR/+OGHxT7Eij4UJkyYoJTp9Xrp3bu3WFhYyIULF5Ty69evG/QnPz9fWrRoIY8++qhBOQAxMzOTo0ePFlu2u8OIg4ODjBs3rtR1kZ+fLy4uLtKiRQu5ceOGUr5jxw4BIFOnTi22LDNnzjSYR+vWrSUoKKjUNooUrcutW7cqZVlZWeLu7i6tW7dWymbNmiU2NjZy7Ngxg+knT54sWq1Wzpw5IyK3P/jvXt4izZs3l4EDByqP27RpI08//bQAkOTkZBERiYmJEQDKl3FaWppotVqZPXu2wbx+//13qVWrllKu1+ulcePGEhoaKnq9Xql3/fp1adiwofTo0UMpK/pyHDlypME8+/XrJ3Xr1r3n+iraNoGBgZKXl6eUr1ixQgDcM4yI/PNle/DgQYP5FvXpztfetWvXxNHRUSIiIgzqZmRkiIODg0F50etg8uTJBnXj4+MFgGzYsMGgPDY2tlh50Wth3759Stn58+dFp9PJq6++qpRt3ry52HvqXoqW7e4/R0dHiY2NNahrbBgBIAsWLFDK8vLyJDAwUFxcXJSgVVBQYLCtRESuXLkirq6uxV4Dd9u2bVuJ2+tOu3fvFgDy0ksvFXuu6LVYns+3u9dNWd+DEydOFHt7eykoKLjnspFp8TBNJZWdnQ0AsLOzK1P9r7/+GgAQGRlpUP7qq68CQLGxJc2aNUNwcLDyuEOHDgCARx99FPXr1y9WfurUqWJtjh8/Xvl/0WGW/Px8fP/990r5ncdpr1y5gqysLISEhBQ7pAIAXbp0QbNmze6zpLfHBfz88884d+5cic//8ssvOH/+PMaOHWsw5qB3797w8/MrcZzNiy++aPA4JCSkxGUuiYeHB/r166c8tre3x7Bhw3D48GFkZGQAuH3IKCQkBLVr18bFixeVv+7du6OwsBD79u27bzshISHK4Yxr167h119/xQsvvAAnJyelPD4+Ho6OjmjRogWA24fc9Ho9Bg4caNCum5sbGjdurOxqT0pKwvHjxzF48GBcunRJqZebm4vHHnsM+/btK7Yru6R1dunSJeW1W5KibfPiiy/CwsJCKR8+fDgcHBzuuw6M8d133+Hq1at49tlnDZZdq9WiQ4cOJR5mGDNmjMHjzZs3w8HBAT169DCYR1BQEGxtbYvNo1mzZggJCVEeOzs7o2nTpmV+Ld3L1q1b8d133+Hbb7/FmjVr0KRJEzz11FPYv39/uedZq1YtjB49WnlsYWGB0aNH4/z580hMTAQAaLVaZVvp9XpcvnwZBQUFaNu2bYnv4zsVjeHZsWMHbt26VepyaTQa5VDbnTQaDQDjP98aNmyI0NBQg7KyvgcdHR2Rm5uL77777p7LRqbFAayVlL29PYDbXzpl8eeff8LMzKzYmQRubm5wdHTEn3/+aVB+Z+AAoHwReHl5lVh+9/FxMzMzNGrUyKCsSZMmAGBwvHrHjh145513kJSUZHBst+hD5k4NGzYsdfnu9N577yE8PBxeXl4ICgpCr169MGzYMKU/RcvatGnTYtP6+fnhhx9+MCgrOkZ9p9q1axdb5tL4+voWW54714WbmxuOHz+O3377rVg7RYoGYN5LSEgIli9fjhMnTuDkyZPQaDQIDg5WQkpERATi4+PRuXNnmJnd/p1x/PhxiAgaN25c4jyLzlQ5fvw4AChjMkqSlZWF2rVrK4/vfg0VPXflyhXl9Xu3om1zd3/Mzc2LvZ4eVNEyPfrooyU+f3cfa9WqhXr16hWbR1ZWFlxcXEqcx93b7e51Ahj3WrqXRx55xGAA64ABA9C4cWNMmDBBCQ7G8vDwKDZI987XbseOHQEAH3/8MRYsWICUlBSDUHG/92yXLl3w1FNPYcaMGXj//ffRtWtXhIWFYfDgwdDpdABuX7rAw8MDderUKXU+xn6+ldSvsr4Hx44di88//xxPPPEEPD098fjjj2PgwIHo2bPnPZeVHgzDSCVlb28PDw8PHDlyxKjpSvqSL4lWqzWqXO4amFoW8fHx+Pe//41HHnkES5cuhbu7O8zNzbFmzRp8+umnxeqXdbT7wIEDERISgm3btuHbb7/FvHnzMHfuXMTExOCJJ54wup+lLbMp6fV69OjRA5MmTSrx+aIvgHv517/+BQDYt28fTp06hTZt2iiDCf/73/8iJycHhw8fxuzZsw3a1Wg0+Oabb0pcTltbW6UeAMybN6/UMzOK6hYx5WulIhQt07p165TBnne6+3RxnU6nhLg75+Hi4oINGzaU2MbdX2wPc53Y2tqiQ4cO+OKLL5Qzf0p7/xszGPtu69evx/DhwxEWFobXX38dLi4u0Gq1iIqKwsmTJ+85rUajwZYtW/DTTz/hq6++wq5duzBy5EgsWLAAP/30U7HX1P2U9fOtpM+Ssr4HXVxckJSUhF27duGbb77BN998gzVr1mDYsGH4+OOPjeovlR3DSCXWp08frFixAgkJCQaHVErSoEED6PV6HD9+HP7+/kp5ZmYmrl69igYNGpi0b3q9HqdOnTL4Ej127BgAKNdO2Lp1KywtLbFr1y7lVxAArFmz5oHbd3d3x9ixYzF27FicP38ebdq0wezZs/HEE08oy5qamlrsV3FqaqrJ18WJEycgIgYflHevCx8fH+Tk5BhcG6Mk9/qwrV+/PurXr4/4+HicOnVKORzwyCOPIDIyEps3b0ZhYSEeeeQRZRofHx+ICBo2bHjPwOPj4wPgdgi+Xx8fRNG6P378uMG2uXXrFk6fPo2AgACTtVW0TC4uLuVeJh8fH3z//ffo3LmzyU4NLesXalkUFBQAAHJycmBjY6PsnSq6Km2Ru/ccFDl37lyxU5jvfu1u2bIFjRo1QkxMjEHfSzqsUpqOHTuiY8eOmD17Nj799FM899xz2LhxI0aNGgUfHx/s2rULly9fLnXviCk+38r6HgRuH67q27cv+vbtC71ej7Fjx+LDDz/Ef/7zn0p/HZuqimNGKrFJkybBxsYGo0aNQmZmZrHnT548qZxu1qtXLwDAokWLDOosXLgQwO3xEqb2wQcfKP8XEXzwwQcwNzfHY489BuD2r0SNRmPwqywtLQ3bt28vd5uFhYXIysoyKHNxcYGHh4dyGKht27ZwcXHB8uXLDQ4NffPNN0hOTjb5ujh37hy2bdumPM7OzsYnn3yCwMBA5Rf5wIEDkZCQgF27dhWb/urVq8qXirW1tVJWkpCQEOzevRsHDhxQwkhgYCDs7Ozw7rvvKqdPF+nfvz+0Wi1mzJhR7Ne5iODSpUsAgKCgIPj4+GD+/PnIyckp1u6FCxfKujruqW3btnB2dsby5cuRn5+vlK9du7bUZS6v0NBQ2NvbY86cOSWOVyjLMg0cOBCFhYWYNWtWsecKCgrK1eeiL/4HXd7Lly9j//79cHNzUw4jFQWwO8cgFRYWlnphxIKCAuW0V+D2abwffvghnJ2dlddR0d6eO18/P//8MxISEu7bxytXrhR73RXteSt6bz711FMQEeVif3cqmtYUn29lfQ8WvSeKmJmZoVWrVgZ9JtPjnpFKzMfHB59++ikGDRoEf39/gyuw7t+/H5s3b8bw4cMBAAEBAQgPD8eKFStw9epVdOnSBQcOHMDHH3+MsLAwdOvWzaR9s7S0RGxsLMLDw9GhQwd888032LlzJ958801l13Xv3r2xcOFC9OzZE4MHD8b58+cRHR0NX19fg+tlGOPatWuoV68eBgwYgICAANja2uL777/HwYMHsWDBAgC3xx/MnTsXI0aMQJcuXfDss88iMzMTixcvhre3N1555RWTrQfg9u7d559/HgcPHoSrqytWr16NzMxMgz1Ar7/+Or788kv06dMHw4cPR1BQEHJzc/H7779jy5YtSEtLU67D0KxZM2zatAlNmjRBnTp10KJFC2VAakhICDZs2ACNRqMcttFqtejUqRN27dqFrl27GgwM9fHxwTvvvIMpU6YgLS0NYWFhsLOzw+nTp7Ft2za88MILeO2112BmZoaPPvoITzzxBJo3b44RI0bA09MTf//9N/bs2QN7e3t89dVXD7yuzM3N8c4772D06NF49NFHMWjQIJw+fRpr1qwx+ZgRe3t7LFu2DEOHDkWbNm3wzDPPwNnZGWfOnMHOnTvRuXNng0Bdki5dumD06NGIiopCUlISHn/8cZibm+P48ePYvHkzFi9ejAEDBhjVr8DAQGi1WsydOxdZWVnQ6XR49NFHSx2XUmTLli2wtbWFiODcuXNYtWoVrly5guXLlyt7LJo3b46OHTtiypQpyp6GjRs3Kl+0d/Pw8MDcuXORlpaGJk2aYNOmTUhKSsKKFSuU8UR9+vRBTEwM+vXrh969e+P06dNYvnw5mjVrVmJwvdPHH3+MpUuXol+/fvDx8cG1a9ewcuVK2NvbKwGjW7duGDp0KP773//i+PHj6NmzJ/R6PeLj49GtWzeMHz/eJJ9vZX0Pjho1CpcvX8ajjz6KevXq4c8//8SSJUsQGBhosFeGTEyNU3jIOMeOHZOIiAjx9vYWCwsLsbOzk86dO8uSJUvk5s2bSr1bt27JjBkzpGHDhmJubi5eXl4yZcoUgzoit0996927d7F2ABQ7Zbbo9Mp58+YpZeHh4WJjYyMnT56Uxx9/XKytrcXV1VWmTZtmcD0SEZFVq1ZJ48aNRafTiZ+fn6xZs0Y5VfF+bd/5XNGprnl5efL6669LQECA2NnZiY2NjQQEBJR4TZBNmzZJ69atRafTSZ06deS5556Tv/76y6BO0bLcraQ+lqRoXe7atUtatWqlLOfmzZuL1b127ZpMmTJFfH19xcLCQpycnKRTp04yf/58g+tV7N+/X4KCgsTCwqLYab5Hjx5Vrslyp3feeafE67wU2bp1q/zrX/8SGxsbsbGxET8/Pxk3bpykpqYa1Dt8+LD0799f6tatKzqdTho0aCADBw6UuLi4YuvmztNoRUo/rbQkS5culYYNG4pOp5O2bdvKvn37pEuXLiY9tbfInj17JDQ0VBwcHMTS0lJ8fHxk+PDh8ssvvyh1SnsdFFmxYoUEBQWJlZWV2NnZScuWLWXSpEly7tw5pU5p76u7l0tEZOXKldKoUSPRarX3Pc23pFN7bWxsJDg4WD7//PNi9U+ePCndu3cXnU6nXPPku+++K/HU3ubNm8svv/wiwcHBYmlpKQ0aNDC4Ro3I7dNr58yZIw0aNBCdTietW7eWHTt2SHh4uDRo0KDUfouIHDp0SJ599lmpX7++6HQ6cXFxkT59+hise5Hbpw/PmzdP/Pz8xMLCQpydneWJJ56QxMREpc6Dfr6JlO09uGXLFnn88cfFxcVFLCwspH79+jJ69GhJT0+/57LSg9GIVJLRZlRlDB8+HFu2bLnvryIiqry6du2KixcvGj1InqgicMwIERERqYphhIiIiFTFMEJERESq4pgRIiIiUhX3jBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUZHUb27duHvn37wsPDAxqNBtu3b7/vNHv37kWbNm2g0+ng6+uLtWvXlqOrREREVB0ZHUZyc3MREBCA6OjoMtU/ffo0evfujW7duiEpKQkvv/wyRo0ahV27dhndWSIiIqp+NCIi5Z5Yo8G2bdsQFhZWap033ngDO3fuxJEjR5SyZ555BlevXkVsbGx5myYiIqJqosLHjCQkJKB79+4GZaGhoUhISKjopomIiKgKqFXRDWRkZMDV1dWgzNXVFdnZ2bhx4wasrKyKTZOXl4e8vDzlsV6vx+XLl1G3bl1oNJqK7jIRERGZgIjg2rVr8PDwgJlZ6fs/KjyMlEdUVBRmzJihdjeIiIjIBM6ePYt69eqV+nyFhxE3NzdkZmYalGVmZsLe3r7EvSIAMGXKFERGRiqPs7KyUL9+fZw9exb29vYV2l8iIiIyjezsbHh5ecHOzu6e9So8jAQHB+Prr782KPvuu+8QHBxc6jQ6nQ46na5Yub29PcMIERFRFXO/IRZGD2DNyclBUlISkpKSANw+dTcpKQlnzpwBcHuvxrBhw5T6L774Ik6dOoVJkyYhJSUFS5cuxeeff45XXnnF2KaJiIioGjI6jPzyyy9o3bo1WrduDQCIjIxE69atMXXqVABAenq6EkwAoGHDhti5cye+++47BAQEYMGCBfjoo48QGhpqokUgIiKiquyBrjPysGRnZ8PBwQFZWVk8TENERFRFlPX7m/emISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFW11O4A0cNWWFiI+Ph4pKenw93dHSEhIdBqtWp3i4ioxuKeEapRYmJi4Ovri27dumHw4MHo1q0bfH19ERMTo3bXiIhqLIYRqjFiYmIwYMAAtGzZEgkJCbh27RoSEhLQsmVLDBgwgIGEiEglGhERtTtxP9nZ2XBwcEBWVhbs7e3V7g5VQYWFhfD19UXLli2xfft2mJn9k8P1ej3CwsJw5MgRHD9+nIdsiMjkrl+/jpSUlDLXv3HjBtLS0uDt7Q0rKyuj2vLz84O1tbWxXawQZf3+5pgRqhHi4+ORlpaGzz77zCCIAICZmRmmTJmCTp06IT4+Hl27dlWnk0RUbaWkpCAoKOihtJWYmIg2bdo8lLZMhWGEaoT09HQAQIsWLUp8vqi8qB4RkSn5+fkhMTGxzPWTk5MxZMgQrF+/Hv7+/ka3VdUwjFCN4O7uDgA4cuQIOnbsWOz5I0eOGNQjIjIla2vrcu2t8Pf3r3J7OcqjXANYo6Oj4e3tDUtLS3To0AEHDhwote6tW7cwc+ZM+Pj4wNLSEgEBAYiNjS13h4nKIyQkBN7e3pgzZw70er3Bc3q9HlFRUWjYsCFCQkJU6iERUc1ldBjZtGkTIiMjMW3aNBw6dAgBAQEIDQ3F+fPnS6z/9ttv48MPP8SSJUvwxx9/4MUXX0S/fv1w+PDhB+48UVlptVosWLAAO3bsQFhYmMHZNGFhYdixYwfmz5/PwatERCow+myaDh06oF27dvjggw8A3P5V6eXlhQkTJmDy5MnF6nt4eOCtt97CuHHjlLKnnnoKVlZWWL9+fZna5Nk0ZCoxMTF49dVXkZaWppQ1bNgQ8+fPR//+/dXrGBHRHQ4dOoSgoKAqORj1ThVyNk1+fj4SExMxZcoUpczMzAzdu3dHQkJCidPk5eXB0tLSoMzKygo//PBDqe3k5eUhLy9PeZydnW1MN4lK1b9/fzz55JO8AisRUSViVBi5ePEiCgsL4erqalDu6upa6vnToaGhWLhwIR555BH4+PggLi4OMTExKCwsLLWdqKgozJgxw5iuEZWZVqvl6btERJVIhV+BdfHixWjcuDH8/PxgYWGB8ePHY8SIEcWu9XCnKVOmICsrS/k7e/ZsRXeTiIiIVGJUGHFycoJWq0VmZqZBeWZmJtzc3EqcxtnZGdu3b0dubi7+/PNPpKSkwNbWFo0aNSq1HZ1OB3t7e4M/IiIiqp6MCiMWFhYICgpCXFycUqbX6xEXF4fg4OB7TmtpaQlPT08UFBRg69atePLJJ8vXYyIiIqpWjL7oWWRkJMLDw9G2bVu0b98eixYtQm5uLkaMGAEAGDZsGDw9PREVFQUA+Pnnn/H3338jMDAQf//9N6ZPnw69Xo9JkyaZdkmIiIioSjI6jAwaNAgXLlzA1KlTkZGRgcDAQMTGxiqDWs+cOWMwHuTmzZt4++23cerUKdja2qJXr15Yt24dHB0dTbYQREREVHXxrr1ERESVDK8zQlTFGHtrbqD8t+euTLfmJiKqLhhGqMrjrbmJiKo2hpFyMvbXeHl/iQP8NX4/xt6aGyj/7bmr4q25HybupSKi8mAYKSf+Gq88yntrbqDm3J77YeH7gojKg2GknIz9NV7eX+JFbRFVBdxLRUTlwTBSTuX9Nc5f4lSdcS8VEZVHhd+bhoiIiOheGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQq3puGiKiaun79OlJSUspc/8aNG0hLS4O3tzesrKzKPJ2fnx+sra3L00UiAAwjRETVVkpKCoKCgiq8ncTERN7kkB4IwwgRUTXl5+eHxMTEMtdPTk7GkCFDsH79evj7+xvVDtGDYBghIqqmrK2ty7XHwt/fn3s66KHiAFYiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVlSuMREdHw9vbG5aWlujQoQMOHDhwz/qLFi1C06ZNYWVlBS8vL7zyyiu4efNmuTpMRERE1YvRYWTTpk2IjIzEtGnTcOjQIQQEBCA0NBTnz58vsf6nn36KyZMnY9q0aUhOTsaqVauwadMmvPnmmw/ceSIiIqr6jA4jCxcuREREBEaMGIFmzZph+fLlsLa2xurVq0usv3//fnTu3BmDBw+Gt7c3Hn/8cTz77LP33ZtCRERENYNRYSQ/Px+JiYno3r37PzMwM0P37t2RkJBQ4jSdOnVCYmKiEj5OnTqFr7/+Gr169Sq1nby8PGRnZxv8ERERUfVUy5jKFy9eRGFhIVxdXQ3KXV1dkZKSUuI0gwcPxsWLF/Gvf/0LIoKCggK8+OKL9zxMExUVhRkzZhjTNSIiIqqiKvxsmr1792LOnDlYunQpDh06hJiYGOzcuROzZs0qdZopU6YgKytL+Tt79mxFd5OIiIhUYtSeEScnJ2i1WmRmZhqUZ2Zmws3NrcRp/vOf/2Do0KEYNWoUAKBly5bIzc3FCy+8gLfeegtmZsXzkE6ng06nM6ZrRERED83x48dx7dq1Cpt/cnKywb8Vxc7ODo0bN67QNsrCqDBiYWGBoKAgxMXFISwsDACg1+sRFxeH8ePHlzjN9evXiwUOrVYLABCRcnSZiIhIPcePH0eTJk0eSltDhgyp8DaOHTumeiAxKowAQGRkJMLDw9G2bVu0b98eixYtQm5uLkaMGAEAGDZsGDw9PREVFQUA6Nu3LxYuXIjWrVujQ4cOOHHiBP7zn/+gb9++SighIiKqKor2iKxfvx7+/v4V0saNGzeQlpYGb29vWFlZVUgbycnJGDJkSIXu4Skro8PIoEGDcOHCBUydOhUZGRkIDAxEbGysMqj1zJkzBntC3n77bWg0Grz99tv4+++/4ezsjL59+2L27NmmWwoiIqKHzN/fH23atKmw+Xfu3LnC5l3ZGB1GAGD8+PGlHpbZu3evYQO1amHatGmYNm1aeZoiIiKiao73piEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkarKdTn46qoibwld024HTUREVFYMI//zsG4JXVNuB01EpleRP5gA/mgi9TCM/E9F3xK6pt0OmohM62H9YAL4o4kePoaRu1TkLaFr0u2gici0KvoHE8AfTaQehhEioiqkIn8wAfzRROrg2TRERESkKoYRIiIiUhXDCBEREamKYYSIiKiSSTiXgCe3P4mEcwlqd+WhYBghIiKqREQEiw8txqmsU1h8aDFERO0uVTiGESIiokpk/7n9OHrpKADg6KWj2H9uv8o9qngMI0RERJWEiGDJ4SUw09z+ejbTmGHJ4SXVfu8IrzNClVJ1uOx1dbnkdXXYFkD12R6kPk3BTbR2M4PV1WPAOdP+pt9/8TdlrwgA6EV/e+/I7+vQ2amVSduyunoMrd3MoCm4adL5lgfDCFU61emy11X9ktfVaVsAVX97UOVgmXMGh0bbAvtGA/tMN18BsMTDFWYWFtBrNEq5mQiW/PQOOp3LhKb0yY3mD+DQaFsk55wB0MmEczYewwhVOtXhstfV5ZLX1WFbANVne1DlcNO2Ptp8mIMNGzbA38/PZPPdf/E3HD08r1i5XqPBUZ0O+/svMenekeSUFDz33HNY1au+yeZZXgwjVGnxsteVB7cF0T+kliUOZ+hxw7EJ4BFomnmKYMmhd6GBBoLi40M00GDJma/RqeVQaDSm2T9yI0OPwxl6SC1Lk8zvQXAAKxERkcpu6W8hIzejxCACAAJBRm4GbulvPeSePRzcM0JERKQyC60FNvbZiMs3L5dap45lHVhoLR5irx4ehhEiIqJKwM3GDW42bmp3QxU8TENERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCMPSU27HTQREVFZMYw8BDXxdtBERERlxTDyENTE20ETERGVFcNIBaupt4MmIiIqK4aRCla0V0QvegB33A6ae0eIiIgAMIxUqLv3ihTh3hEiIqJ/MIxUoLv3ihTh3hEiIqJ/8N40/6MpuInWbmawunoMOPfgGU1EsOTA3HvfDvrAXHRqP8Nkt4O2unoMrd3MoCm4aZL5ERERPQwMI/9jmXMGh0bbAvtGA/sefH63AGR4eUJqaUt8XiDIuHwCt1Z2hanuwegP4NBoWyTnnAHQyURzJSIiqljlCiPR0dGYN28eMjIyEBAQgCVLlqB9+/Yl1u3atSv+7//+r1h5r169sHPnzvI0XyFu2tZHmw9zsGHDBvj7+T3w/CwAbLx5CZfzr5Vap46FPSws6zxwW0WSU1Lw3HPPYVWv+iabpxpMvZdKDdxLRaZWHd4XQPV4b1y/fh0AcOjQoQpr48aNG0hLS4O3tzesrKwqpI3k5OQKmW95GB1GNm3ahMjISCxfvhwdOnTAokWLEBoaitTUVLi4uBSrHxMTg/z8fOXxpUuXEBAQgKeffvrBem5iUssShzP0uOHYBPAINMk83f7397DcyNDjcIYeUsvyIbZqeqbeS1WSBEsd3q1bG5MvXUHwzTyTz597qcou4VwC3j3wLia3n4xgj2C1u1NpVYf3BVA93hspKSkAgIiICJV7Yhp2dnZqd8H4MLJw4UJERERgxIgRAIDly5dj586dWL16NSZPnlysfp06hr/8N27cCGtr60oXRqjyMPVeqruJCBYfmIZT2aexuGlHdDThuJ0i1WUvVUW7++rEHd07mnxbVBfV4X0BVI/3RlhYGADAz88P1tbWFdJGcnIyhgwZgvXr18Pf379C2gBuB5HGjRtX2PzLyqgwkp+fj8TEREyZMkUpMzMzQ/fu3ZGQULZ7rqxatQrPPPMMbGxsSq2Tl5eHvLx/Unl2drYx3aQqriL2Ut1p/98/4mj2aQDA0ezT2I/r6OzR2aRtVJe9VBWtpKsTd/Y07baoLqrD+wKoHu8NJycnjBo16qG05e/vjzZt2jyUttRk1IHHixcvorCwEK6urgblrq6uyMjIuO/0Bw4cwJEjR+67EaOiouDg4KD8eXl5GdNNolLxiriVB7dF5cFtQWp7qGfTrFq1Ci1btix1sGuRKVOmIDIyUnmcnZ3NQEImcecvccDwmi/8RV5cRQ6a3H/xt5K3xe/r0NmplUnbqg6DJisS3xekNqPCiJOTE7RaLTIzMw3KMzMz4eZ276Gaubm52LhxI2bOnHnfdnQ6HXQ6nTFdI7qvO3/93XkhuqJfgZ08OnG8wl0qatCkAFji4QozCwvo71jnZiJY8tM76HQuE6bcEtVh0GRF4fuCKgOjwoiFhQWCgoIQFxenDODR6/WIi4vD+PHj7znt5s2bkZeXhyFDhpS7s0QP4u5ff0X4K7B0FTVocv/F33D08Lxi5XqNBkd1Ouzvv8Ske0eqw6DJisL3BVUGRh+miYyMRHh4ONq2bYv27dtj0aJFyM3NVc6uGTZsGDw9PREVFWUw3apVqxAWFoa6deuapudERij69XfPK+LyV2AxFTFoUkSw5NC7994WZ75Gp5ZDTbYtqsOgyYrA9wVVFkaHkUGDBuHChQuYOnUqMjIyEBgYiNjYWGVQ65kzZ2BmZnhsOTU1FT/88AO+/fZb0/SayEi39LeQkZtR4gcu8L8r4uZm4Jb+Fiy0promLpWE26Ly4LagyqJcA1jHjx9f6mGZvXv3Fitr2rQpR2WTqiy0FtjYZyMu37xcap06lnX4gfsQcFtUHtwWVFnw3jRUY7jZuMHN5mFeE5dKw21ReXBbUGVQdW9wQERERNUCwwgRERGpimGEiIiIVMUwQkRERKriANb/uX79OgDg0KFDFTL/GzduIC0tDd7e3rCysqqQNpKTkytkvkRERBWJYeR/UlJSAAAREREq9+TB2dnZqd0FIiKiMmMY+Z+iy9v7+fnB2tra5PNPTk7GkCFDsH79evj7+5t8/kXs7OzQuHHjCps/ERGRqTGM/I+TkxNGjRpV4e34+/ujTZs2Fd4OERFRVcEBrERERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamK96ahSuf69esAgEOHDlVYGzdu3EBaWhq8vb1hZWVl8vknJyebfJ5qqA7bAqg+24OoumIYoUonJSUFABAREaFyTx6cnZ2d2l14INVpWwBVf3sQVVcMI1TphIWFAQD8/PxgbW1dIW0kJydjyJAhWL9+Pfz9/SukDTs7OzRu3LhC5v2wVJdtAVT97cG9VFSdMYxQpePk5IRRo0Y9lLb8/f3Rpk2bh9JWVcRtUXlwLxVVZwwjRERVAPdSUXXGMEJEVAVwLxVVZzy1l4iIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpqlxhJDo6Gt7e3rC0tESHDh1w4MCBe9a/evUqxo0bB3d3d+h0OjRp0gRff/11uTpMRERE1UstYyfYtGkTIiMjsXz5cnTo0AGLFi1CaGgoUlNT4eLiUqx+fn4+evToARcXF2zZsgWenp74888/4ejoaIr+ExERURVndBhZuHAhIiIiMGLECADA8uXLsXPnTqxevRqTJ08uVn/16tW4fPky9u/fD3NzcwCAt7f3g/WaiIiIqg2jwkh+fj4SExMxZcoUpczMzAzdu3dHQkJCidN8+eWXCA4Oxrhx4/DFF1/A2dkZgwcPxhtvvAGtVlviNHl5ecjLy1MeZ2dnG9NNIiKiSuX69etISUkpc/3k5GSDf43h5+cHa2tro6dTk1Fh5OLFiygsLISrq6tBuaura6kr+dSpU9i9ezeee+45fP311zhx4gTGjh2LW7duYdq0aSVOExUVhRkzZhjTNSIiokorJSUFQUFBRk83ZMgQo6dJTExEmzZtjJ5OTUYfpjGWXq+Hi4sLVqxYAa1Wi6CgIPz999+YN29eqWFkypQpiIyMVB5nZ2fDy8urortKRERUIfz8/JCYmFjm+jdu3EBaWhq8vb1hZWVldFtVjVFhxMnJCVqtFpmZmQblmZmZcHNzK3Ead3d3mJubGxyS8ff3R0ZGBvLz82FhYVFsGp1OB51OZ0zXiIiIKi1ra2uj91Z07ty5gnpT+Rh1aq+FhQWCgoIQFxenlOn1esTFxSE4OLjEaTp37owTJ05Ar9crZceOHYO7u3uJQYSIiIhqFqOvMxIZGYmVK1fi448/RnJyMsaMGYPc3Fzl7Jphw4YZDHAdM2YMLl++jIkTJ+LYsWPYuXMn5syZg3HjxpluKYiIiKjKMnrMyKBBg3DhwgVMnToVGRkZCAwMRGxsrDKo9cyZMzAz+yfjeHl5YdeuXXjllVfQqlUreHp6YuLEiXjjjTdMtxRERERUZZVrAOv48eMxfvz4Ep/bu3dvsbLg4GD89NNP5WmKiIiIqjnem4aIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlJVhd8or7ri7aCJiIhMg2GknHg7aCIiItNgGCkn3g6aiIjINBhGyom3gyYiIjINDmAlIiIiVTGMEBERkaoYRoiIiEhVHDNCVZ6xp1kD5T/VmqdZExGZHsMIVXnlPc0aMP5Ua55mTURkegwjVOUZe5o1UP5TrXmaNRGR6TGMUJVXntOsAZ5qTURUWXAAKxEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKrivWmIyGSuX7+OlJQUo6ZJTk42+Les/Pz8YG1tbdQ0RFQ5MYwQkcmkpKQgKCioXNMOGTLEqPqJiYnlukEiEVU+DCNEZDJ+fn5ITEw0apobN24gLS0N3t7esLKyMqotIqoeGEaIyGSsra3Ltbeic+fOFdAbIqoqOICViIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqjiAlYiomjL2ui+85guphWGEiKiaKu91X3jNF3rYGEaIiKopY6/7wmu+kFo0IiJqd+J+srOz4eDggKysLNjb26vdHSIiIiqDsn5/l2sAa3R0NLy9vWFpaYkOHTrgwIEDpdZdu3YtNBqNwZ+lpWV5miUiIqJqyOgwsmnTJkRGRmLatGk4dOgQAgICEBoaivPnz5c6jb29PdLT05W/P//884E6TURERNWH0WFk4cKFiIiIwIgRI9CsWTMsX74c1tbWWL16danTaDQauLm5KX+urq4P1GkiIiKqPowKI/n5+UhMTET37t3/mYGZGbp3746EhIRSp8vJyUGDBg3g5eWFJ598EkePHi1/j4mIiKhaMSqMXLx4EYWFhcX2bLi6uiIjI6PEaZo2bYrVq1fjiy++wPr166HX69GpUyf89ddfpbaTl5eH7Oxsgz8iIiKqnir8CqzBwcEYNmwYAgMD0aVLF8TExMDZ2RkffvhhqdNERUXBwcFB+fPy8qrobhIREZFKjAojTk5O0Gq1yMzMNCjPzMyEm5tbmeZhbm6O1q1b48SJE6XWmTJlCrKyspS/s2fPGtNNIiIiqkKMCiMWFhYICgpCXFycUqbX6xEXF4fg4OAyzaOwsBC///473N3dS62j0+lgb29v8EdkKoWFhdi7dy8+++wz7N27F4WFhWp3iYioRjP6CqyRkZEIDw9H27Zt0b59eyxatAi5ubkYMWIEAGDYsGHw9PREVFQUAGDmzJno2LEjfH19cfXqVcybNw9//vknRo0aZdolISqDmJgYvPrqq0hLS1PKvL29sWDBAvTv31+9jhER1WBGjxkZNGgQ5s+fj6lTpyIwMBBJSUmIjY1VBrWeOXMG6enpSv0rV64gIiIC/v7+6NWrF7Kzs7F//340a9bMdEtBVAYxMTEYMGAAWrZsiYSEBFy7dg0JCQlo2bIlBgwYgJiYGLW7SERUI/Fy8FQjFBYWwtfXFy1btsT27dthZvZPDtfr9QgLC8ORI0dw/PhxaLVaFXtKRFR9VOjl4Imqmvj4eKSlpeHNN980CCLA7WvlTJkyBadPn0Z8fLxKPSQiqrkYRqhGKDp02KJFixKfLyq/8xAjERE9HAwjVCMUnb115MiREp8vKr/XWV5ERFQxGEaoRggJCYG3tzfmzJkDvV5v8Jxer0dUVBQaNmyIkJAQlXpIRFRzMYxQjaDVarFgwQLs2LEDYWFhBmfThIWFYceOHZg/fz4HrxIRqcDo64wQVVX9+/fHli1b8Oqrr6JTp05KecOGDbFlyxZeZ4SISCU8tZdqnMLCQsTHxyM9PR3u7u4ICQnhHhEiogpQ1u9v7hmhGker1aJr165qd4OIiP6HY0aIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYo3yqMah3ftJSKqXLhnhGqUmJgY+Pr6olu3bhg8eDC6desGX19fxMTEqN01IqIai2GEaoyYmBgMGDAALVu2REJCAq5du4aEhAS0bNkSAwYMYCAhIlKJRkRE7U7cT3Z2NhwcHJCVlQV7e3u1u0NVUGFhIXx9fdGyZUts374dZmb/5HC9Xo+wsDAcOXIEx48f5yEbIiITKev3N/eMUI0QHx+PtLQ0vPnmmwZBBADMzMwwZcoUnD59GvHx8Sr1kIio5mIYoRohPT0dANCiRYsSny8qL6pHREQPD8MI1Qju7u4AgCNHjpT4fFF5UT0iInp4GEaoRggJCYG3tzfmzJkDvV5v8Jxer0dUVBQaNmyIkJAQlXpIRFRzMYxQjaDVarFgwQLs2LEDYWFhBmfThIWFYceOHZg/fz4HrxIRqYAXPaMao3///tiyZQteffVVdOrUSSlv2LAhtmzZgv79+6vYOyKimoun9lKNwyuwEhE9HGX9/uaeEapxtFotunbtqnY3iIjofzhmhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqipXGImOjoa3tzcsLS3RoUMHHDhwoEzTbdy4ERqNBmFhYeVploiIiKoho8PIpk2bEBkZiWnTpuHQoUMICAhAaGgozp8/f8/p0tLS8NprryEkJKTcnSUiIqLqx+gwsnDhQkRERGDEiBFo1qwZli9fDmtra6xevbrUaQoLC/Hcc89hxowZaNSo0QN1mIiIiKoXo8JIfn4+EhMT0b17939mYGaG7t27IyEhodTpZs6cCRcXFzz//PNlaicvLw/Z2dkGf0RERFQ9GRVGLl68iMLCQri6uhqUu7q6IiMjo8RpfvjhB6xatQorV64scztRUVFwcHBQ/ry8vIzpJhEREVUhFXo2zbVr1zB06FCsXLkSTk5OZZ5uypQpyMrKUv7Onj1bgb0kIiIiNdUyprKTkxO0Wi0yMzMNyjMzM+Hm5las/smTJ5GWloa+ffsqZXq9/nbDtWohNTUVPj4+xabT6XTQ6XTGdI2IiIiqKKP2jFhYWCAoKAhxcXFKmV6vR1xcHIKDg4vV9/Pzw++//46kpCTl79///je6deuGpKQkHn4hIiIi4/aMAEBkZCTCw8PRtm1btG/fHosWLUJubi5GjBgBABg2bBg8PT0RFRUFS0tLtGjRwmB6R0dHAChWTkRERDWT0WFk0KBBuHDhAqZOnYqMjAwEBgYiNjZWGdR65swZmJnxwq5ERERUNhoREbU7cT/Z2dlwcHBAVlYW7O3t1e4OERERlUFZv7+5C4OIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVTGMEBERkaoYRoiIiEhVDCNERESkKoYRIiIiUhXDCBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqKlcYiY6Ohre3NywtLdGhQwccOHCg1LoxMTFo27YtHB0dYWNjg8DAQKxbt67cHSYiIqLqxegwsmnTJkRGRmLatGk4dOgQAgICEBoaivPnz5dYv06dOnjrrbeQkJCA3377DSNGjMCIESOwa9euB+48ERERVX0aERFjJujQoQPatWuHDz74AACg1+vh5eWFCRMmYPLkyWWaR5s2bdC7d2/MmjWrTPWzs7Ph4OCArKws2NvbG9NdIiIiUklZv79rGTPT/Px8JCYmYsqUKUqZmZkZunfvjoSEhPtOLyLYvXs3UlNTMXfu3FLr5eXlIS8vT3mclZUF4PZCERERUdVQ9L19v/0eRoWRixcvorCwEK6urgblrq6uSElJKXW6rKwseHp6Ii8vD1qtFkuXLkWPHj1KrR8VFYUZM2YUK/fy8jKmu0RERFQJXLt2DQ4ODqU+b1QYKS87OzskJSUhJycHcXFxiIyMRKNGjdC1a9cS60+ZMgWRkZHKY71ej8uXL6Nu3brQaDQPo8sml52dDS8vL5w9e5aHmioBbo/Kg9ui8uC2qDyqy7YQEVy7dg0eHh73rGdUGHFycoJWq0VmZqZBeWZmJtzc3EqdzszMDL6+vgCAwMBAJCcnIyoqqtQwotPpoNPpDMocHR2N6WqlZW9vX6VfWNUNt0flwW1ReXBbVB7VYVvca49IEaPOprGwsEBQUBDi4uKUMr1ej7i4OAQHB5d5Pnq93mBMCBEREdVcRh+miYyMRHh4ONq2bYv27dtj0aJFyM3NxYgRIwAAw4YNg6enJ6KiogDcHv/Rtm1b+Pj4IC8vD19//TXWrVuHZcuWmXZJiIiIqEoyOowMGjQIFy5cwNSpU5GRkYHAwEDExsYqg1rPnDkDM7N/drjk5uZi7Nix+Ouvv2BlZQU/Pz+sX78egwYNMt1SVAE6nQ7Tpk0rdviJ1MHtUXlwW1Qe3BaVR03bFkZfZ4SIiIjIlHhvGiIiIlIVwwgRERGpimGEiIiIVMUwch/Tp09HYGCg2t2gBzB8+HCEhYWp3Q2iB6bRaLB9+/Yy19+7dy80Gg2uXr1aYX0iMoUaGUYSEhKg1WrRu3fvCpm/t7c3NBoNNBoNtFotPDw88Pzzz+PKlSsV0l5JKvOHUEZGBiZOnAhfX19YWlrC1dUVnTt3xrJly3D9+vUKb3/48OHK9tFoNKhbty569uyJ3377rcLbvpOxXywPS0ZGBiZMmIBGjRpBp9PBy8sLffv2Nbi+0L2sXbu2xIsUdu3a1WC9u7q64umnn8aff/5p4iUoXVpaGjQaDZKSkh5am8a6V3hOT0/HE088YdL27vWD6/Dhwxg0aBDc3d2h0+nQoEED9OnTB1999ZVyr5GidVr0Z2FhAV9fX7zzzjsG9yOZPn06NBoNevbsWaydefPmQaPRlHohzMqgsLAQnTp1Qv/+/Q3Ks7Ky4OXlhbfeeksp27p1Kx599FHUrl0bVlZWaNq0KUaOHInDhw8rddauXWuw3mxtbREUFISYmJiHtkzA7fflyy+//FDbLEmNDCOrVq3ChAkTsG/fPpw7d65C2pg5cybS09Nx5swZbNiwAfv27cNLL71UIW1VJadOnULr1q3x7bffYs6cOTh8+DASEhIwadIk7NixA99//32J0926dcuk/ejZsyfS09ORnp6OuLg41KpVC3369DFpG1VRWloagoKCsHv3bsybNw+///47YmNj0a1bN4wbN+6B5x8REYH09HScO3cOX3zxBc6ePYshQ4aYoOc1g5ub20M71fOLL75Ax44dkZOTg48//hjJycmIjY1Fv3798Pbbbys3MC3y/fffIz09HcePH8eMGTMwe/ZsrF692qCOu7s79uzZg7/++sugfPXq1ahfv36FL9OD0Gq1WLt2LWJjY7FhwwalfMKECahTpw6mTZsGAHjjjTcwaNAgBAYG4ssvv0Rqaio+/fRTNGrUyOAms8Dtq6sWfQ4dPnwYoaGhGDhwIFJTUx/qslUKUsNcu3ZNbG1tJSUlRQYNGiSzZ882eD4qKkpcXFzE1tZWRo4cKW+88YYEBAQozx84cEC6d+8udevWFXt7e3nkkUckMTHRYB4NGjSQ999/36Bs1qxZ0qxZM4OyLVu2SLNmzcTCwkIaNGgg8+fPN3j+8uXLMnToUHF0dBQrKyvp2bOnHDt2THk+LS1N+vTpI46OjmJtbS3NmjWTnTt3yunTpwWAwV94eHj5V5oJhYaGSr169SQnJ6fE5/V6vYiIAJClS5dK3759xdraWqZNmyYFBQUycuRI8fb2FktLS2nSpIksWrTIYPqCggJ55ZVXxMHBQerUqSOvv/66DBs2TJ588kmlTnh4uMFjEZH4+HgBIOfPn1fKfvvtN+nWrZtYWlpKnTp1JCIiQq5du6Y8X1hYKDNmzBBPT0+xsLCQgIAA+eabb5Tn8/LyZNy4ceLm5iY6nU7q168vc+bMEZHbr5E7t0+DBg3KszpN7oknnhBPT88St8+VK1dERGTBggXSokULsba2lnr16smYMWOU9bJnz55ir71p06aJiEiXLl1k4sSJBvNct26dWFtbG5Tt3btX2rVrJxYWFuLm5iZvvPGG3Lp1S3n+5s2bMmHCBHF2dhadTiedO3eWAwcOKM9fvnxZBg8eLE5OTmJpaSm+vr6yevVqEZFifevSpcsDrjHTK+n1WQSAbNu2TXn8448/SkBAgOh0OgkKCpJt27YJADl8+LCI/LM9vv/+ewkKChIrKysJDg6WlJQUERFZs2ZNsXWyZs0aycnJkbp160q/fv1K7WfRe7Xo86aozSKPPfaYjB07Vnk8bdo0CQgIkD59+sg777xjsAxOTk4yZsyYSrk97rZ48WKpXbu2nDt3TrZv3y7m5uaSlJQkIiIJCQkCQBYvXlzitEXrTOT2undwcDB4vrCwUMzNzeXzzz9Xyu73PSBy/++S6Oho8fX1FZ1OJy4uLvLUU0+JyO3X2t3b//Tp0+VdNQ+kxoWRVatWSdu2bUVE5KuvvhIfHx/lBbJp0ybR6XTy0UcfSUpKirz11ltiZ2dnEEbi4uJk3bp1kpycLH/88Yc8//zz4urqKtnZ2Uqdu8PIX3/9Je3bt5cRI0YoZb/88ouYmZnJzJkzJTU1VdasWSNWVlayZs0apc6///1v8ff3l3379klSUpKEhoaKr6+v5Ofni4hI7969pUePHvLbb7/JyZMn5auvvpL/+7//k4KCAtm6dasAkNTUVElPT5erV69WwNo0zsWLF0Wj0UhUVNR96wIQFxcXWb16tZw8eVL+/PNPyc/Pl6lTp8rBgwfl1KlTsn79erG2tpZNmzYp082dO1dq164tW7duVbaPnZ3dPcPItWvXZPTo0eLr6yuFhYUiIpKTkyPu7u7Sv39/+f333yUuLk4aNmxoEOoWLlwo9vb28tlnn0lKSopMmjRJzM3NlQ+KefPmiZeXl+zbt0/S0tIkPj5ePv30UxEROX/+vPLBn56ebhCC1HLp0iXRaDRKYCrN+++/L7t375bTp09LXFycNG3aVMaMGSMitwPYokWLxN7eXtLT0yU9PV0JKneHkUuXLknfvn2lW7duStlff/0l1tbWMnbsWElOTpZt27aJk5OTEmhERF566SXx8PCQr7/+Wo4ePSrh4eFSu3ZtuXTpkoiIjBs3TgIDA+XgwYNy+vRp+e677+TLL78Ukds/Joq+nNPT05VpKpOyhpGsrCypU6eODBkyRI4ePSpff/21NGnSpMQw0qFDB9m7d68cPXpUQkJCpFOnTiIicv36dXn11VelefPmyva6fv26xMTECABJSEi4b39LCiMHDx4UR0dH+fjjj5WyojASExMjvr6+Svnzzz8vEydOlIkTJ1aJMKLX66Vr167y2GOPiYuLi8yaNUt57qWXXhJbW1uD8Fyau8NIQUGBrF69WszNzeXEiRNK+f2+B+73XXLw4EHRarXy6aefSlpamhw6dEgJS1evXpXg4GCJiIhQtn9BQYEJ1pLxalwY6dSpk/Jr+tatW+Lk5CR79uwREZHg4GCDJC8i0qFDB4MwcrfCwkKxs7OTr776Silr0KCBWFhYiI2NjVhaWiofBkW/LEVEBg8eLD169DCY1+uvv67sPTl27JgAkB9//FF5/uLFi2JlZaWk5pYtW8r06dNL7FfRh9Cdbartp59+EgASExNjUF63bl2xsbERGxsbmTRpkojc/tB9+eWX7zvPcePGKSlfRMTd3V3ee+895fGtW7ekXr16xcKIVqtV2gQg7u7uBnu4VqxYIbVr1zbYQ7Bz504xMzOTjIwMERHx8PAotmetXbt2ymtowoQJ8uijjxr8GrrT3b9y1fbzzz+XuH3uZ/PmzVK3bl3lcUm/+ERuhxFzc3OxsbERa2trASBNmjQx+CX25ptvStOmTQ3WWXR0tNja2kphYaHk5OSIubm5bNiwQXk+Pz9fPDw8lO3et29fg+B/p9J+xVcmZQ0jy5Ytk7p168qNGzeU51euXFnqnpEiO3fuFADKdEUh4U7vvvuuAJDLly8rZQcOHFDeMzY2NspnXtE6tbKyEhsbGzE3NxcA8sILLxjMs6id/Px8cXFxkf/7v/+TnJwcsbOzk19//bXKhBERkeTkZAEgLVu2NAgePXv2lFatWhnUXbBggcF6K/phWLRXqqjczMxMdDqdwQ/SsnwP3O+7ZOvWrWJvb2/wg/lOJe2xVEONGjOSmpqKAwcO4NlnnwUA1KpVC4MGDcKqVasAAMnJyejQoYPBNHffADAzMxMRERFo3LgxHBwcYG9vj5ycHJw5c8ag3uuvv46kpCT89ttvysC/3r17o7CwUGmrc+fOBtN07twZx48fR2FhIZKTk1GrVi2D/tStWxdNmzZFcnIyAOCll17CO++8g86dO2PatGkPfQCmqRw4cABJSUlo3ry5wQ0U27ZtW6xudHQ0goKC4OzsDFtbW6xYsUJZ91lZWUhPTzdYZ7Vq1SpxPt26dUNSUhKSkpJw4MABhIaG4oknnlAGUyYnJyMgIAA2NjbKNJ07d4Zer0dqaiqys7Nx7ty5Erdh0fYZPnw4kpKS0LRpU7z00kv49ttvH2AtVTwp48WYv//+ezz22GPw9PSEnZ0dhg4dikuXLpVp8PFzzz2HpKQk/Prrr/jhhx/g6+uLxx9/HNeuXQNwe70HBwdDo9Eo03Tu3Bk5OTn466+/cPLkSdy6dctgvZubm6N9+/bKeh8zZgw2btyIwMBATJo0Cfv37zdmNVQZqampaNWqFSwtLZWy9u3bl1i3VatWyv/d3d0BAOfPnzeqvVatWinvmdzcXBQUFBg8v2nTJmXbfv755/jiiy8wefLkYvMxNzfHkCFDsGbNGmzevBlNmjQx6F9VsHr1alhbW+P06dPFxr/cbeTIkUhKSsKHH36I3Nxcg/eZnZ2dsk4PHz6MOXPm4MUXX8RXX30FAGX6Hrjfd0mPHj3QoEEDNGrUCEOHDsWGDRseyokCxqpRYWTVqlUoKCiAh4cHatWqhVq1amHZsmXYunVrscFYpQkPD0dSUhIWL16M/fv3IykpCXXr1kV+fr5BPScnJ/j6+qJx48Z49NFHsWjRIuzfvx979uwx2fKMGjUKp06dwtChQ/H777+jbdu2WLJkicnmb2q+vr7QaDTFBmc1atQIvr6+sLKyMii/MwgAwMaNG/Haa6/h+eefx7fffoukpCSMGDGi2LovCxsbG/j6+sLX1xft2rXDRx99hNzcXKxcudL4BStFmzZtcPr0acyaNQs3btzAwIEDMWDAAJPN39QaN24MjUaDlJSUUuukpaWhT58+aNWqFbZu3YrExERER0cDQJm2g4ODg7LeO3fujFWrVuH48ePYtGmTyZajKFS+8sorOHfuHB577DG89tprJpt/VWRubq78vyjo6fX6Uus3btwYAAzeqzqdTtl2JfHy8oKvry/8/f3x9NNP4+WXX8aCBQtw8+bNYnVHjhyJzZs3Izo6GiNHjizXMqll//79eP/997Fjxw60b98ezz//vBIwGjdujFOnThkMuHd0dISvry88PT2LzcvMzExZp61atUJkZCS6du2KuXPnmqy/dnZ2OHToED777DO4u7tj6tSpCAgIqHRnWtaYMFJQUIBPPvkECxYsUJJoUYr38PDAZ599Bn9/f/z8888G0/30008Gj3/88Ue89NJL6NWrF5o3bw6dToeLFy/et32tVgsAuHHjBgDA398fP/74Y7F5N2nSBFqtFv7+/igoKDDoz6VLl5CamopmzZopZV5eXnjxxRcRExODV199VfkytbCwAABlT0xlULduXfTo0QMffPABcnNzjZ7+xx9/RKdOnTB27Fi0bt0avr6+OHnypPK8g4MD3N3dDdZZQUEBEhMT7ztvjUYDMzMzg+3z66+/GvTzxx9/hJmZGZo2bQp7e3t4eHiUuA3v3D729vYYNGgQVq5ciU2bNmHr1q24fPkygNtfEJVp+9SpUwehoaGIjo4ucftcvXoViYmJ0Ov1WLBgATp27IgmTZoUOyPNwsKizMtV0vsiISHB4Nfjjz/+CDs7O9SrVw8+Pj6wsLAwWO+3bt3CwYMHDda7s7MzwsPDsX79eixatAgrVqxQ+gZUrvdFeTVt2hS///67wd7EgwcPGj2fkrbX448/jjp16jzQl6JWq0VBQUGJIbV58+Zo3rw5jhw5gsGDB5e7jYft+vXrGD58OMaMGYNu3bph1apVOHDgAJYvXw4AePbZZ5GTk4OlS5eWuw2tVmvwfrjf98D9vkuA23uIu3fvjvfeew+//fYb0tLSsHv3bgDGvV8rlLpHiR6ebdu2iYWFRYkDOSdNmiRt27aVjRs3iqWlpaxevVpSU1Nl6tSpxQawtm7dWnr06CF//PGH/PTTTxISEiJWVlYGA1YbNGggM2fOlPT0dDl37pz8/PPP0qVLF3F2dpaLFy+KiEhiYqLBoKO1a9cWG8D65JNPSrNmzSQ+Pl6SkpKkZ8+eBgOXJk6cKLGxsXLq1ClJTEyUDh06yMCBA0Xk9kBAjUYja9eulfPnzxucBaKmEydOiKurq/j5+cnGjRvljz/+kJSUFFm3bp24urpKZGSkiJQ8nmLx4sVib28vsbGxkpqaKm+//bbY29sbbJ93331X6tSpI9u2bZPk5GSJiIgocQBrz549lQFbf/zxh4wdO1Y0Go0yfig3N1fc3d3lqaeekt9//112794tjRo1MhjA+v7774u9vb1s3LhRUlJS5I033jAYwLpgwQL59NNPJTk5WVJTU+X5558XNzc3ZZBs48aNZcyYMZKenm5wbF5NJ0+eFDc3N2nWrJls2bJFjh07Jn/88YcsXrxY/Pz8JCkpSQDIokWL5OTJk/LJJ5+Ip6enwfikH3/8URmncOHCBcnNzRWR28em7xwol5SUJE899ZRYWloqZ3cUDWAdN26cJCcny/bt24sNYJ04caJ4eHjIN998YzCAtWgd/uc//5Ht27fL8ePH5ciRI9KnTx9p3769iNweQ2RlZSXvvPOOZGRkVIqB3XcLDw+Xrl27yuHDhw3+zpw5U+IA1mHDhskff/whsbGx4ufnJwCUsztKGjt2+PBhg7MmNmzYIDY2NnL48GG5cOGC3Lx5U0REYmJixNzcXHr16iWxsbFy8uRJ+fXXX2Xu3LkCQBkUXDRmpGhQ8NmzZ+Xrr78WT09Pg8HJd49NycnJMehXVRgz8tJLL4mvr6/ymhYRWb58udja2irr89VXXxWtViuvvPKKxMfHS1pamiQkJMiQIUNEo9FIVlaWiNweM3LnQO9Tp07Jhx9+KFqtVmbMmKHM/37fA/f7Lvnqq69k8eLFcvjwYUlLS5OlS5eKmZmZHDlyREREIiIipF27dnL69Gm5cOGC8vn0sNWYMNKnTx/p1atXic8VDdz79ddfZfbs2eLk5CS2trYSHh4ukyZNMngDHTp0SNq2bSuWlpbSuHFj2bx5c7GzZ+4+bdPZ2Vl69epVbNBc0elY5ubmUr9+fZk3b57B80WndDk4OIiVlZWEhoYanNI1fvx48fHxEZ1OJ87OzjJ06FAl7IiIzJw5U9zc3ESj0VSaU3tFRM6dOyfjx4+Xhg0birm5udja2kr79u1l3rx5ypu8pDBy8+ZNGT58uDg4OIijo6OMGTNGJk+ebLB9bt26JRMnThR7e3txdHSUyMjIEk/tvXP72NnZSbt27WTLli0G7ZXl1N7p06eLp6enmJubFzu1d8WKFRIYGCg2NjZib28vjz32mBw6dEh5/ssvvxRfX1+pVatWpTm1V+T29hk3bpwyENvT01P+/e9/K0Ft4cKF4u7urrwmP/nkk2JfeC+++KLUrVu32Km9d6732rVrS5cuXWT37t0G7d/v1N4bN27IhAkTxMnJqcRTe2fNmiX+/v5iZWUlderUkSeffFJOnTqlPL9y5Urx8vISMzOzSvnlV9LplgDk+eefL/HU3latWomFhYUEBQXJp59+KgCUcFeWMHLz5k156qmnxNHRUTnDq8jBgwdlwIAB4uLiIrVq1ZK6detKaGiobNy4sdipvUV/Wq1W6tWrJxEREQZniZU0UPZOlT2M7N27V7RarcTHxxd77vHHHzcYrL5p0ybp2rWrODg4iLm5udSrV08GDx4sP/30kzLN3adV63Q6adKkicyePdvgjJb7fQ+I3Pu7JD4+Xrp06SK1a9cWKysradWqlcEZiKmpqdKxY0exsrJS9dRejUgZR60REVGltmHDBowYMQJZWVnFxmARVWa11O4AERGVzyeffIJGjRrB09MTv/76K9544w0MHDiQQYSqHIYRIqIqKiMjA1OnTkVGRgbc3d3x9NNPY/bs2Wp3i8hoPExDREREqqoxp/YSERFR5cQwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFT1/5ojrWVDyj6gAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Bupa scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(bupa_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Bupa'] = bupa_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>Bupa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "      <td>97.159847</td>\n",
              "      <td>86.347619</td>\n",
              "      <td>93.815873</td>\n",
              "      <td>71.669748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>78.145238</td>\n",
              "      <td>90.854762</td>\n",
              "      <td>69.783193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>87.076190</td>\n",
              "      <td>93.815079</td>\n",
              "      <td>69.846218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>82.361905</td>\n",
              "      <td>92.849206</td>\n",
              "      <td>69.794118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "      <td>96.792626</td>\n",
              "      <td>83.802381</td>\n",
              "      <td>92.960317</td>\n",
              "      <td>74.475630</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer      Sonar  Ionosphere       Bupa\n",
              "0   AdaBoost  96.552288      97.159847  86.347619   93.815873  71.669748\n",
              "1  GradBoost  98.075163      96.646633  78.145238   90.854762  69.783193\n",
              "2   CatBoost  97.967320      97.378303  87.076190   93.815079  69.846218\n",
              "3   LightGBM  97.120915      97.334612  82.361905   92.849206  69.794118\n",
              "4    XGBoost  97.797386      96.792626  83.802381   92.960317  74.475630"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Bupa'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Pima**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "pima_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Pima\\Diabetes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = pima_df.iloc[:, :-1]\n",
        "y = pima_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:37<00:00,  1.95s/trial, best loss: -0.7857142857142857]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 1500.0, 'learning_rate': 0.010436960322525368, 'max_depth': 1.0, 'max_features': None, 'min_samples_leaf': 5.0, 'min_samples_split': 4.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:24<00:00,  2.01trial/s, best loss: -0.7922077922077922]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': 'sqrt', 'n_estimators': 200, 'learning_rate': 0.05871692740564188, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 9, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 2.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [01:15<00:00,  1.50s/trial, best loss: -0.7792207792207793]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 50, 'learning_rate': 0.010922414344918462, 'min_child_samples': 10, 'max_depth': 4, 'reg_lambda': 4.685483905860218, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 32.80trial/s, best loss: -0.7922077922077922]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'dart', 'num_leaves': 95, 'learning_rate': 0.04955748086609083, 'min_child_samples': 140, 'reg_alpha': 1.1745781431363478, 'reg_lambda': 1.5581466068782919, 'colsample_by_tree': 0.9952093023356591, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:06<00:00,  7.29trial/s, best loss: -0.7987012987012987]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'gbtree', 'learning_rate': 0.011777426690454684, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'colsample_bytree': 0.6642423404208758, 'colsample_bylevel': 0.8389604376670141, 'colsample_bynode': 0.46801910869053165, 'reg_alpha': 1.3842922617481603, 'reg_lambda': 0.25127542856871243, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Pima Dataset ---------\n",
            "[0.80519481 0.76623377 0.75324675 0.85714286 0.81818182 0.74025974\n",
            " 0.74025974 0.71428571 0.75       0.69736842 0.77922078 0.7012987\n",
            " 0.75324675 0.74025974 0.74025974 0.84415584 0.80519481 0.72727273\n",
            " 0.73684211 0.82894737 0.81818182 0.74025974 0.77922078 0.68831169\n",
            " 0.75324675 0.76623377 0.74025974 0.81818182 0.71052632 0.69736842\n",
            " 0.80519481 0.75324675 0.76623377 0.77922078 0.79220779 0.74025974\n",
            " 0.77922078 0.80519481 0.76315789 0.67105263 0.76623377 0.77922078\n",
            " 0.84415584 0.81818182 0.80519481 0.75324675 0.68831169 0.77922078\n",
            " 0.73684211 0.69736842 0.76623377 0.77922078 0.76623377 0.72727273\n",
            " 0.76623377 0.80519481 0.71428571 0.74025974 0.76315789 0.72368421\n",
            " 0.76623377 0.74025974 0.75324675 0.76623377 0.71428571 0.83116883\n",
            " 0.74025974 0.74025974 0.76315789 0.75       0.80519481 0.75324675\n",
            " 0.72727273 0.84415584 0.74025974 0.74025974 0.77922078 0.72727273\n",
            " 0.75       0.73684211 0.72727273 0.72727273 0.79220779 0.79220779\n",
            " 0.77922078 0.81818182 0.74025974 0.67532468 0.76315789 0.81578947\n",
            " 0.79220779 0.76623377 0.80519481 0.77922078 0.75324675 0.77922078\n",
            " 0.76623377 0.71428571 0.69736842 0.76315789]\n",
            "Accuracy: 76.10% (3.95%)\n",
            "Execution Time: 251.89 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Pima Dataset ---------\n",
            "[0.74025974 0.77922078 0.77922078 0.81818182 0.79220779 0.76623377\n",
            " 0.72727273 0.74025974 0.71052632 0.75       0.76623377 0.7012987\n",
            " 0.80519481 0.75324675 0.72727273 0.85714286 0.83116883 0.76623377\n",
            " 0.71052632 0.82894737 0.75324675 0.74025974 0.80519481 0.7012987\n",
            " 0.79220779 0.77922078 0.75324675 0.83116883 0.73684211 0.75\n",
            " 0.79220779 0.75324675 0.76623377 0.80519481 0.81818182 0.72727273\n",
            " 0.81818182 0.79220779 0.73684211 0.64473684 0.74025974 0.77922078\n",
            " 0.83116883 0.79220779 0.80519481 0.76623377 0.7012987  0.83116883\n",
            " 0.77631579 0.69736842 0.79220779 0.75324675 0.76623377 0.74025974\n",
            " 0.77922078 0.81818182 0.76623377 0.67532468 0.72368421 0.75\n",
            " 0.74025974 0.71428571 0.72727273 0.81818182 0.7012987  0.84415584\n",
            " 0.74025974 0.72727273 0.77631579 0.76315789 0.79220779 0.75324675\n",
            " 0.72727273 0.83116883 0.76623377 0.77922078 0.76623377 0.75324675\n",
            " 0.69736842 0.80263158 0.75324675 0.7012987  0.81818182 0.79220779\n",
            " 0.81818182 0.77922078 0.75324675 0.66233766 0.71052632 0.80263158\n",
            " 0.77922078 0.79220779 0.79220779 0.76623377 0.74025974 0.79220779\n",
            " 0.76623377 0.75324675 0.73684211 0.76315789]\n",
            "Accuracy: 76.43% (4.16%)\n",
            "Execution Time: 12.60 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Pima Dataset ---------\n",
            "[0.77922078 0.74025974 0.76623377 0.84415584 0.80519481 0.72727273\n",
            " 0.72727273 0.74025974 0.69736842 0.77631579 0.72727273 0.68831169\n",
            " 0.79220779 0.71428571 0.67532468 0.87012987 0.83116883 0.74025974\n",
            " 0.67105263 0.78947368 0.72727273 0.74025974 0.80519481 0.68831169\n",
            " 0.77922078 0.75324675 0.80519481 0.81818182 0.72368421 0.73684211\n",
            " 0.80519481 0.76623377 0.75324675 0.76623377 0.79220779 0.7012987\n",
            " 0.77922078 0.77922078 0.75       0.64473684 0.71428571 0.81818182\n",
            " 0.79220779 0.77922078 0.76623377 0.76623377 0.66233766 0.77922078\n",
            " 0.76315789 0.72368421 0.83116883 0.76623377 0.72727273 0.71428571\n",
            " 0.75324675 0.81818182 0.75324675 0.72727273 0.73684211 0.76315789\n",
            " 0.74025974 0.76623377 0.72727273 0.77922078 0.74025974 0.85714286\n",
            " 0.75324675 0.67532468 0.76315789 0.73684211 0.76623377 0.76623377\n",
            " 0.74025974 0.80519481 0.72727273 0.75324675 0.76623377 0.79220779\n",
            " 0.71052632 0.73684211 0.76623377 0.71428571 0.79220779 0.84415584\n",
            " 0.77922078 0.75324675 0.75324675 0.67532468 0.64473684 0.81578947\n",
            " 0.72727273 0.74025974 0.75324675 0.71428571 0.76623377 0.80519481\n",
            " 0.81818182 0.74025974 0.71052632 0.73684211]\n",
            "Accuracy: 75.53% (4.47%)\n",
            "Execution Time: 14.87 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Pima Dataset ---------\n",
            "[0.79220779 0.79220779 0.74025974 0.81818182 0.81818182 0.74025974\n",
            " 0.80519481 0.72727273 0.65789474 0.68421053 0.68831169 0.71428571\n",
            " 0.81818182 0.74025974 0.77922078 0.85714286 0.72727273 0.77922078\n",
            " 0.71052632 0.85526316 0.79220779 0.75324675 0.77922078 0.62337662\n",
            " 0.80519481 0.74025974 0.71428571 0.79220779 0.75       0.72368421\n",
            " 0.79220779 0.74025974 0.80519481 0.79220779 0.76623377 0.74025974\n",
            " 0.76623377 0.77922078 0.76315789 0.69736842 0.72727273 0.84415584\n",
            " 0.79220779 0.77922078 0.76623377 0.83116883 0.63636364 0.77922078\n",
            " 0.75       0.75       0.80519481 0.77922078 0.76623377 0.74025974\n",
            " 0.79220779 0.77922078 0.66233766 0.68831169 0.80263158 0.78947368\n",
            " 0.79220779 0.83116883 0.71428571 0.81818182 0.7012987  0.79220779\n",
            " 0.75324675 0.74025974 0.75       0.75       0.83116883 0.74025974\n",
            " 0.68831169 0.79220779 0.68831169 0.76623377 0.76623377 0.77922078\n",
            " 0.72368421 0.75       0.71428571 0.76623377 0.76623377 0.77922078\n",
            " 0.74025974 0.77922078 0.75324675 0.7012987  0.71052632 0.78947368\n",
            " 0.75324675 0.79220779 0.79220779 0.71428571 0.79220779 0.72727273\n",
            " 0.76623377 0.80519481 0.75       0.73684211]\n",
            "Accuracy: 75.92% (4.53%)\n",
            "Execution Time: 2.17 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Pima Dataset ---------\n",
            "[0.76623377 0.74025974 0.74025974 0.84415584 0.81818182 0.76623377\n",
            " 0.71428571 0.72727273 0.71052632 0.71052632 0.71428571 0.68831169\n",
            " 0.77922078 0.76623377 0.67532468 0.84415584 0.79220779 0.71428571\n",
            " 0.72368421 0.77631579 0.76623377 0.75324675 0.77922078 0.72727273\n",
            " 0.77922078 0.7012987  0.72727273 0.84415584 0.72368421 0.72368421\n",
            " 0.72727273 0.77922078 0.77922078 0.79220779 0.77922078 0.71428571\n",
            " 0.77922078 0.75324675 0.72368421 0.69736842 0.76623377 0.72727273\n",
            " 0.80519481 0.77922078 0.74025974 0.76623377 0.72727273 0.74025974\n",
            " 0.76315789 0.71052632 0.76623377 0.72727273 0.76623377 0.75324675\n",
            " 0.74025974 0.77922078 0.75324675 0.75324675 0.72368421 0.76315789\n",
            " 0.77922078 0.75324675 0.77922078 0.75324675 0.72727273 0.81818182\n",
            " 0.74025974 0.74025974 0.77631579 0.75       0.80519481 0.72727273\n",
            " 0.76623377 0.79220779 0.74025974 0.72727273 0.80519481 0.74025974\n",
            " 0.72368421 0.72368421 0.7012987  0.68831169 0.80519481 0.77922078\n",
            " 0.81818182 0.77922078 0.74025974 0.67532468 0.69736842 0.82894737\n",
            " 0.76623377 0.75324675 0.77922078 0.75324675 0.74025974 0.76623377\n",
            " 0.76623377 0.74025974 0.71052632 0.76315789]\n",
            "Accuracy: 75.33% (3.63%)\n",
            "Execution Time: 3.70 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "pima_scores = []\n",
        "pima_mean = []\n",
        "pima_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()     \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    pima_scores.append(results)\n",
        "    pima_mean.append(results.mean()*100)\n",
        "    pima_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Pima Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Pima scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(pima_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Pima'] = pima_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>Bupa</th>\n",
              "      <th>Pima</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "      <td>97.159847</td>\n",
              "      <td>86.347619</td>\n",
              "      <td>93.815873</td>\n",
              "      <td>71.669748</td>\n",
              "      <td>76.101504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>78.145238</td>\n",
              "      <td>90.854762</td>\n",
              "      <td>69.783193</td>\n",
              "      <td>76.426863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>87.076190</td>\n",
              "      <td>93.815079</td>\n",
              "      <td>69.846218</td>\n",
              "      <td>75.527683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>82.361905</td>\n",
              "      <td>92.849206</td>\n",
              "      <td>69.794118</td>\n",
              "      <td>75.920711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "      <td>96.792626</td>\n",
              "      <td>83.802381</td>\n",
              "      <td>92.960317</td>\n",
              "      <td>74.475630</td>\n",
              "      <td>75.334074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer      Sonar  Ionosphere       Bupa  \\\n",
              "0   AdaBoost  96.552288      97.159847  86.347619   93.815873  71.669748   \n",
              "1  GradBoost  98.075163      96.646633  78.145238   90.854762  69.783193   \n",
              "2   CatBoost  97.967320      97.378303  87.076190   93.815079  69.846218   \n",
              "3   LightGBM  97.120915      97.334612  82.361905   92.849206  69.794118   \n",
              "4    XGBoost  97.797386      96.792626  83.802381   92.960317  74.475630   \n",
              "\n",
              "        Pima  \n",
              "0  76.101504  \n",
              "1  76.426863  \n",
              "2  75.527683  \n",
              "3  75.920711  \n",
              "4  75.334074  "
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Pima'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Heart**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "heart_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Heart\\Heart.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = heart_df.iloc[:, :-1]\n",
        "y = heart_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:54<00:00,  1.10s/trial, best loss: -0.8888888888888888]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 600.0, 'learning_rate': 0.03167747886969513, 'max_depth': 1.0, 'max_features': None, 'min_samples_leaf': 3.0, 'min_samples_split': 4.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:20<00:00,  2.42trial/s, best loss: -0.8703703703703703]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': None, 'n_estimators': 850, 'learning_rate': 0.053611707225416305, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 0.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:46<00:00,  1.07trial/s, best loss: -0.9074074074074074]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 100, 'learning_rate': 0.06012626813664707, 'min_child_samples': 6, 'max_depth': 5, 'reg_lambda': 3.702478129069811, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 49.49trial/s, best loss: -0.9074074074074074]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'goss', 'num_leaves': 50, 'learning_rate': 0.08551965156387403, 'min_child_samples': 80, 'reg_alpha': 1.1979901584013886, 'reg_lambda': 1.2997746583338796, 'colsample_by_tree': 0.9107668642217731, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:04<00:00, 10.58trial/s, best loss: -0.9074074074074074]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'gbtree', 'learning_rate': 0.011008587946644721, 'gamma': 0, 'max_depth': 4, 'min_child_weight': 1, 'colsample_bytree': 0.8371384244356821, 'colsample_bylevel': 0.8446187355631347, 'colsample_bynode': 0.4599194143427724, 'reg_alpha': 1.5147748652336739, 'reg_lambda': 1.4296997246408663, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Heart Dataset ---------\n",
            "[0.81481481 0.81481481 0.81481481 0.96296296 0.77777778 0.77777778\n",
            " 0.88888889 0.77777778 0.77777778 0.96296296 0.85185185 0.77777778\n",
            " 0.88888889 0.81481481 0.85185185 0.85185185 0.92592593 0.62962963\n",
            " 0.81481481 0.85185185 0.77777778 0.96296296 0.81481481 0.77777778\n",
            " 0.92592593 0.81481481 0.85185185 0.74074074 0.81481481 0.88888889\n",
            " 0.81481481 0.88888889 0.85185185 0.81481481 0.81481481 0.88888889\n",
            " 0.81481481 0.77777778 0.85185185 0.77777778 0.81481481 0.96296296\n",
            " 0.74074074 0.85185185 0.7037037  0.88888889 0.77777778 0.88888889\n",
            " 0.77777778 0.88888889 0.66666667 0.81481481 0.85185185 0.81481481\n",
            " 0.77777778 0.88888889 0.81481481 0.81481481 0.81481481 0.88888889\n",
            " 0.85185185 0.88888889 0.85185185 0.88888889 0.85185185 0.77777778\n",
            " 0.85185185 0.74074074 0.77777778 0.92592593 0.88888889 0.85185185\n",
            " 0.77777778 0.85185185 0.77777778 0.81481481 0.77777778 0.85185185\n",
            " 0.81481481 0.85185185 0.92592593 0.85185185 0.85185185 0.85185185\n",
            " 0.74074074 0.85185185 0.85185185 0.7037037  0.88888889 0.85185185\n",
            " 0.85185185 0.88888889 0.88888889 0.81481481 0.88888889 0.7037037\n",
            " 0.81481481 0.85185185 0.77777778 0.85185185]\n",
            "Accuracy: 83.11% (6.16%)\n",
            "Execution Time: 73.76 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Heart Dataset ---------\n",
            "[0.74074074 0.85185185 0.81481481 0.88888889 0.77777778 0.7037037\n",
            " 0.77777778 0.7037037  0.85185185 0.92592593 0.85185185 0.81481481\n",
            " 0.88888889 0.88888889 0.81481481 0.85185185 0.81481481 0.74074074\n",
            " 0.85185185 0.81481481 0.74074074 0.92592593 0.77777778 0.7037037\n",
            " 0.85185185 0.77777778 0.81481481 0.74074074 0.74074074 0.88888889\n",
            " 0.85185185 0.88888889 0.74074074 0.81481481 0.74074074 0.88888889\n",
            " 0.74074074 0.77777778 0.77777778 0.74074074 0.7037037  0.88888889\n",
            " 0.74074074 0.77777778 0.85185185 0.77777778 0.74074074 0.81481481\n",
            " 0.81481481 0.85185185 0.59259259 0.74074074 0.77777778 0.85185185\n",
            " 0.74074074 0.88888889 0.85185185 0.85185185 0.77777778 0.77777778\n",
            " 0.88888889 0.81481481 0.81481481 0.88888889 0.77777778 0.74074074\n",
            " 0.77777778 0.77777778 0.81481481 0.88888889 0.92592593 0.85185185\n",
            " 0.74074074 0.77777778 0.88888889 0.7037037  0.77777778 0.77777778\n",
            " 0.77777778 0.88888889 0.81481481 0.85185185 0.74074074 0.81481481\n",
            " 0.77777778 0.85185185 0.85185185 0.7037037  0.88888889 0.81481481\n",
            " 0.81481481 0.74074074 0.88888889 0.74074074 0.85185185 0.74074074\n",
            " 0.85185185 0.74074074 0.74074074 0.81481481]\n",
            "Accuracy: 80.44% (6.31%)\n",
            "Execution Time: 66.21 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Heart Dataset ---------\n",
            "[0.81481481 0.85185185 0.85185185 0.96296296 0.77777778 0.81481481\n",
            " 0.85185185 0.74074074 0.81481481 0.96296296 0.85185185 0.81481481\n",
            " 0.85185185 0.77777778 0.85185185 0.88888889 0.92592593 0.66666667\n",
            " 0.85185185 0.81481481 0.7037037  0.96296296 0.81481481 0.81481481\n",
            " 0.96296296 0.81481481 0.85185185 0.81481481 0.77777778 0.96296296\n",
            " 0.88888889 0.92592593 0.85185185 0.81481481 0.77777778 0.88888889\n",
            " 0.77777778 0.81481481 0.81481481 0.81481481 0.81481481 0.92592593\n",
            " 0.81481481 0.85185185 0.74074074 0.81481481 0.81481481 0.88888889\n",
            " 0.88888889 0.88888889 0.7037037  0.81481481 0.88888889 0.88888889\n",
            " 0.85185185 0.88888889 0.85185185 0.77777778 0.85185185 0.88888889\n",
            " 0.85185185 0.88888889 0.81481481 0.88888889 0.88888889 0.77777778\n",
            " 0.85185185 0.7037037  0.88888889 0.92592593 0.88888889 0.77777778\n",
            " 0.81481481 0.88888889 0.92592593 0.81481481 0.77777778 0.92592593\n",
            " 0.81481481 0.88888889 0.96296296 0.85185185 0.92592593 0.85185185\n",
            " 0.77777778 0.85185185 0.81481481 0.77777778 0.88888889 0.85185185\n",
            " 0.81481481 0.92592593 0.85185185 0.81481481 0.85185185 0.81481481\n",
            " 0.81481481 0.85185185 0.81481481 0.92592593]\n",
            "Accuracy: 84.48% (6.06%)\n",
            "Execution Time: 17.19 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Heart Dataset ---------\n",
            "[0.92592593 0.88888889 0.85185185 0.88888889 0.74074074 0.77777778\n",
            " 0.81481481 0.62962963 0.66666667 0.92592593 0.81481481 0.88888889\n",
            " 1.         0.81481481 0.81481481 0.77777778 0.96296296 0.74074074\n",
            " 0.85185185 0.81481481 0.7037037  0.96296296 0.81481481 0.77777778\n",
            " 0.96296296 0.81481481 0.81481481 0.81481481 0.74074074 0.92592593\n",
            " 0.92592593 0.81481481 0.85185185 0.85185185 0.74074074 0.96296296\n",
            " 0.85185185 0.85185185 0.77777778 0.81481481 0.74074074 0.88888889\n",
            " 0.85185185 0.66666667 0.74074074 0.74074074 0.7037037  0.85185185\n",
            " 0.85185185 0.85185185 0.62962963 0.88888889 0.88888889 0.88888889\n",
            " 0.81481481 0.88888889 0.96296296 0.81481481 0.85185185 0.77777778\n",
            " 0.88888889 0.77777778 0.85185185 0.88888889 0.81481481 0.85185185\n",
            " 0.85185185 0.66666667 0.88888889 0.85185185 0.92592593 0.92592593\n",
            " 0.77777778 0.85185185 0.88888889 0.85185185 0.77777778 0.81481481\n",
            " 0.81481481 0.81481481 0.88888889 0.81481481 0.7037037  0.85185185\n",
            " 0.85185185 0.85185185 0.74074074 0.7037037  0.81481481 0.74074074\n",
            " 0.81481481 0.88888889 0.85185185 0.77777778 0.88888889 0.81481481\n",
            " 0.81481481 0.77777778 0.88888889 0.92592593]\n",
            "Accuracy: 82.85% (7.58%)\n",
            "Execution Time: 0.83 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Heart Dataset ---------\n",
            "[0.81481481 0.81481481 0.85185185 0.96296296 0.81481481 0.77777778\n",
            " 0.81481481 0.74074074 0.81481481 0.96296296 0.85185185 0.77777778\n",
            " 0.88888889 0.77777778 0.88888889 0.85185185 0.96296296 0.7037037\n",
            " 0.81481481 0.88888889 0.77777778 0.96296296 0.77777778 0.85185185\n",
            " 0.96296296 0.74074074 0.88888889 0.77777778 0.81481481 0.96296296\n",
            " 0.88888889 0.92592593 0.88888889 0.81481481 0.74074074 0.88888889\n",
            " 0.77777778 0.85185185 0.85185185 0.81481481 0.77777778 0.92592593\n",
            " 0.85185185 0.96296296 0.74074074 0.81481481 0.81481481 0.92592593\n",
            " 0.85185185 0.85185185 0.7037037  0.81481481 0.85185185 0.92592593\n",
            " 0.85185185 0.85185185 0.85185185 0.81481481 0.88888889 0.92592593\n",
            " 0.81481481 0.85185185 0.88888889 0.88888889 0.88888889 0.81481481\n",
            " 0.81481481 0.7037037  0.88888889 0.92592593 0.88888889 0.81481481\n",
            " 0.77777778 0.85185185 0.88888889 0.81481481 0.77777778 0.92592593\n",
            " 0.81481481 0.92592593 0.92592593 0.85185185 0.77777778 0.85185185\n",
            " 0.74074074 0.85185185 0.81481481 0.77777778 0.85185185 0.85185185\n",
            " 0.85185185 0.92592593 0.88888889 0.81481481 0.88888889 0.77777778\n",
            " 0.81481481 0.81481481 0.85185185 0.88888889]\n",
            "Accuracy: 84.52% (6.25%)\n",
            "Execution Time: 3.65 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "heart_scores = []\n",
        "heart_mean = []\n",
        "heart_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    heart_scores.append(results)\n",
        "    heart_mean.append(results.mean()*100)\n",
        "    heart_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Heart Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Heart scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(heart_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Heart'] = heart_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>Bupa</th>\n",
              "      <th>Pima</th>\n",
              "      <th>Heart</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>96.552288</td>\n",
              "      <td>97.159847</td>\n",
              "      <td>86.347619</td>\n",
              "      <td>93.815873</td>\n",
              "      <td>71.669748</td>\n",
              "      <td>76.101504</td>\n",
              "      <td>83.111111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>78.145238</td>\n",
              "      <td>90.854762</td>\n",
              "      <td>69.783193</td>\n",
              "      <td>76.426863</td>\n",
              "      <td>80.444444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>87.076190</td>\n",
              "      <td>93.815079</td>\n",
              "      <td>69.846218</td>\n",
              "      <td>75.527683</td>\n",
              "      <td>84.481481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>82.361905</td>\n",
              "      <td>92.849206</td>\n",
              "      <td>69.794118</td>\n",
              "      <td>75.920711</td>\n",
              "      <td>82.851852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>97.797386</td>\n",
              "      <td>96.792626</td>\n",
              "      <td>83.802381</td>\n",
              "      <td>92.960317</td>\n",
              "      <td>74.475630</td>\n",
              "      <td>75.334074</td>\n",
              "      <td>84.518519</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer      Sonar  Ionosphere       Bupa  \\\n",
              "0   AdaBoost  96.552288      97.159847  86.347619   93.815873  71.669748   \n",
              "1  GradBoost  98.075163      96.646633  78.145238   90.854762  69.783193   \n",
              "2   CatBoost  97.967320      97.378303  87.076190   93.815079  69.846218   \n",
              "3   LightGBM  97.120915      97.334612  82.361905   92.849206  69.794118   \n",
              "4    XGBoost  97.797386      96.792626  83.802381   92.960317  74.475630   \n",
              "\n",
              "        Pima      Heart  \n",
              "0  76.101504  83.111111  \n",
              "1  76.426863  80.444444  \n",
              "2  75.527683  84.481481  \n",
              "3  75.920711  82.851852  \n",
              "4  75.334074  84.518519  "
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Heart'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Tic-Tac-Toe**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "tictactoe_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Datasets2\\TicTacToe\\TicTacToe.data', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = tictactoe_df.iloc[:, :-1]\n",
        "y = tictactoe_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "dummies = {\n",
        "            'x': 0,\n",
        "            'o': 1,\n",
        "            'b': 2,\n",
        "          }\n",
        "Xdummies = X.iloc[:, 0: 9].replace(dummies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = X.iloc[:, 0:9].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 958 entries, 0 to 957\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype   \n",
            "---  ------  --------------  -----   \n",
            " 0   0       958 non-null    category\n",
            " 1   1       958 non-null    category\n",
            " 2   2       958 non-null    category\n",
            " 3   3       958 non-null    category\n",
            " 4   4       958 non-null    category\n",
            " 5   5       958 non-null    category\n",
            " 6   6       958 non-null    category\n",
            " 7   7       958 non-null    category\n",
            " 8   8       958 non-null    category\n",
            "dtypes: category(9)\n",
            "memory usage: 9.7 KB\n"
          ]
        }
      ],
      "source": [
        "X.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "Xd_train, Xd_test, y_train, y_test = train_test_split(Xdummies, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:10<00:00,  1.42s/trial, best loss: -1.0]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 1200.0, 'learning_rate': 0.012995597957317272, 'max_depth': 5.0, 'max_features': 'sqrt', 'min_samples_leaf': 5.0, 'min_samples_split': 6.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:21<00:00,  2.33trial/s, best loss: -0.9895833333333334]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': None, 'n_estimators': 850, 'learning_rate': 0.09835742587463962, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 0.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [16:36<00:00, 19.92s/trial, best loss: -1.0]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 550, 'learning_rate': 0.0479901225935416, 'min_child_samples': 1, 'max_depth': 6, 'reg_lambda': 3.3766279624518107, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:02<00:00, 22.46trial/s, best loss: -0.9791666666666666]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'gbdt', 'num_leaves': 40, 'learning_rate': 0.03093317727046676, 'min_child_samples': 30, 'reg_alpha': 1.1181200940142002, 'reg_lambda': 0.1343877109037468, 'colsample_by_tree': 0.9322694823034943, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:03<00:00, 15.41trial/s, best loss: -0.9114583333333334]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'learning_rate': 0.015858365192780624, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 4, 'colsample_bytree': 0.8259174419467931, 'colsample_bylevel': 0.8522099926467701, 'colsample_bynode': 0.8793876631648405, 'reg_alpha': 0.9168342498423954, 'reg_lambda': 3.5271765321495203, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(Xd_train, y_train)\n",
        "    y_pred = clf.predict(Xd_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(Xd_train, y_train)\n",
        "    y_pred = clf.predict(Xd_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params, cat_features=list(X.columns))\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params, categorical_feature=list(X.columns), feature_name=list(X.columns))\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params, enable_categorical=True, tree_method='hist')\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_xgboost = {\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)    \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "d_names = [\n",
        "          'AdaBoost',\n",
        "          'GradBoost',\n",
        "          ]\n",
        "c_names =   [      \n",
        "          'CatBoost',\n",
        "          'LightGBM',\n",
        "          'XGBoost'\n",
        "          ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Tictactoe Dataset ---------\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1.]\n",
            "Accuracy: 100.00% (0.00%)\n",
            "Execution Time: 183.96 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Tictactoe Dataset ---------\n",
            "[0.94791667 0.98958333 0.97916667 0.94791667 0.97916667 0.9375\n",
            " 0.98958333 0.94791667 0.96842105 0.94736842 0.92708333 0.97916667\n",
            " 0.95833333 0.96875    0.97916667 0.94791667 0.97916667 0.96875\n",
            " 0.94736842 0.92631579 0.97916667 0.96875    0.94791667 0.94791667\n",
            " 1.         0.97916667 0.96875    0.95833333 0.95789474 0.95789474\n",
            " 0.95833333 0.96875    0.94791667 0.9375     0.98958333 0.96875\n",
            " 0.95833333 0.95833333 0.93684211 0.96842105 0.9375     0.94791667\n",
            " 0.98958333 0.97916667 0.94791667 0.94791667 0.95833333 0.96875\n",
            " 0.97894737 0.97894737 0.96875    0.97916667 0.96875    0.95833333\n",
            " 0.9375     0.94791667 0.98958333 0.94791667 0.97894737 0.95789474\n",
            " 0.92708333 0.97916667 0.92708333 0.98958333 0.92708333 0.96875\n",
            " 0.97916667 0.98958333 0.95789474 0.94736842 0.95833333 1.\n",
            " 0.96875    0.96875    0.9375     0.9375     0.94791667 0.94791667\n",
            " 0.97894737 0.97894737 0.96875    0.97916667 0.95833333 0.98958333\n",
            " 0.88541667 0.94791667 0.98958333 0.94791667 0.95789474 0.94736842\n",
            " 0.95833333 0.95833333 0.96875    0.9375     0.97916667 0.97916667\n",
            " 0.97916667 0.9375     0.93684211 0.96842105]\n",
            "Accuracy: 96.14% (1.97%)\n",
            "Execution Time: 69.18 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Tictactoe Dataset ---------\n",
            "[1.         1.         1.         1.         1.         0.98958333\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         0.98958333 0.98958333 1.\n",
            " 1.         1.         0.98958333 1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         0.98958333 1.         1.         1.\n",
            " 1.         1.         0.98947368 1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         0.97894737\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         0.98958333 1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         0.98958333 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "Accuracy: 99.90% (0.35%)\n",
            "Execution Time: 1529.82 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Tictactoe Dataset ---------\n",
            "[0.97916667 0.95833333 0.95833333 0.96875    0.96875    0.96875\n",
            " 0.95833333 0.96875    0.98947368 0.96842105 0.98958333 0.96875\n",
            " 0.94791667 0.95833333 0.96875    0.92708333 0.95833333 1.\n",
            " 0.98947368 0.96842105 0.94791667 0.96875    0.97916667 0.92708333\n",
            " 0.97916667 0.95833333 0.9375     0.91666667 0.97894737 0.98947368\n",
            " 0.98958333 0.95833333 0.94791667 0.97916667 0.97916667 0.98958333\n",
            " 1.         0.95833333 0.93684211 0.96842105 0.95833333 0.9375\n",
            " 0.96875    0.94791667 1.         0.98958333 0.96875    0.9375\n",
            " 0.90526316 0.95789474 0.95833333 0.95833333 0.96875    0.98958333\n",
            " 0.92708333 0.98958333 0.96875    0.9375     0.89473684 0.93684211\n",
            " 0.96875    1.         0.90625    0.98958333 0.94791667 0.97916667\n",
            " 0.94791667 0.97916667 0.93684211 0.94736842 0.95833333 0.97916667\n",
            " 0.95833333 0.90625    0.94791667 0.97916667 0.98958333 0.95833333\n",
            " 0.95789474 0.94736842 0.94791667 0.98958333 0.94791667 0.94791667\n",
            " 0.96875    0.92708333 0.95833333 0.95833333 0.93684211 0.92631579\n",
            " 0.9375     0.97916667 0.96875    0.9375     1.         0.96875\n",
            " 0.95833333 0.97916667 0.95789474 0.94736842]\n",
            "Accuracy: 96.08% (2.27%)\n",
            "Execution Time: 4.19 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Tictactoe Dataset ---------\n",
            "[0.95833333 0.89583333 0.95833333 0.9375     0.90625    0.92708333\n",
            " 0.875      0.91666667 0.94736842 0.93684211 0.86458333 0.92708333\n",
            " 0.9375     0.97916667 0.97916667 0.92708333 0.92708333 0.92708333\n",
            " 0.90526316 0.92631579 0.88541667 0.91666667 0.92708333 0.90625\n",
            " 0.91666667 0.96875    0.90625    0.89583333 0.95789474 0.90526316\n",
            " 0.92708333 0.89583333 0.91666667 0.88541667 0.90625    0.95833333\n",
            " 0.92708333 0.90625    0.90526316 0.93684211 0.94791667 0.90625\n",
            " 0.9375     0.875      0.90625    0.92708333 0.9375     0.92708333\n",
            " 0.87368421 0.92631579 0.90625    0.92708333 0.91666667 0.90625\n",
            " 0.89583333 0.92708333 0.9375     0.90625    0.94736842 0.90526316\n",
            " 0.90625    0.91666667 0.90625    0.91666667 0.89583333 0.91666667\n",
            " 0.9375     0.91666667 0.96842105 0.94736842 0.94791667 0.89583333\n",
            " 0.92708333 0.88541667 0.89583333 0.91666667 0.9375     0.92708333\n",
            " 0.94736842 0.90526316 0.89583333 0.89583333 0.94791667 0.92708333\n",
            " 0.91666667 0.9375     0.94791667 0.89583333 0.94736842 0.88421053\n",
            " 0.91666667 0.94791667 0.92708333 0.91666667 0.95833333 0.88541667\n",
            " 0.9375     0.875      0.93684211 0.88421053]\n",
            "Accuracy: 92.05% (2.43%)\n",
            "Execution Time: 7.41 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "tictactoe_scores = []\n",
        "tictactoe_mean = []\n",
        "tictactoe_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in d_names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "        \n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, Xdummies, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    tictactoe_scores.append(results)\n",
        "    tictactoe_mean.append(results.mean()*100)\n",
        "    tictactoe_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Tictactoe Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')    \n",
        "         \n",
        "for algorithm_name in c_names:    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(cat_features=list(X.columns),\n",
        "                                n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(categorical_feature=list(X.columns),\n",
        "                            feature_name=list(X.columns),\n",
        "                            boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(enable_categorical=True,\n",
        "                            tree_method='hist',\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    tictactoe_scores.append(results)\n",
        "    tictactoe_mean.append(results.mean()*100)\n",
        "    tictactoe_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Tictactoe Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Tictactoe scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(tictactoe_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['TicTacToe'] = tictactoe_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['TicTacToe'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Liver**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "liver_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Liver\\Bupa.data', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = liver_df.iloc[:, :-1]\n",
        "y = liver_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since version 1.3.2 XGBoost needs target columns to start with 0 value\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:07<00:00,  1.36s/trial, best loss: -0.8695652173913043]\n",
            "Best hyperparameters for AdaBoost:\n",
            "{'n_estimators': 1150.0, 'learning_rate': 0.036566586849114326, 'max_depth': 6.0, 'max_features': 'sqrt', 'min_samples_leaf': 3.0, 'min_samples_split': 2.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:19<00:00,  2.53trial/s, best loss: -0.8115942028985508]\n",
            "Best hyperparameters for GradBoost:\n",
            "{'criterion': 'friedman_mse', 'max_features': None, 'n_estimators': 650, 'learning_rate': 0.06856648459048352, 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'min_impurity_decrease': 0.0, 'ccp_alpha': 0.0, 'random_state': 42}\n",
            "100%|██████████| 50/50 [01:04<00:00,  1.28s/trial, best loss: -0.7971014492753623]\n",
            "Best hyperparameters for CatBoost:\n",
            "{'n_estimators': 1000, 'learning_rate': 0.07885766008379519, 'min_child_samples': 8, 'max_depth': 2, 'reg_lambda': 2.215819236413667, 'silent': True, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:01<00:00, 46.32trial/s, best loss: -0.7391304347826086]\n",
            "Best hyperparameters for LightGBM:\n",
            "{'class_weight': 'balanced', 'boosting_type': 'goss', 'num_leaves': 50, 'learning_rate': 0.030743334125495195, 'min_child_samples': 20, 'reg_alpha': 1.2374175460929842, 'reg_lambda': 2.7904588669270254, 'colsample_by_tree': 0.6043264075687251, 'verbosity': -1, 'random_state': 42}\n",
            "100%|██████████| 50/50 [00:06<00:00,  7.85trial/s, best loss: -0.7971014492753623]\n",
            "Best hyperparameters for XGBoost:\n",
            "{'booster': 'dart', 'learning_rate': 0.04329402990235971, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 2, 'colsample_bytree': 0.8839036553414338, 'colsample_bylevel': 0.13572776354954574, 'colsample_bynode': 0.32883430164648214, 'reg_alpha': 0.43424116154739917, 'reg_lambda': 1.9753629991445285, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "from hyperopt.pyll import scope\n",
        "import warnings\n",
        "\n",
        "# Filter out the FutureWarning related to is_sparse\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
        "\n",
        "best_hyperparams = {\n",
        "    'AdaBoost': {},\n",
        "    'GradBoost': {},\n",
        "    'CatBoost': {},\n",
        "    'LightGBM': {},\n",
        "    'XGBoost': {}\n",
        "}\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "def optimize_adaboost(params):\n",
        "    estimator_params = params['estimator']\n",
        "    estimator = DecisionTreeClassifier(**estimator_params)\n",
        "\n",
        "    clf = AdaBoostClassifier(estimator=estimator, n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], random_state=params['random_state'])\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_gradientboost(params):\n",
        "    clf = GradientBoostingClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_catboost(params):\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_lightgbm(params):\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_xgboost(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return -accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Define the hyperparameter search space for each algorithm\n",
        "\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_adaboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'estimator': {\n",
        "        'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),  # Decision tree depth\n",
        "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 8, 2)),  # Min samples required to split\n",
        "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1)),  # Min samples required in a leaf node\n",
        "        'max_features': hp.choice('max_features', max_features_choices),\n",
        "    },\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "criterion_choices = ['friedman_mse', 'squared_error']\n",
        "max_features_choices = [None, 'sqrt', 'log2']\n",
        "space_gradientboost = {\n",
        "    'criterion': hp.choice('criterion', criterion_choices),\n",
        "    'max_features': hp.choice('max_features', max_features_choices),\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "    'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.0, 0.5, 0.1),\n",
        "    'min_impurity_decrease': hp.quniform('min_impurity_decrease', 0.0, 5, 1),\n",
        "    'ccp_alpha': hp.quniform('ccp_alpha', 0.0, 5, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "space_catboost = {\n",
        "    'n_estimators': 1 + scope.int(hp.quniform('n_estimators', 5, 1500, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'silent': True\n",
        "}\n",
        "\n",
        "class_weight_choices = ['balanced']\n",
        "boosting_type_choices = ['gbdt', 'dart', 'goss']\n",
        "space_lightgbm = {\n",
        "    'class_weight': hp.choice('class_weight', class_weight_choices),                                              \n",
        "    'boosting_type': hp.choice('boosting_type', boosting_type_choices),\n",
        "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 100, 5)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.1, 1.0),\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "booster_choices = ['gbtree', 'dart']\n",
        "space_xgboost = {\n",
        "    'booster': hp.choice('booster', booster_choices),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
        "    'gamma': scope.int(hp.quniform('gamma', 0, 10, 1)),\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 6, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1)),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 5.0),\n",
        "    'verbosity': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Define optimization functions and algorithm names\n",
        "optimizers = [\n",
        "    (optimize_adaboost, space_adaboost, 'AdaBoost'),\n",
        "    (optimize_gradientboost, space_gradientboost, 'GradBoost'),\n",
        "    (optimize_catboost, space_catboost, 'CatBoost'),\n",
        "    (optimize_lightgbm, space_lightgbm, 'LightGBM'),\n",
        "    (optimize_xgboost, space_xgboost, 'XGBoost')\n",
        "]\n",
        "\n",
        "\n",
        "# Performing hyperparameter tuning for each algorithm\n",
        "\n",
        "rstate=np.random.default_rng(42)\n",
        "\n",
        "for optimize_fn, space, algorithm_name in optimizers:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best AdaBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': best['n_estimators'],\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': best['max_depth'],\n",
        "            'max_features': max_features_label,\n",
        "            'min_samples_leaf': best['min_samples_leaf'],\n",
        "            'min_samples_split': best['min_samples_split'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "\n",
        "\n",
        "        # Map the choice labels        \n",
        "        criterion_label = criterion_choices[best['criterion']]\n",
        "        max_features_label = max_features_choices[best['max_features']]\n",
        "\n",
        "        # Store the best GradBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'criterion': criterion_label,\n",
        "            'max_features': max_features_label,\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'min_weight_fraction_leaf': best['min_weight_fraction_leaf'],\n",
        "            'min_impurity_decrease': best['min_impurity_decrease'],\n",
        "            'ccp_alpha': best['ccp_alpha'],\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])           \n",
        "    \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Store the best CatBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'silent': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'LightGBM':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        class_weight_label = class_weight_choices[best['class_weight']]\n",
        "        boosting_type_label = boosting_type_choices[best['boosting_type']]\n",
        "\n",
        "        # Store the best LightGBM hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'class_weight': class_weight_label,\n",
        "            'boosting_type': boosting_type_label,\n",
        "            'num_leaves': int(best['num_leaves']),\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'min_child_samples': int(best['min_child_samples']),\n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],\n",
        "            'colsample_by_tree': best['colsample_by_tree'],\n",
        "            'verbosity': -1,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])\n",
        "\n",
        "    if algorithm_name == 'XGBoost':\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=optimize_fn, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=rstate)\n",
        "        \n",
        "        # Map the choice labels\n",
        "        booster_label = booster_choices[best['booster']]        \n",
        " \n",
        "        # Store the best XGBoost hyperparameters\n",
        "        best_hyperparams[algorithm_name] = {\n",
        "            'booster': booster_label,\n",
        "            'learning_rate': best['learning_rate'],\n",
        "            'gamma': int(best['gamma']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_child_weight': int(best['min_child_weight']),\n",
        "            'colsample_bytree': best['colsample_bytree'],\n",
        "            'colsample_bylevel': best['colsample_bylevel'],\n",
        "            'colsample_bynode': best['colsample_bynode'],            \n",
        "            'reg_alpha': best['reg_alpha'],\n",
        "            'reg_lambda': best['reg_lambda'],            \n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters for {algorithm_name}:\")\n",
        "        print(best_hyperparams[algorithm_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['AdaBoost', 'GradBoost', 'CatBoost', 'LightGBM', 'XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- AdaBoost on Liver Dataset ---------\n",
            "[0.77142857 0.74285714 0.71428571 0.71428571 0.77142857 0.88235294\n",
            " 0.67647059 0.64705882 0.67647059 0.61764706 0.65714286 0.57142857\n",
            " 0.68571429 0.8        0.68571429 0.70588235 0.70588235 0.76470588\n",
            " 0.73529412 0.67647059 0.65714286 0.71428571 0.8        0.74285714\n",
            " 0.77142857 0.70588235 0.79411765 0.73529412 0.70588235 0.67647059\n",
            " 0.77142857 0.77142857 0.74285714 0.82857143 0.74285714 0.79411765\n",
            " 0.64705882 0.70588235 0.70588235 0.67647059 0.74285714 0.8\n",
            " 0.65714286 0.68571429 0.77142857 0.73529412 0.73529412 0.64705882\n",
            " 0.67647059 0.70588235 0.65714286 0.77142857 0.74285714 0.62857143\n",
            " 0.77142857 0.70588235 0.70588235 0.70588235 0.58823529 0.58823529\n",
            " 0.74285714 0.77142857 0.6        0.57142857 0.68571429 0.73529412\n",
            " 0.61764706 0.82352941 0.85294118 0.67647059 0.8        0.65714286\n",
            " 0.82857143 0.74285714 0.65714286 0.79411765 0.76470588 0.70588235\n",
            " 0.67647059 0.73529412 0.8        0.68571429 0.71428571 0.82857143\n",
            " 0.68571429 0.70588235 0.76470588 0.64705882 0.73529412 0.64705882\n",
            " 0.82857143 0.74285714 0.74285714 0.6        0.65714286 0.82352941\n",
            " 0.64705882 0.76470588 0.61764706 0.67647059]\n",
            "Accuracy: 71.67% (6.58%)\n",
            "Execution Time: 176.18 seconds\n",
            "------------------------------\n",
            "--------- GradBoost on Liver Dataset ---------\n",
            "[0.88571429 0.71428571 0.71428571 0.71428571 0.74285714 0.82352941\n",
            " 0.67647059 0.70588235 0.67647059 0.67647059 0.6        0.68571429\n",
            " 0.65714286 0.77142857 0.68571429 0.67647059 0.76470588 0.61764706\n",
            " 0.64705882 0.79411765 0.62857143 0.68571429 0.71428571 0.71428571\n",
            " 0.74285714 0.79411765 0.82352941 0.61764706 0.67647059 0.73529412\n",
            " 0.71428571 0.8        0.71428571 0.8        0.77142857 0.64705882\n",
            " 0.64705882 0.70588235 0.70588235 0.58823529 0.68571429 0.71428571\n",
            " 0.74285714 0.68571429 0.74285714 0.76470588 0.64705882 0.61764706\n",
            " 0.64705882 0.70588235 0.68571429 0.8        0.65714286 0.65714286\n",
            " 0.77142857 0.67647059 0.67647059 0.70588235 0.73529412 0.64705882\n",
            " 0.62857143 0.74285714 0.62857143 0.62857143 0.6        0.70588235\n",
            " 0.44117647 0.91176471 0.70588235 0.70588235 0.82857143 0.65714286\n",
            " 0.68571429 0.71428571 0.68571429 0.73529412 0.73529412 0.61764706\n",
            " 0.61764706 0.70588235 0.71428571 0.71428571 0.65714286 0.8\n",
            " 0.77142857 0.64705882 0.55882353 0.61764706 0.64705882 0.61764706\n",
            " 0.68571429 0.74285714 0.8        0.48571429 0.6        0.79411765\n",
            " 0.73529412 0.64705882 0.67647059 0.76470588]\n",
            "Accuracy: 69.78% (7.34%)\n",
            "Execution Time: 53.18 seconds\n",
            "------------------------------\n",
            "--------- CatBoost on Liver Dataset ---------\n",
            "[0.85714286 0.77142857 0.71428571 0.68571429 0.65714286 0.82352941\n",
            " 0.58823529 0.55882353 0.61764706 0.70588235 0.74285714 0.6\n",
            " 0.62857143 0.71428571 0.65714286 0.64705882 0.73529412 0.61764706\n",
            " 0.73529412 0.73529412 0.65714286 0.62857143 0.65714286 0.74285714\n",
            " 0.68571429 0.67647059 0.79411765 0.64705882 0.73529412 0.70588235\n",
            " 0.77142857 0.68571429 0.68571429 0.77142857 0.82857143 0.88235294\n",
            " 0.61764706 0.64705882 0.70588235 0.64705882 0.77142857 0.68571429\n",
            " 0.74285714 0.65714286 0.77142857 0.67647059 0.70588235 0.61764706\n",
            " 0.70588235 0.70588235 0.71428571 0.68571429 0.68571429 0.65714286\n",
            " 0.8        0.73529412 0.70588235 0.76470588 0.67647059 0.64705882\n",
            " 0.74285714 0.68571429 0.62857143 0.6        0.77142857 0.73529412\n",
            " 0.52941176 0.82352941 0.85294118 0.61764706 0.77142857 0.57142857\n",
            " 0.71428571 0.82857143 0.71428571 0.79411765 0.73529412 0.58823529\n",
            " 0.64705882 0.64705882 0.74285714 0.74285714 0.6        0.77142857\n",
            " 0.65714286 0.61764706 0.76470588 0.64705882 0.64705882 0.70588235\n",
            " 0.65714286 0.65714286 0.74285714 0.6        0.71428571 0.85294118\n",
            " 0.64705882 0.64705882 0.55882353 0.79411765]\n",
            "Accuracy: 69.85% (7.31%)\n",
            "Execution Time: 97.87 seconds\n",
            "------------------------------\n",
            "--------- LightGBM on Liver Dataset ---------\n",
            "[0.8        0.74285714 0.65714286 0.65714286 0.6        0.73529412\n",
            " 0.55882353 0.73529412 0.58823529 0.76470588 0.6        0.68571429\n",
            " 0.6        0.8        0.77142857 0.73529412 0.79411765 0.64705882\n",
            " 0.73529412 0.73529412 0.6        0.54285714 0.68571429 0.68571429\n",
            " 0.82857143 0.82352941 0.85294118 0.55882353 0.76470588 0.67647059\n",
            " 0.71428571 0.68571429 0.71428571 0.82857143 0.71428571 0.76470588\n",
            " 0.52941176 0.73529412 0.79411765 0.64705882 0.62857143 0.77142857\n",
            " 0.71428571 0.68571429 0.77142857 0.73529412 0.70588235 0.55882353\n",
            " 0.58823529 0.70588235 0.65714286 0.77142857 0.65714286 0.54285714\n",
            " 0.82857143 0.79411765 0.82352941 0.76470588 0.70588235 0.64705882\n",
            " 0.6        0.8        0.71428571 0.74285714 0.68571429 0.58823529\n",
            " 0.61764706 0.85294118 0.67647059 0.61764706 0.68571429 0.68571429\n",
            " 0.68571429 0.71428571 0.74285714 0.79411765 0.70588235 0.58823529\n",
            " 0.58823529 0.79411765 0.74285714 0.74285714 0.8        0.8\n",
            " 0.62857143 0.70588235 0.58823529 0.76470588 0.58823529 0.64705882\n",
            " 0.62857143 0.68571429 0.74285714 0.51428571 0.71428571 0.73529412\n",
            " 0.70588235 0.67647059 0.76470588 0.58823529]\n",
            "Accuracy: 69.79% (8.14%)\n",
            "Execution Time: 1.77 seconds\n",
            "------------------------------\n",
            "--------- XGBoost on Liver Dataset ---------\n",
            "[0.91428571 0.8        0.71428571 0.71428571 0.62857143 0.79411765\n",
            " 0.64705882 0.76470588 0.70588235 0.73529412 0.71428571 0.74285714\n",
            " 0.65714286 0.82857143 0.77142857 0.70588235 0.85294118 0.67647059\n",
            " 0.67647059 0.79411765 0.77142857 0.68571429 0.82857143 0.71428571\n",
            " 0.85714286 0.76470588 0.79411765 0.61764706 0.67647059 0.73529412\n",
            " 0.77142857 0.77142857 0.71428571 0.85714286 0.77142857 0.76470588\n",
            " 0.61764706 0.76470588 0.73529412 0.73529412 0.8        0.74285714\n",
            " 0.71428571 0.74285714 0.71428571 0.85294118 0.67647059 0.67647059\n",
            " 0.73529412 0.76470588 0.77142857 0.8        0.74285714 0.65714286\n",
            " 0.8        0.76470588 0.79411765 0.70588235 0.76470588 0.70588235\n",
            " 0.74285714 0.85714286 0.71428571 0.71428571 0.71428571 0.70588235\n",
            " 0.61764706 0.85294118 0.88235294 0.67647059 0.82857143 0.65714286\n",
            " 0.82857143 0.82857143 0.74285714 0.76470588 0.73529412 0.61764706\n",
            " 0.61764706 0.70588235 0.77142857 0.74285714 0.8        0.82857143\n",
            " 0.74285714 0.70588235 0.79411765 0.70588235 0.61764706 0.76470588\n",
            " 0.68571429 0.77142857 0.85714286 0.6        0.68571429 0.88235294\n",
            " 0.76470588 0.76470588 0.76470588 0.70588235]\n",
            "Accuracy: 74.48% (6.77%)\n",
            "Execution Time: 16.48 seconds\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "liver_scores = []\n",
        "liver_mean = []\n",
        "liver_std = []\n",
        "model_names = []\n",
        "execution_times = []\n",
        "\n",
        "for algorithm_name in names:\n",
        "    if algorithm_name == 'AdaBoost':\n",
        "        base_estimator = DecisionTreeClassifier(max_depth=int(best_hyperparams[algorithm_name]['max_depth']),\n",
        "                                                max_features=best_hyperparams[algorithm_name]['max_features'],\n",
        "                                                min_samples_leaf=int(best_hyperparams[algorithm_name]['min_samples_leaf']),\n",
        "                                                min_samples_split=int(best_hyperparams[algorithm_name]['min_samples_split']))\n",
        "\n",
        "        clf = AdaBoostClassifier(estimator=base_estimator, \n",
        "                                n_estimators=int(best_hyperparams[algorithm_name]['n_estimators']), \n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                random_state=42)    \n",
        "\n",
        "    if algorithm_name == 'GradBoost':\n",
        "        clf = GradientBoostingClassifier(criterion=best_hyperparams[algorithm_name]['criterion'], \n",
        "                                        max_features=best_hyperparams[algorithm_name]['max_features'], \n",
        "                                        n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                        learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                        max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                        min_samples_split=best_hyperparams[algorithm_name]['min_samples_split'],\n",
        "                                        min_samples_leaf=best_hyperparams[algorithm_name]['min_samples_leaf'],\n",
        "                                        min_weight_fraction_leaf=best_hyperparams[algorithm_name]['min_weight_fraction_leaf'],\n",
        "                                        min_impurity_decrease=best_hyperparams[algorithm_name]['min_impurity_decrease'],\n",
        "                                        ccp_alpha=best_hyperparams[algorithm_name]['ccp_alpha'],\n",
        "                                        random_state=42)\n",
        "         \n",
        "    if algorithm_name == 'CatBoost':\n",
        "        clf = CatBoostClassifier(n_estimators=best_hyperparams[algorithm_name]['n_estimators'],\n",
        "                                learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                                min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                                max_depth=best_hyperparams[algorithm_name]['max_depth'],\n",
        "                                reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                                silent=True,\n",
        "                                random_state=42)                        \n",
        "        \n",
        "    if algorithm_name == 'LightGBM':\n",
        "        clf = LGBMClassifier(boosting_type=best_hyperparams[algorithm_name]['boosting_type'], \n",
        "                            class_weight=best_hyperparams[algorithm_name]['class_weight'], \n",
        "                            colsample_by_tree=best_hyperparams[algorithm_name]['colsample_by_tree'],\n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            min_child_samples=best_hyperparams[algorithm_name]['min_child_samples'],\n",
        "                            num_leaves=best_hyperparams[algorithm_name]['num_leaves'],\n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=-1,\n",
        "                            random_state=42)\n",
        "               \n",
        "    if algorithm_name == 'XGBoost':\n",
        "        clf = XGBClassifier(booster=best_hyperparams[algorithm_name]['booster'], \n",
        "                            learning_rate=best_hyperparams[algorithm_name]['learning_rate'],\n",
        "                            gamma=best_hyperparams[algorithm_name]['gamma'], \n",
        "                            max_depth=best_hyperparams[algorithm_name]['max_depth'], \n",
        "                            min_child_weight=best_hyperparams[algorithm_name]['min_child_weight'],\n",
        "                            colsample_bytree=best_hyperparams[algorithm_name]['colsample_bytree'],\n",
        "                            colsample_bylevel=best_hyperparams[algorithm_name]['colsample_bylevel'],\n",
        "                            colsample_bynode=best_hyperparams[algorithm_name]['colsample_bynode'],                            \n",
        "                            reg_alpha=best_hyperparams[algorithm_name]['reg_alpha'],\n",
        "                            reg_lambda=best_hyperparams[algorithm_name]['reg_lambda'],\n",
        "                            verbosity=0,\n",
        "                            random_state=42)\n",
        "\n",
        "    start_time = time.time()    \n",
        "    results = cross_val_score(clf, X, y, cv=rskf)\n",
        "    end_time = time.time()\n",
        "    liver_scores.append(results)\n",
        "    liver_mean.append(results.mean()*100)\n",
        "    liver_std.append(results.std()*100)\n",
        "    model_names.append(algorithm_name)\n",
        "    execution_time = end_time - start_time  \n",
        "    execution_times.append(execution_time)\n",
        "    print(f'--------- {algorithm_name} on Liver Dataset ---------')\n",
        "    print(results)\n",
        "    print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
        "    print(f'Execution Time: {execution_time:.2f} seconds')\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# algorithm comparison\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different Liver scores')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ylim(0.3, 1)\n",
        "plt.boxplot(liver_scores, showmeans=True)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_results['Liver'] = liver_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Algo_results' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32me:\\Cursos\\MestradoCienciaComputação\\Seminario\\Code\\AlgorithmComparison\\AlgorithmComparison\\AlgorithmComparison3.ipynb Cell 147\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Cursos/MestradoCienciaComputa%C3%A7%C3%A3o/Seminario/Code/AlgorithmComparison/AlgorithmComparison/AlgorithmComparison3.ipynb#Y266sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Algo_results\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Algo_results' is not defined"
          ]
        }
      ],
      "source": [
        "Algo_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_results_tr = Algo_results.set_index('Names').T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_results_tr.to_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Algo_Results\\TunedAlgoResults.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Names</th>\n",
              "      <th>AdaBoost</th>\n",
              "      <th>GradBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>XGBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Wine</th>\n",
              "      <td>96.552288</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.797386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <td>97.159847</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>96.792626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sonar</th>\n",
              "      <td>86.347619</td>\n",
              "      <td>78.145238</td>\n",
              "      <td>87.076190</td>\n",
              "      <td>82.361905</td>\n",
              "      <td>83.802381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ionosphere</th>\n",
              "      <td>93.815873</td>\n",
              "      <td>90.854762</td>\n",
              "      <td>93.815079</td>\n",
              "      <td>92.849206</td>\n",
              "      <td>92.960317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bupa</th>\n",
              "      <td>71.669748</td>\n",
              "      <td>69.783193</td>\n",
              "      <td>69.846218</td>\n",
              "      <td>69.794118</td>\n",
              "      <td>74.475630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pima</th>\n",
              "      <td>76.101504</td>\n",
              "      <td>76.426863</td>\n",
              "      <td>75.527683</td>\n",
              "      <td>75.920711</td>\n",
              "      <td>75.334074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Heart</th>\n",
              "      <td>83.111111</td>\n",
              "      <td>80.444444</td>\n",
              "      <td>84.481481</td>\n",
              "      <td>82.851852</td>\n",
              "      <td>84.518519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TicTacToe</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>96.137281</td>\n",
              "      <td>99.895504</td>\n",
              "      <td>96.083772</td>\n",
              "      <td>92.046820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Liver</th>\n",
              "      <td>71.669748</td>\n",
              "      <td>69.783193</td>\n",
              "      <td>69.846218</td>\n",
              "      <td>69.794118</td>\n",
              "      <td>74.475630</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Names            AdaBoost  GradBoost   CatBoost   LightGBM    XGBoost\n",
              "Wine            96.552288  98.075163  97.967320  97.120915  97.797386\n",
              "Breast_Cancer   97.159847  96.646633  97.378303  97.334612  96.792626\n",
              "Sonar           86.347619  78.145238  87.076190  82.361905  83.802381\n",
              "Ionosphere      93.815873  90.854762  93.815079  92.849206  92.960317\n",
              "Bupa            71.669748  69.783193  69.846218  69.794118  74.475630\n",
              "Pima            76.101504  76.426863  75.527683  75.920711  75.334074\n",
              "Heart           83.111111  80.444444  84.481481  82.851852  84.518519\n",
              "TicTacToe      100.000000  96.137281  99.895504  96.083772  92.046820\n",
              "Liver           71.669748  69.783193  69.846218  69.794118  74.475630"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Tuned_Algo_results_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "Algo_time_results['Liver'] = pd.Series(execution_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Wine</th>\n",
              "      <th>Breast_Cancer</th>\n",
              "      <th>Sonar</th>\n",
              "      <th>Ionosphere</th>\n",
              "      <th>Bupa</th>\n",
              "      <th>Pima</th>\n",
              "      <th>Heart</th>\n",
              "      <th>TicTacToe</th>\n",
              "      <th>Liver</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>22.984195</td>\n",
              "      <td>61.192441</td>\n",
              "      <td>684.584288</td>\n",
              "      <td>12.880605</td>\n",
              "      <td>177.651713</td>\n",
              "      <td>251.894216</td>\n",
              "      <td>73.763926</td>\n",
              "      <td>183.959124</td>\n",
              "      <td>176.182806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GradBoost</td>\n",
              "      <td>15.547652</td>\n",
              "      <td>21.186948</td>\n",
              "      <td>8.568928</td>\n",
              "      <td>103.116306</td>\n",
              "      <td>53.798335</td>\n",
              "      <td>12.604379</td>\n",
              "      <td>66.210400</td>\n",
              "      <td>69.184264</td>\n",
              "      <td>53.183083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>98.037048</td>\n",
              "      <td>43.536956</td>\n",
              "      <td>760.839417</td>\n",
              "      <td>198.149810</td>\n",
              "      <td>104.234700</td>\n",
              "      <td>14.871501</td>\n",
              "      <td>17.193328</td>\n",
              "      <td>1529.824171</td>\n",
              "      <td>97.871292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>3.596812</td>\n",
              "      <td>1.555351</td>\n",
              "      <td>1.454328</td>\n",
              "      <td>3.212697</td>\n",
              "      <td>1.904428</td>\n",
              "      <td>2.167165</td>\n",
              "      <td>0.829186</td>\n",
              "      <td>4.189671</td>\n",
              "      <td>1.766400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>41.520385</td>\n",
              "      <td>19.884495</td>\n",
              "      <td>22.485083</td>\n",
              "      <td>5.808953</td>\n",
              "      <td>20.480817</td>\n",
              "      <td>3.699006</td>\n",
              "      <td>3.654459</td>\n",
              "      <td>7.410980</td>\n",
              "      <td>16.477840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Names       Wine  Breast_Cancer       Sonar  Ionosphere        Bupa  \\\n",
              "0   AdaBoost  22.984195      61.192441  684.584288   12.880605  177.651713   \n",
              "1  GradBoost  15.547652      21.186948    8.568928  103.116306   53.798335   \n",
              "2   CatBoost  98.037048      43.536956  760.839417  198.149810  104.234700   \n",
              "3   LightGBM   3.596812       1.555351    1.454328    3.212697    1.904428   \n",
              "4    XGBoost  41.520385      19.884495   22.485083    5.808953   20.480817   \n",
              "\n",
              "         Pima      Heart    TicTacToe       Liver  \n",
              "0  251.894216  73.763926   183.959124  176.182806  \n",
              "1   12.604379  66.210400    69.184264   53.183083  \n",
              "2   14.871501  17.193328  1529.824171   97.871292  \n",
              "3    2.167165   0.829186     4.189671    1.766400  \n",
              "4    3.699006   3.654459     7.410980   16.477840  "
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Algo_time_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_time_results_tr = Algo_time_results.set_index('Names').T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_time_results_tr.to_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Algo_Results\\TunedAlgoTimeResults.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Friedman's Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-posthocs in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (0.7.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (1.25.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (1.11.2)\n",
            "Requirement already satisfied: statsmodels in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (0.14.0)\n",
            "Requirement already satisfied: pandas>=0.20.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (2.1.0)\n",
            "Requirement already satisfied: seaborn in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (0.12.2)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from scikit-posthocs) (3.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.20.0->scikit-posthocs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.20.0->scikit-posthocs) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from pandas>=0.20.0->scikit-posthocs) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (10.0.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from matplotlib->scikit-posthocs) (6.0.1)\n",
            "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from statsmodels->scikit-posthocs) (0.5.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->scikit-posthocs) (3.16.2)\n",
            "Requirement already satisfied: six in c:\\users\\erikc\\anaconda3\\envs\\algocomparison\\lib\\site-packages (from patsy>=0.5.2->statsmodels->scikit-posthocs) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-posthocs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scikit_posthocs as sp\n",
        "from scipy.stats import friedmanchisquare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_results_tr = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Algo_Results\\TunedAlgoResults.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AdaBoost</th>\n",
              "      <th>GradBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>XGBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>96.552288</td>\n",
              "      <td>98.075163</td>\n",
              "      <td>97.967320</td>\n",
              "      <td>97.120915</td>\n",
              "      <td>97.797386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>97.159847</td>\n",
              "      <td>96.646633</td>\n",
              "      <td>97.378303</td>\n",
              "      <td>97.334612</td>\n",
              "      <td>96.792626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>86.347619</td>\n",
              "      <td>78.145238</td>\n",
              "      <td>87.076190</td>\n",
              "      <td>82.361905</td>\n",
              "      <td>83.802381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>93.815873</td>\n",
              "      <td>90.854762</td>\n",
              "      <td>93.815079</td>\n",
              "      <td>92.849206</td>\n",
              "      <td>92.960317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>71.669748</td>\n",
              "      <td>69.783193</td>\n",
              "      <td>69.846218</td>\n",
              "      <td>69.794118</td>\n",
              "      <td>74.475630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>76.101504</td>\n",
              "      <td>76.426863</td>\n",
              "      <td>75.527683</td>\n",
              "      <td>75.920711</td>\n",
              "      <td>75.334074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>83.111111</td>\n",
              "      <td>80.444444</td>\n",
              "      <td>84.481481</td>\n",
              "      <td>82.851852</td>\n",
              "      <td>84.518519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>96.137281</td>\n",
              "      <td>99.895504</td>\n",
              "      <td>96.083772</td>\n",
              "      <td>92.046820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>71.669748</td>\n",
              "      <td>69.783193</td>\n",
              "      <td>69.846218</td>\n",
              "      <td>69.794118</td>\n",
              "      <td>74.475630</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     AdaBoost  GradBoost   CatBoost   LightGBM    XGBoost\n",
              "0   96.552288  98.075163  97.967320  97.120915  97.797386\n",
              "1   97.159847  96.646633  97.378303  97.334612  96.792626\n",
              "2   86.347619  78.145238  87.076190  82.361905  83.802381\n",
              "3   93.815873  90.854762  93.815079  92.849206  92.960317\n",
              "4   71.669748  69.783193  69.846218  69.794118  74.475630\n",
              "5   76.101504  76.426863  75.527683  75.920711  75.334074\n",
              "6   83.111111  80.444444  84.481481  82.851852  84.518519\n",
              "7  100.000000  96.137281  99.895504  96.083772  92.046820\n",
              "8   71.669748  69.783193  69.846218  69.794118  74.475630"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Tuned_Algo_results_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "stat, p_value = friedmanchisquare(Tuned_Algo_results_tr['AdaBoost'], Tuned_Algo_results_tr['GradBoost'], Tuned_Algo_results_tr['CatBoost'], Tuned_Algo_results_tr['LightGBM'], Tuned_Algo_results_tr['XGBoost'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.08228404018011351"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are no significant differences among the models.\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print('There are significant differences among the models.')\n",
        "else:\n",
        "    print('There are no significant differences among the models.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Algorithms running time Friedman's Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tuned_Algo_results_tr = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Algo_Results\\TunedAlgoTimeResults.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AdaBoost</th>\n",
              "      <th>GradBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>XGBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22.984195</td>\n",
              "      <td>15.547652</td>\n",
              "      <td>98.037048</td>\n",
              "      <td>3.596812</td>\n",
              "      <td>41.520385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>61.192441</td>\n",
              "      <td>21.186948</td>\n",
              "      <td>43.536956</td>\n",
              "      <td>1.555351</td>\n",
              "      <td>19.884495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>684.584288</td>\n",
              "      <td>8.568928</td>\n",
              "      <td>760.839417</td>\n",
              "      <td>1.454328</td>\n",
              "      <td>22.485083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12.880605</td>\n",
              "      <td>103.116306</td>\n",
              "      <td>198.149810</td>\n",
              "      <td>3.212697</td>\n",
              "      <td>5.808953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>177.651713</td>\n",
              "      <td>53.798335</td>\n",
              "      <td>104.234700</td>\n",
              "      <td>1.904428</td>\n",
              "      <td>20.480817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>251.894216</td>\n",
              "      <td>12.604379</td>\n",
              "      <td>14.871501</td>\n",
              "      <td>2.167165</td>\n",
              "      <td>3.699006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>73.763926</td>\n",
              "      <td>66.210400</td>\n",
              "      <td>17.193328</td>\n",
              "      <td>0.829186</td>\n",
              "      <td>3.654459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>183.959124</td>\n",
              "      <td>69.184264</td>\n",
              "      <td>1529.824171</td>\n",
              "      <td>4.189671</td>\n",
              "      <td>7.410980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>176.182806</td>\n",
              "      <td>53.183083</td>\n",
              "      <td>97.871292</td>\n",
              "      <td>1.766400</td>\n",
              "      <td>16.477840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     AdaBoost   GradBoost     CatBoost  LightGBM    XGBoost\n",
              "0   22.984195   15.547652    98.037048  3.596812  41.520385\n",
              "1   61.192441   21.186948    43.536956  1.555351  19.884495\n",
              "2  684.584288    8.568928   760.839417  1.454328  22.485083\n",
              "3   12.880605  103.116306   198.149810  3.212697   5.808953\n",
              "4  177.651713   53.798335   104.234700  1.904428  20.480817\n",
              "5  251.894216   12.604379    14.871501  2.167165   3.699006\n",
              "6   73.763926   66.210400    17.193328  0.829186   3.654459\n",
              "7  183.959124   69.184264  1529.824171  4.189671   7.410980\n",
              "8  176.182806   53.183083    97.871292  1.766400  16.477840"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Tuned_Algo_results_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "stat, p_value = friedmanchisquare(Tuned_Algo_results_tr['AdaBoost'], Tuned_Algo_results_tr['GradBoost'], Tuned_Algo_results_tr['CatBoost'], Tuned_Algo_results_tr['LightGBM'], Tuned_Algo_results_tr['XGBoost'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8.583811686749627e-06"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are significant running time differences among the algorithm.\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print('There are significant running time differences among the algorithm.')\n",
        "else:\n",
        "    print('There are no significant running time differences among the algorithm.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Algorithms running time Nemenyi test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "nemenyi_results = sp.posthoc_nemenyi_friedman(a=Tuned_Algo_results_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AdaBoost</th>\n",
              "      <th>GradBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>XGBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AdaBoost</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.381128</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.056476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GradBoost</th>\n",
              "      <td>0.381128</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.381128</td>\n",
              "      <td>0.056476</td>\n",
              "      <td>0.897740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CatBoost</th>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.381128</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.056476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LightGBM</th>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.056476</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.381128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost</th>\n",
              "      <td>0.056476</td>\n",
              "      <td>0.897740</td>\n",
              "      <td>0.056476</td>\n",
              "      <td>0.381128</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           AdaBoost  GradBoost  CatBoost  LightGBM   XGBoost\n",
              "AdaBoost   1.000000   0.381128  0.900000  0.001000  0.056476\n",
              "GradBoost  0.381128   1.000000  0.381128  0.056476  0.897740\n",
              "CatBoost   0.900000   0.381128  1.000000  0.001000  0.056476\n",
              "LightGBM   0.001000   0.056476  0.001000  1.000000  0.381128\n",
              "XGBoost    0.056476   0.897740  0.056476  0.381128  1.000000"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nemenyi_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Algorithm 1 and 2 are not significantly different (p-value = 0.3811).\n",
            "Algorithm 1 and 3 are not significantly different (p-value = 0.9000).\n",
            "Algorithm 1 and 4 are significantly different (p-value = 0.0010).\n",
            "Algorithm 1 and 5 are not significantly different (p-value = 0.0565).\n",
            "Algorithm 2 and 3 are not significantly different (p-value = 0.3811).\n",
            "Algorithm 2 and 4 are not significantly different (p-value = 0.0565).\n",
            "Algorithm 2 and 5 are not significantly different (p-value = 0.8977).\n",
            "Algorithm 3 and 4 are significantly different (p-value = 0.0010).\n",
            "Algorithm 3 and 5 are not significantly different (p-value = 0.0565).\n",
            "Algorithm 4 and 5 are not significantly different (p-value = 0.3811).\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.05\n",
        "\n",
        "# Print p-values for all pairs of models\n",
        "for i in range(nemenyi_results.shape[0]):\n",
        "    for j in range(i + 1, nemenyi_results.shape[1]):\n",
        "        model1 = i + 1\n",
        "        model2 = j + 1\n",
        "        p_value = nemenyi_results.iloc[i, j]\n",
        "\n",
        "        if p_value < alpha:\n",
        "            print(f\"Algorithm {model1} and {model2} are significantly different (p-value = {p_value:.4f}).\")\n",
        "        else:\n",
        "            print(f\"Algorithm {model1} and {model2} are not significantly different (p-value = {p_value:.4f}).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Comparison between the untuned and tuned algorithm performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "untuned_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Algo_Results\\AlgoResults.csv')\n",
        "tuned_df = pd.read_csv('E:\\Cursos\\MestradoCienciaComputação\\Seminario\\Datasets\\Algo_Results\\TunedAlgoResults.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9, 5)\n",
            "(9, 5)\n"
          ]
        }
      ],
      "source": [
        "print(untuned_df.shape)\n",
        "print(tuned_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets = ['Wine', 'Breast Cancer', 'Sonar', 'Ionosphere', 'Bupa', 'Pima', 'Heart', 'TicTacToe', 'Liver']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **AdaBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "AdaBoost_before_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    AdaBoost_value = untuned_df.iloc[i, :]['AdaBoost']\n",
        "    AdaBoost_before_array.append(AdaBoost_value)\n",
        "\n",
        "AdaBoost_before = np.array(AdaBoost_before_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([89.83006536, 95.80051151, 83.18809524, 93.02777778, 72.25462185,\n",
              "       75.11859193, 79.81481481, 77.3814693 , 72.25462185])"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "AdaBoost_before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "AdaBoost_after_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    AdaBoost_value = tuned_df.iloc[i, :]['AdaBoost']\n",
        "    AdaBoost_after_array.append(AdaBoost_value)\n",
        "\n",
        "AdaBoost_after = np.array(AdaBoost_after_array)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 96.55228758,  97.15984655,  86.34761905,  93.81587302,\n",
              "        71.6697479 ,  76.10150376,  83.11111111, 100.        ,\n",
              "        71.6697479 ])"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "AdaBoost_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistic, p_value = stats.wilcoxon(AdaBoost_before, AdaBoost_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Statistic: 3.0\n",
            "P-value: 0.01953125\n",
            "Reject the null hypothesis: There is a significant difference between AdaBoost algorithm.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Statistic: {statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f'Reject the null hypothesis: There is a significant difference between AdaBoost algorithm.')\n",
        "else:\n",
        "    print(f'Fail to reject the null hypothesis: There is no significant difference between AdaBoost algorithm.')      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **GradBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "GradBoost_before_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    GradBoost_value = untuned_df.iloc[i, :]['GradBoost']\n",
        "    GradBoost_before_array.append(GradBoost_value)\n",
        "\n",
        "GradBoost_before = np.array(GradBoost_before_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([95.01633987, 96.50063939, 83.99285714, 92.97222222, 72.32268908,\n",
              "       76.23615858, 80.07407407, 92.07817982, 72.23613445])"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GradBoost_before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "GradBoost_after_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    GradBoost_value = tuned_df.iloc[i, :]['GradBoost']\n",
        "    GradBoost_after_array.append(GradBoost_value)\n",
        "\n",
        "GradBoost_after = np.array(GradBoost_after_array)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([98.0751634 , 96.64663257, 78.1452381 , 90.8547619 , 69.78319328,\n",
              "       76.42686261, 80.44444444, 96.1372807 , 69.78319328])"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GradBoost_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistic, p_value = stats.wilcoxon(GradBoost_before, GradBoost_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Statistic: 21.0\n",
            "P-value: 0.91015625\n",
            "Fail to reject the null hypothesis: There is no significant difference between GradBoost algorithm.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Statistic: {statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f'Reject the null hypothesis: There is a significant difference between GradBoost algorithm.')\n",
        "else:\n",
        "    print(f'Fail to reject the null hypothesis: There is no significant difference between GradBoost algorithm.')      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **CatBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "CatBoost_before_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    CatBoost_value = untuned_df.iloc[i, :]['CatBoost']\n",
        "    CatBoost_before_array.append(CatBoost_value)\n",
        "\n",
        "CatBoost_before = np.array(CatBoost_before_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([97.97712418, 97.08716965, 87.15238095, 93.4531746 , 74.40336134,\n",
              "       76.24931647, 81.88888889, 99.88486842, 74.40336134])"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CatBoost_before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [],
      "source": [
        "CatBoost_after_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    CatBoost_value = tuned_df.iloc[i, :]['CatBoost']\n",
        "    CatBoost_after_array.append(CatBoost_value)\n",
        "\n",
        "CatBoost_after = np.array(CatBoost_after_array)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([97.96732026, 97.3783035 , 87.07619048, 93.81507937, 69.84621849,\n",
              "       75.52768284, 84.48148148, 99.89550439, 69.84621849])"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CatBoost_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistic, p_value = stats.wilcoxon(CatBoost_before, CatBoost_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Statistic: 18.0\n",
            "P-value: 0.65234375\n",
            "Fail to reject the null hypothesis: There is no significant difference between CatBoost algorithm.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Statistic: {statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f'Reject the null hypothesis: There is a significant difference between CatBoost algorithm.')\n",
        "else:\n",
        "    print(f'Fail to reject the null hypothesis: There is no significant difference between CatBoost algorithm.')      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **LightGBM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "LightGBM_before_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    LightGBM_value = untuned_df.iloc[i, :]['LightGBM']\n",
        "    LightGBM_before_array.append(LightGBM_value)\n",
        "\n",
        "LightGBM_before = np.array(LightGBM_before_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([97.42156863, 96.63299233, 88.12380952, 93.70634921, 71.80168067,\n",
              "       73.98513329, 80.88888889, 99.1127193 , 71.80168067])"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LightGBM_before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "LightGBM_after_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    LightGBM_value = tuned_df.iloc[i, :]['LightGBM']\n",
        "    LightGBM_after_array.append(LightGBM_value)\n",
        "\n",
        "LightGBM_after = np.array(LightGBM_after_array)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([97.12091503, 97.33461211, 82.36190476, 92.84920635, 69.79411765,\n",
              "       75.92071087, 82.85185185, 96.08377193, 69.79411765])"
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LightGBM_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistic, p_value = stats.wilcoxon(LightGBM_before, LightGBM_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Statistic: 11.0\n",
            "P-value: 0.203125\n",
            "Fail to reject the null hypothesis: There is no significant difference between LightGBM algorithm.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Statistic: {statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f'Reject the null hypothesis: There is a significant difference between LightGBM algorithm.')\n",
        "else:\n",
        "    print(f'Fail to reject the null hypothesis: There is no significant difference between LightGBM algorithm.')      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [],
      "source": [
        "XGBoost_before_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    XGBoost_value = untuned_df.iloc[i, :]['XGBoost']\n",
        "    XGBoost_before_array.append(XGBoost_value)\n",
        "\n",
        "XGBoost_before = np.array(XGBoost_before_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([96.67647059, 96.42902813, 85.00714286, 92.43015873, 70.57983193,\n",
              "       73.84244703, 80.37037037, 98.81030702, 70.57983193])"
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "XGBoost_before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [],
      "source": [
        "XGBoost_after_array = []\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    XGBoost_value = tuned_df.iloc[i, :]['XGBoost']\n",
        "    XGBoost_after_array.append(XGBoost_value)\n",
        "\n",
        "XGBoost_after = np.array(XGBoost_after_array)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([97.79738562, 96.79262575, 83.80238095, 92.96031746, 74.47563025,\n",
              "       75.33407382, 84.51851852, 92.04682018, 74.47563025])"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "XGBoost_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistic, p_value = stats.wilcoxon(XGBoost_before, XGBoost_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Statistic: 13.0\n",
            "P-value: 0.30078125\n",
            "Fail to reject the null hypothesis: There is no significant difference between XGBoost algorithm.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Statistic: {statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f'Reject the null hypothesis: There is a significant difference between XGBoost algorithm.')\n",
        "else:\n",
        "    print(f'Fail to reject the null hypothesis: There is no significant difference between XGBoost algorithm.')      "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMVO8koMTTTdYQJS3YoNuih",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
